{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regression Network on Radar Dataset (Bonneville Salt Flats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Training labels: \n",
      "[[ 8.3355 13.325  19.525 ]\n",
      " [18.156  17.388  32.775 ]\n",
      " [ 8.777  24.73   18.582 ]\n",
      " ...\n",
      " [15.785  15.992  29.641 ]\n",
      " [ 3.1736 10.716  12.27  ]\n",
      " [ 4.5552 21.035  14.234 ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 29 # Bonneville Salt Flats\n",
    "names = os.listdir(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/')\n",
    "names = sorted(names, key = index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'data/EXAMPLES/num{scenario_idx}_Ground_Truth_25k.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "y_train = y[:int(0.8*len(names))]\n",
    "y_test = y[(int(0.8*len(names))+1):]\n",
    "training_names = names[:int(0.8*len(names))]\n",
    "test_names = names[(int(0.8*len(names))+1):]\n",
    "\n",
    "print('Training labels: ')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.45], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [10851, 215, -5.45] # Tensor corner\n",
    "rng_res_tr = 59.9585/2         # Range resolution\n",
    "az_step_tr = 0.4               # Azimuth step size\n",
    "el_step_tr = 0.01              # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [10851, 215, -5.45] # Tensor corner\n",
    "rng_res_ts = 59.9585/2         # Range resolution\n",
    "az_step_ts = 0.4               # Azimuth step size\n",
    "el_step_ts = 0.01              # Elevation step size\n",
    "\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, normalize = True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind*bs, (ind+1)*bs):\n",
    "        try: temp = sio.loadmat(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/'+names[j])['P']\n",
    "        except: break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j,:])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "4999\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "#         self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "#         self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "#         self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(self.batchnorm1(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(self.batchnorm2(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "#         return output_reg\n",
    "    \n",
    "# from torchsummary import summary\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# model = Net()\n",
    "# model = model.to(device)\n",
    "# if device == 'cuda:0':\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "#     cudnn.benchmark = True\n",
    "# print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CART (Convolutional Adaptive Radar Transformer) and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 21,\n",
    "        seq_len: int = 26,\n",
    "        conv_channels: list = [32, 64],\n",
    "        kernel_size: int = 3,\n",
    "        pool_size: int = 2,\n",
    "        d_model: int = 64,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "        num_outputs: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # --- Local feature extractor (1D CNN) ---\n",
    "        self.conv1 = nn.Conv1d(in_channels, conv_channels[0], kernel_size, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(conv_channels[0])\n",
    "        self.conv2 = nn.Conv1d(conv_channels[0], conv_channels[1], kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(conv_channels[1])\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "\n",
    "        # compute reduced length after two poolings\n",
    "        reduced_len = seq_len // (pool_size**2)\n",
    "\n",
    "        # --- Token projection to transformer dimension ---\n",
    "        self.token_proj = nn.Linear(conv_channels[1], d_model)\n",
    "\n",
    "        # CLS token & positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, reduced_len + 1, d_model))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, num_outputs)\n",
    "        )\n",
    "\n",
    "        # Initialize\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (batch, channels=in_channels, length=seq_len)\n",
    "        # 1) CNN blocks\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        # x: (batch, conv_channels[1], reduced_len)\n",
    "\n",
    "        # 2) prepare transformer tokens\n",
    "        x = x.permute(0, 2, 1)  # -> (batch, seq', channels)\n",
    "        x = self.token_proj(x)  # -> (batch, seq', d_model)\n",
    "\n",
    "        # prepend CLS token\n",
    "        bs = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(bs, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch, seq'+1, d_model)\n",
    "\n",
    "        # add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # 3) Transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # 4) classification head on CLS\n",
    "        cls_rep = x[:, 0]\n",
    "        out = self.mlp_head(cls_rep)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 50  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7986.94181327  7681.26209447 -1015.22564403]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8266.08314467  7274.5550108  -1015.0579382 ]\n",
      "Train Loss: 25.199370 ---- Test Loss: 1.444526\n",
      "Epoch 1/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7994.31818544  7695.42741823 -1016.61244156]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8302.76916101  7214.52311836 -1013.96080029]\n",
      "Train Loss: 0.910260 ---- Test Loss: 0.236410\n",
      "Epoch 2/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8024.81914179  7674.77815561 -1017.31901688]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8306.07880287  7242.64739406 -1015.89312195]\n",
      "Train Loss: 0.415928 ---- Test Loss: 0.175200\n",
      "Epoch 3/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8007.98704631  7715.99213198 -1018.82139979]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8322.73589523  7215.44231857 -1015.40639421]\n",
      "Train Loss: 0.301340 ---- Test Loss: 0.145126\n",
      "Epoch 4/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7986.11364939  7741.98292876 -1019.03525046]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8300.67214309  7245.80971751 -1015.70920317]\n",
      "Train Loss: 0.249184 ---- Test Loss: 0.143444\n",
      "Epoch 5/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8033.03060327  7688.65329931 -1018.7413962 ]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8313.35703521  7234.76216304 -1015.92157229]\n",
      "Train Loss: 0.209435 ---- Test Loss: 0.094678\n",
      "Epoch 6/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7979.25716588  7744.34518002 -1018.73503942]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8316.26784033  7229.96492579 -1015.83380152]\n",
      "Train Loss: 0.190889 ---- Test Loss: 0.138700\n",
      "Epoch 7/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7981.0146923   7728.36835486 -1017.83180046]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8314.90112021  7230.36486408 -1015.76291515]\n",
      "Train Loss: 0.176777 ---- Test Loss: 0.103688\n",
      "Epoch 8/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8012.98891613  7708.25037933 -1018.659598  ]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8300.60458226  7243.40432113 -1015.55870543]\n",
      "Train Loss: 0.157878 ---- Test Loss: 0.111548\n",
      "Epoch 9/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8012.5776916   7702.95747434 -1018.29631739]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8325.00923807  7217.92719892 -1015.71478931]\n",
      "Train Loss: 0.140310 ---- Test Loss: 0.089917\n",
      "Epoch 10/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7975.33849526  7735.40023508 -1017.90670976]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8308.27619045  7239.99026402 -1015.88486543]\n",
      "Train Loss: 0.134146 ---- Test Loss: 0.090968\n",
      "Epoch 11/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7996.50033835  7721.94829714 -1018.44281543]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8320.49891808  7226.21997636 -1015.90178394]\n",
      "Train Loss: 0.122611 ---- Test Loss: 0.088219\n",
      "Epoch 12/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8019.36922229  7695.54238211 -1018.27447428]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8317.80129013  7232.26902276 -1016.07983971]\n",
      "Train Loss: 0.118657 ---- Test Loss: 0.088489\n",
      "Epoch 13/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8036.54465488  7709.90564655 -1020.3209734 ]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8312.54766146  7250.1125039  -1016.79485123]\n",
      "Train Loss: 0.114875 ---- Test Loss: 0.184835\n",
      "Epoch 14/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8003.81703198  7717.46888739 -1018.64022509]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8327.06415353  7217.87383257 -1015.85470001]\n",
      "Train Loss: 0.134216 ---- Test Loss: 0.079878\n",
      "Epoch 15/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8023.11892938  7695.19126182 -1018.50010469]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8317.63318396  7230.61705851 -1015.96822975]\n",
      "Train Loss: 0.095899 ---- Test Loss: 0.081026\n",
      "Epoch 16/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7997.9675313   7733.22254278 -1019.25722905]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8326.54024842  7215.59074808 -1015.68036309]\n",
      "Train Loss: 0.090136 ---- Test Loss: 0.079634\n",
      "Epoch 17/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8012.89640472  7695.85028343 -1017.86621888]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8311.6970571   7244.34427096 -1016.38630452]\n",
      "Train Loss: 0.101582 ---- Test Loss: 0.111391\n",
      "Epoch 18/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8018.82959557  7723.07691464 -1019.98712908]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8324.3058162   7224.35733945 -1016.05421721]\n",
      "Train Loss: 0.096265 ---- Test Loss: 0.091869\n",
      "Epoch 19/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-7996.09401646  7720.45393524 -1018.32093589]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8323.96554326  7220.92117892 -1015.82293236]\n",
      "Train Loss: 0.080150 ---- Test Loss: 0.082610\n",
      "Epoch 20/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8022.99623011  7713.89298227 -1019.67876926]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8325.63233935  7223.04024543 -1016.06700484]\n",
      "Train Loss: 0.073919 ---- Test Loss: 0.054160\n",
      "Epoch 21/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8008.64008083  7712.98735335 -1018.67350101]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8320.19221571  7232.16934282 -1016.24014632]\n",
      "Train Loss: 0.071027 ---- Test Loss: 0.076841\n",
      "Epoch 22/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8017.76607338  7714.26784256 -1019.35722114]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8321.31026669  7237.11821992 -1016.61725869]\n",
      "Train Loss: 0.067389 ---- Test Loss: 0.075982\n",
      "Epoch 23/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8032.00714758  7679.63867513 -1018.10273172]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8322.21795359  7236.73220731 -1016.65704665]\n",
      "Train Loss: 0.069178 ---- Test Loss: 0.096267\n",
      "Epoch 24/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8032.03896721  7687.14923675 -1018.58048389]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8324.238283    7221.42391372 -1015.87229301]\n",
      "Train Loss: 0.086778 ---- Test Loss: 0.051722\n",
      "Epoch 25/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8012.73291423  7722.74437705 -1019.56374992]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8335.15239533  7210.14419213 -1015.9519188 ]\n",
      "Train Loss: 0.071078 ---- Test Loss: 0.063559\n",
      "Epoch 26/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8003.41204013  7728.53248621 -1019.31735629]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8312.28370831  7242.60535098 -1016.32176013]\n",
      "Train Loss: 0.079116 ---- Test Loss: 0.120859\n",
      "Epoch 27/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8012.42112584  7725.76845151 -1019.73547161]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8322.93245106  7228.19192487 -1016.19035395]\n",
      "Train Loss: 0.061246 ---- Test Loss: 0.060282\n",
      "Epoch 28/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8019.32295861  7718.02297024 -1019.69854909]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8321.46699817  7230.46675932 -1016.22589162]\n",
      "Train Loss: 0.052776 ---- Test Loss: 0.048930\n",
      "Epoch 29/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8000.75613919  7714.23976007 -1018.23300524]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8310.83990676  7241.62360457 -1016.16196004]\n",
      "Train Loss: 0.051299 ---- Test Loss: 0.071656\n",
      "Epoch 30/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8015.48168045  7702.86177193 -1018.48205687]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8316.01163637  7239.19462174 -1016.37441708]\n",
      "Train Loss: 0.049910 ---- Test Loss: 0.047667\n",
      "Epoch 31/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8018.80260249  7711.95282265 -1019.27862351]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8315.11112015  7238.06145993 -1016.24321824]\n",
      "Train Loss: 0.047321 ---- Test Loss: 0.065041\n",
      "Epoch 32/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8006.1756665   7715.70452759 -1018.68361655]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8327.94342774  7228.49653788 -1016.55757084]\n",
      "Train Loss: 0.049830 ---- Test Loss: 0.058369\n",
      "Epoch 33/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8011.93567177  7703.87963821 -1018.31247149]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8323.00040833  7225.42293794 -1016.02772995]\n",
      "Train Loss: 0.045321 ---- Test Loss: 0.037605\n",
      "Epoch 34/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8018.71453206  7718.2153172  -1019.67060743]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8323.8894944   7232.45921807 -1016.51493141]\n",
      "Train Loss: 0.046147 ---- Test Loss: 0.045527\n",
      "Epoch 35/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8014.913443    7705.24835314 -1018.59604326]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8321.29149873  7231.93349512 -1016.30236807]\n",
      "Train Loss: 0.044660 ---- Test Loss: 0.042123\n",
      "Epoch 36/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8014.10178455  7710.82918684 -1018.89687511]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8331.63688366  7225.65880262 -1016.6433116 ]\n",
      "Train Loss: 0.043913 ---- Test Loss: 0.049604\n",
      "Epoch 37/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8004.26177704  7725.86424052 -1019.20357336]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8330.45431845  7226.03319819 -1016.58357257]\n",
      "Train Loss: 0.046444 ---- Test Loss: 0.055243\n",
      "Epoch 38/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8018.8603028   7721.57386424 -1019.89363462]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8317.32074993  7226.021283   -1015.66858981]\n",
      "Train Loss: 0.068756 ---- Test Loss: 0.046654\n",
      "Epoch 39/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8005.7317862   7717.31749086 -1018.75688775]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8328.88756922  7226.00695409 -1016.47288904]\n",
      "Train Loss: 0.044175 ---- Test Loss: 0.051443\n",
      "Epoch 40/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8021.83354723  7702.09995066 -1018.85338974]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8325.64973291  7228.55317399 -1016.40132294]\n",
      "Train Loss: 0.040548 ---- Test Loss: 0.041067\n",
      "Epoch 41/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8009.35932807  7707.90819612 -1018.39823713]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8321.47444913  7227.70238648 -1016.05928764]\n",
      "Train Loss: 0.036261 ---- Test Loss: 0.041227\n",
      "Epoch 42/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8007.25488537  7716.210537   -1018.78698137]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8326.48669656  7228.92682708 -1016.48216516]\n",
      "Train Loss: 0.043277 ---- Test Loss: 0.053757\n",
      "Epoch 43/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8022.95169484  7700.939986   -1018.85369232]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8330.68967221  7218.79731    -1016.16301638]\n",
      "Train Loss: 0.041196 ---- Test Loss: 0.041386\n",
      "Epoch 44/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8027.83019151  7708.02008783 -1019.62533528]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8319.10871409  7226.56048835 -1015.82561455]\n",
      "Train Loss: 0.040262 ---- Test Loss: 0.051088\n",
      "Epoch 45/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8023.77790767  7704.70401301 -1019.14712141]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8328.3729676   7224.40016837 -1016.33999349]\n",
      "Train Loss: 0.039998 ---- Test Loss: 0.033374\n",
      "Epoch 46/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8012.88215044  7698.64660811 -1018.04275576]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8324.89366472  7222.34033257 -1015.97328838]\n",
      "Train Loss: 0.039011 ---- Test Loss: 0.047514\n",
      "Epoch 47/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8021.01758042  7700.26170291 -1018.6828262 ]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8320.70138099  7227.64383787 -1016.00194605]\n",
      "Train Loss: 0.036287 ---- Test Loss: 0.030548\n",
      "Epoch 48/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8004.14033295  7707.38954882 -1018.02080832]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8322.7951665   7229.49825176 -1016.25976539]\n",
      "Train Loss: 0.033143 ---- Test Loss: 0.031231\n",
      "Epoch 49/50\n",
      "----------\n",
      "[-8012.26266049  7711.66548905 -1018.82859387]\n",
      "[-8010.00980873  7699.55751358 -1017.91084153]\n",
      "[-8324.52703383  7216.12098584 -1015.57213474]\n",
      "[-8325.64217119  7222.83853895 -1016.0555045 ]\n",
      "Train Loss: 0.034563 ---- Test Loss: 0.020003\n",
      "4326.399531841278\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20      # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-'*10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names)//bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:,0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names)//bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:,0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "\n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "        \n",
    "        epoch_loss_train = running_loss_train*x_train.size()[0]/len(training_names)\n",
    "        epoch_loss_test = running_loss_test*x_test.size()[0]/len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch%5==0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 0.10075804757509119\n",
      "Localization Error (m) = 9.243764370800344\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical),3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    cartesian[:,0] = np.multiply(np.cos(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,1] = np.multiply(-np.sin(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,2] = np.multiply(np.sin(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range,az,el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test),3))\n",
    "labels_test_reg = np.zeros((len(y_test),3))\n",
    "\n",
    "for i in range(0, len(y_test)//bs):  \n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, True)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs*i : bs*i + bs] = (labels_test[:,0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs*i : bs*i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs*i : bs*i + bs, 2] = labels_test_reg[bs*i : bs*i + bs, 2]\n",
    "    \n",
    "azim_tot = 0\n",
    "for i in np.arange(0,len(out_test_reg),1):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i,1] - labels_test_reg[i,1])\n",
    "    \n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "\n",
    "sum_tot = 0\n",
    "for i in np.arange(0,len(new_data) - (len(new_data) % bs),1):\n",
    "    sum_tot += np.linalg.norm(new_data[i,:] - new_labels_data[i,:])\n",
    "    \n",
    "# def reject_outliers(data, m=2):\n",
    "#     return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "# \n",
    "# azims = (np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) - np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]))[:,1]\n",
    "# azims = reject_outliers(azims)\n",
    "# mse = np.mean(azims**2)\n",
    "# bias_sq = np.mean(azims)**2\n",
    "# var = np.var(azims)\n",
    "# print(mse, bias_sq, var)\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
