{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regression Network on Radar Dataset (Great Smoky Mountains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Training labels: \n",
      "[[ 1.4283  18.424   25.558  ]\n",
      " [12.268   22.517   50.832  ]\n",
      " [ 0.86123 19.459   43.564  ]\n",
      " ...\n",
      " [-0.23936 20.845   36.264  ]\n",
      " [ 6.6502  21.851   37.151  ]\n",
      " [ 1.8854  22.812   40.971  ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 76 # Great Smoky Mountains\n",
    "names = os.listdir(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names = sorted(names, key = index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'num{scenario_idx}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "y_train = y[:int(0.9*len(names))]\n",
    "y_test = y[(int(0.9*len(names))+1):]\n",
    "training_names = names[:int(0.9*len(names))]\n",
    "test_names = names[(int(0.9*len(names))+1):]\n",
    "\n",
    "print('Training labels: ')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.45], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [11388, 215, -6.15] # Tensor corner\n",
    "rng_res_tr = 59.9585/2         # Range resolution\n",
    "az_step_tr = 0.4               # Azimuth step size\n",
    "el_step_tr = 0.01              # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11388, 215, -6.15] # Tensor corner\n",
    "rng_res_ts = 59.9585/2         # Range resolution\n",
    "az_step_ts = 0.4               # Azimuth step size\n",
    "el_step_ts = 0.01              # Elevation step size\n",
    "\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, normalize = True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind*bs, (ind+1)*bs):\n",
    "        try: temp = sio.loadmat(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/'+names[j])['P']\n",
    "        except: break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j,:])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 32, 24]           2,048\n",
      "       BatchNorm1d-2               [-1, 32, 24]              64\n",
      "            Conv1d-3               [-1, 64, 10]           6,208\n",
      "       BatchNorm1d-4               [-1, 64, 10]             128\n",
      "            Linear-5                   [-1, 20]           6,420\n",
      "            Linear-6                    [-1, 2]              42\n",
      "               Net-7                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 14,910\n",
      "Trainable params: 14,910\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "        self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "        return output_reg\n",
    "    \n",
    "from torchsummary import summary\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 20  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8332.7092787   7982.44400517 -1163.32317433]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8163.32024805  8028.58461784 -1126.63768157]\n",
      "Train Loss: 19.946576 ---- Test Loss: 10.486786\n",
      "Epoch 1/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8340.71881635  7979.66579736 -1163.71276809]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8163.36621639  8019.18014334 -1125.99222543]\n",
      "Train Loss: 9.815422 ---- Test Loss: 10.307250\n",
      "Epoch 2/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8346.8583253   7970.1699834  -1163.49875694]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8171.67370811  8011.52620898 -1126.04813866]\n",
      "Train Loss: 9.760194 ---- Test Loss: 10.289728\n",
      "Epoch 3/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8350.59042044  7965.22713455 -1163.42690703]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8170.67287441  8015.87279179 -1126.27729795]\n",
      "Train Loss: 9.732732 ---- Test Loss: 10.053620\n",
      "Epoch 4/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8353.59756331  7968.10081471 -1163.84623989]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8166.94676175  8023.62919418 -1126.55034917]\n",
      "Train Loss: 9.721252 ---- Test Loss: 9.934000\n",
      "Epoch 5/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8351.39019885  7970.87668875 -1163.87842236]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8161.14572584  8021.95769777 -1126.02793867]\n",
      "Train Loss: 9.720246 ---- Test Loss: 10.044536\n",
      "Epoch 6/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8355.38774088  7964.75550767 -1163.74411316]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8160.69238944  8015.86601303 -1125.57601046]\n",
      "Train Loss: 9.724785 ---- Test Loss: 10.101016\n",
      "Epoch 7/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8365.43468274  7953.27738662 -1163.67985002]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8177.66060996  7995.47994816 -1125.36448732]\n",
      "Train Loss: 9.736475 ---- Test Loss: 10.298241\n",
      "Epoch 8/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8366.13499351  7952.84672533 -1163.70110474]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8179.41872993  7983.94135463 -1124.69483319]\n",
      "Train Loss: 9.731353 ---- Test Loss: 10.442019\n",
      "Epoch 9/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8368.49008988  7952.16942984 -1163.82616384]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8185.05757653  7981.32410377 -1124.91214615]\n",
      "Train Loss: 9.722552 ---- Test Loss: 10.345664\n",
      "Epoch 10/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8365.73148005  7957.15413812 -1163.97086022]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8171.39478135  7988.71334831 -1124.45816925]\n",
      "Train Loss: 9.706943 ---- Test Loss: 10.271877\n",
      "Epoch 11/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8367.2053713   7955.96042233 -1163.99560075]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8175.96204553  7989.42016744 -1124.82817271]\n",
      "Train Loss: 9.704649 ---- Test Loss: 10.218852\n",
      "Epoch 12/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8366.42331259  7957.61229658 -1164.05323066]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8176.41048245  7986.14750724 -1124.63470007]\n",
      "Train Loss: 9.698535 ---- Test Loss: 10.341197\n",
      "Epoch 13/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8365.63246861  7958.636359   -1164.06661899]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8185.36593451  7978.68909056 -1124.75287491]\n",
      "Train Loss: 9.689949 ---- Test Loss: 10.347499\n",
      "Epoch 14/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8367.74804345  7954.90019992 -1163.96160152]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8181.04231598  7977.67656468 -1124.37870476]\n",
      "Train Loss: 9.687819 ---- Test Loss: 10.372988\n",
      "Epoch 15/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8370.08487091  7954.06559243 -1164.07439361]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8180.42015953  7979.53025408 -1124.46223094]\n",
      "Train Loss: 9.682956 ---- Test Loss: 10.359112\n",
      "Epoch 16/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8366.55439512  7956.25769254 -1163.9686936 ]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8181.91133851  7971.59508197 -1124.02225334]\n",
      "Train Loss: 9.675587 ---- Test Loss: 10.344922\n",
      "Epoch 17/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8364.38986404  7960.73011267 -1164.1213725 ]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8172.80540109  7984.66183338 -1124.2787939 ]\n",
      "Train Loss: 9.678336 ---- Test Loss: 10.144599\n",
      "Epoch 18/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8366.80593396  7962.05272157 -1164.38973854]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8177.42902985  7975.89862435 -1124.00202396]\n",
      "Train Loss: 9.669944 ---- Test Loss: 10.333270\n",
      "Epoch 19/20\n",
      "----------\n",
      "[-8256.00079179  7759.51989019 -1142.24377699]\n",
      "[-8365.65268732  7961.41060404 -1164.26088963]\n",
      "[-8142.22103724  8033.80673867 -1125.51926151]\n",
      "[-8164.49855002  7990.76943414 -1124.11455315]\n",
      "Train Loss: 9.660878 ---- Test Loss: 10.173482\n",
      "1140.2187929153442\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20      # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-'*10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names)//bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:,0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names)//bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:,0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "\n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "        \n",
    "        epoch_loss_train = running_loss_train*x_train.size()[0]/len(training_names)\n",
    "        epoch_loss_test = running_loss_test*x_test.size()[0]/len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch%5==0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_accuracy = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 1.3516481265794942\n",
      "Localization Error (m) = 145.3416231923269\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical),3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    cartesian[:,0] = np.multiply(np.cos(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,1] = np.multiply(-np.sin(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,2] = np.multiply(np.sin(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range,az,el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test),3))\n",
    "labels_test_reg = np.zeros((len(y_test),3))\n",
    "\n",
    "for i in range(0, len(y_test)//bs):  \n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, True)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs*i : bs*i + bs] = (labels_test[:,0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs*i : bs*i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs*i : bs*i + bs, 2] = labels_test_reg[bs*i : bs*i + bs, 2]\n",
    "    \n",
    "azim_tot = 0\n",
    "for i in np.arange(0,len(out_test_reg),1):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i,1] - labels_test_reg[i,1])\n",
    "    \n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "\n",
    "sum_tot = 0\n",
    "for i in np.arange(0,len(new_data) - (len(new_data) % bs),1):\n",
    "    sum_tot += np.linalg.norm(new_data[i,:] - new_labels_data[i,:])\n",
    "    \n",
    "# def reject_outliers(data, m=2):\n",
    "#     return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "# \n",
    "# azims = (np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) - np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]))[:,1]\n",
    "# azims = reject_outliers(azims)\n",
    "# mse = np.mean(azims**2)\n",
    "# bias_sq = np.mean(azims)**2\n",
    "# var = np.var(azims)\n",
    "# print(mse, bias_sq, var)\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
