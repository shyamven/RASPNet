{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regression Network on Radar Dataset (Great Smoky Mountains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Training labels: \n",
      "[[ 1.4283  18.424   25.558  ]\n",
      " [12.268   22.517   50.832  ]\n",
      " [ 0.86123 19.459   43.564  ]\n",
      " ...\n",
      " [-0.23936 20.845   36.264  ]\n",
      " [ 6.6502  21.851   37.151  ]\n",
      " [ 1.8854  22.812   40.971  ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 76 # Great Smoky Mountains\n",
    "names = os.listdir(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/')\n",
    "names = sorted(names, key = index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'data/EXAMPLES/num{scenario_idx}_Ground_Truth_25k.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "y_train = y[:int(0.8*len(names))]\n",
    "y_test = y[(int(0.8*len(names))+1):]\n",
    "training_names = names[:int(0.8*len(names))]\n",
    "test_names = names[(int(0.8*len(names))+1):]\n",
    "\n",
    "print('Training labels: ')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.45], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [11388, 215, -6.15] # Tensor corner\n",
    "rng_res_tr = 59.9585/2         # Range resolution\n",
    "az_step_tr = 0.4               # Azimuth step size\n",
    "el_step_tr = 0.01              # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11388, 215, -6.15] # Tensor corner\n",
    "rng_res_ts = 59.9585/2         # Range resolution\n",
    "az_step_ts = 0.4               # Azimuth step size\n",
    "el_step_ts = 0.01              # Elevation step size\n",
    "\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, normalize = True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind*bs, (ind+1)*bs):\n",
    "        try: temp = sio.loadmat(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/'+names[j])['P']\n",
    "        except: break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j,:])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "4999\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "#         self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "#         self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "#         self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(self.batchnorm1(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(self.batchnorm2(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "#         return output_reg\n",
    "    \n",
    "# from torchsummary import summary\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# model = Net()\n",
    "# model = model.to(device)\n",
    "# if device == 'cuda:0':\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "#     cudnn.benchmark = True\n",
    "# print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CART (Convolutional Adaptive Radar Transformer) and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 21,\n",
    "        seq_len: int = 26,\n",
    "        d_model: int = 64,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        num_outputs: int = 2\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 1) Patch embedding: each of the seq_len timeâ€‘steps is a token of dim in_features\n",
    "        self.patch_embed = nn.Linear(in_features, d_model)\n",
    "\n",
    "        # 2) CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "\n",
    "        # 3) Positional embeddings for (seq_len + 1) tokens\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, seq_len + 1, d_model))\n",
    "\n",
    "        # 4) Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # so input shape is (batch, seq, d_model)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 5) Classification / regression head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, num_outputs)\n",
    "        )\n",
    "\n",
    "        # initialize\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_features=21, seq_len=26)\n",
    "        returns: (batch_size, num_outputs=2)\n",
    "        \"\"\"\n",
    "        bs = x.size(0)\n",
    "        # reshape to (batch, seq_len, in_features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # patch embedding -> (batch, seq_len, d_model)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # prepend cls token -> (batch, seq_len+1, d_model)\n",
    "        cls_tokens = self.cls_token.expand(bs, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # take CLS representation\n",
    "        cls_rep = x[:, 0]  # (batch, d_model)\n",
    "\n",
    "        # head to outputs\n",
    "        out = self.mlp_head(cls_rep)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 20  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8446.93940955  7981.55347249 -1147.0451923 ]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8481.52790092  7995.24291983 -1168.22856889]\n",
      "Train Loss: 55.960185 ---- Test Loss: 10.976706\n",
      "Epoch 1/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8315.54195395  8129.35164786 -1147.80343994]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8382.81102593  8136.15744895 -1170.84118982]\n",
      "Train Loss: 10.629118 ---- Test Loss: 9.865820\n",
      "Epoch 2/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8303.51565298  8154.54063355 -1148.69559164]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8401.19708512  8123.39484984 -1171.27471062]\n",
      "Train Loss: 9.974255 ---- Test Loss: 9.735353\n",
      "Epoch 3/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8284.44620511  8146.70774895 -1146.81124634]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8412.12368113  8107.23615186 -1170.93780942]\n",
      "Train Loss: 9.874945 ---- Test Loss: 9.696237\n",
      "Epoch 4/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8303.50229503  8118.11158504 -1146.17816341]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8407.55564283  8098.87580589 -1170.02671634]\n",
      "Train Loss: 9.825217 ---- Test Loss: 9.694521\n",
      "Epoch 5/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8329.30497046  8105.46372339 -1147.12962021]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8417.89404059  8094.40616894 -1170.46265213]\n",
      "Train Loss: 9.800585 ---- Test Loss: 9.672118\n",
      "Epoch 6/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8305.65221363  8118.8396185  -1146.38013438]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8412.03966041  8093.63588855 -1169.98624314]\n",
      "Train Loss: 9.791048 ---- Test Loss: 9.698279\n",
      "Epoch 7/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8307.37355104  8121.26467764 -1146.66894555]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8426.69450452  8078.71058157 -1170.00938942]\n",
      "Train Loss: 9.777145 ---- Test Loss: 9.646385\n",
      "Epoch 8/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8332.53534077  8104.71843473 -1147.30685591]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8418.44833285  8088.82043015 -1170.11474518]\n",
      "Train Loss: 9.763981 ---- Test Loss: 9.661785\n",
      "Epoch 9/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8290.4090359   8118.63257394 -1145.29042913]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8422.52586824  8086.10313429 -1170.2208394 ]\n",
      "Train Loss: 9.756588 ---- Test Loss: 9.647662\n",
      "Epoch 10/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8315.58653345  8095.1762708  -1145.45110444]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8419.75857516  8084.75902425 -1169.92747076]\n",
      "Train Loss: 9.756752 ---- Test Loss: 9.640166\n",
      "Epoch 11/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8334.27618498  8103.88435617 -1147.37263969]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8417.13994709  8090.33126165 -1170.12511795]\n",
      "Train Loss: 9.750003 ---- Test Loss: 9.629461\n",
      "Epoch 12/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8332.3309473   8094.44099582 -1146.58533766]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8415.56640092  8090.74293089 -1170.04001447]\n",
      "Train Loss: 9.754414 ---- Test Loss: 9.617939\n",
      "Epoch 13/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8317.27383841  8100.73877946 -1145.95344263]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8423.47862929  8087.90853564 -1170.41504231]\n",
      "Train Loss: 9.736004 ---- Test Loss: 9.632289\n",
      "Epoch 14/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8312.02478958  8096.82281939 -1145.31262571]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8420.77569474  8083.4526173  -1169.91032632]\n",
      "Train Loss: 9.745642 ---- Test Loss: 9.623375\n",
      "Epoch 15/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8324.94087     8101.04467413 -1146.51673156]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8418.59598879  8092.79233935 -1170.40126517]\n",
      "Train Loss: 9.736811 ---- Test Loss: 9.632714\n",
      "Epoch 16/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8319.94214427  8096.25544928 -1145.83347042]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8424.52412054  8084.44376723 -1170.25016144]\n",
      "Train Loss: 9.725977 ---- Test Loss: 9.597010\n",
      "Epoch 17/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8321.57243439  8099.12910449 -1146.14660404]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8424.17089537  8086.29302349 -1170.35295965]\n",
      "Train Loss: 9.721516 ---- Test Loss: 9.608261\n",
      "Epoch 18/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8300.35018397  8123.64507483 -1146.33767329]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8411.66618193  8097.12244802 -1170.20158506]\n",
      "Train Loss: 9.720013 ---- Test Loss: 9.632531\n",
      "Epoch 19/20\n",
      "----------\n",
      "[-8326.08026266  8099.5871036  -1146.49701187]\n",
      "[-8321.94758428  8100.32006403 -1146.25512693]\n",
      "[-8427.13167156  8069.92994113 -1169.43217239]\n",
      "[-8423.19154879  8091.76477037 -1170.66201005]\n",
      "Train Loss: 9.733179 ---- Test Loss: 9.614636\n",
      "1559.3383734226227\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20      # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-'*10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names)//bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:,0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names)//bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:,0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "\n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "        \n",
    "        epoch_loss_train = running_loss_train*x_train.size()[0]/len(training_names)\n",
    "        epoch_loss_test = running_loss_test*x_test.size()[0]/len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch%5==0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 1.205552564173266\n",
      "Localization Error (m) = 136.08325302978162\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical),3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    cartesian[:,0] = np.multiply(np.cos(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,1] = np.multiply(-np.sin(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,2] = np.multiply(np.sin(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range,az,el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test),3))\n",
    "labels_test_reg = np.zeros((len(y_test),3))\n",
    "\n",
    "for i in range(0, len(y_test)//bs):  \n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, True)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs*i : bs*i + bs] = (labels_test[:,0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs*i : bs*i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs*i : bs*i + bs, 2] = labels_test_reg[bs*i : bs*i + bs, 2]\n",
    "    \n",
    "azim_tot = 0\n",
    "for i in np.arange(0,len(out_test_reg),1):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i,1] - labels_test_reg[i,1])\n",
    "    \n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "\n",
    "sum_tot = 0\n",
    "for i in np.arange(0,len(new_data) - (len(new_data) % bs),1):\n",
    "    sum_tot += np.linalg.norm(new_data[i,:] - new_labels_data[i,:])\n",
    "    \n",
    "# def reject_outliers(data, m=2):\n",
    "#     return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "# \n",
    "# azims = (np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) - np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]))[:,1]\n",
    "# azims = reject_outliers(azims)\n",
    "# mse = np.mean(azims**2)\n",
    "# bias_sq = np.mean(azims)**2\n",
    "# var = np.var(azims)\n",
    "# print(mse, bias_sq, var)\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
