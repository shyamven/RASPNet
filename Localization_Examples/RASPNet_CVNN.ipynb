{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88b60e-aa2e-484d-868b-2a95c2c72a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from complexPyTorch.complexLayers import ComplexLinear, ComplexConv2d\n",
    "from complexPyTorch.complexFunctions import complex_relu, complex_avg_pool2d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ce98d-d69a-4cb8-aa10-b12b847f892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Dataset\n",
    "class RASPNetDataset(Dataset):\n",
    "    def __init__(self, data_dir, csv_file, split, limit):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Directory with all the data.\n",
    "            csv_file (string): Path to the csv file with labels.\n",
    "            split (string): 'train' or 'test'.\n",
    "            limit (int): Number of samples to load.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.limit = limit\n",
    "\n",
    "        # Load labels\n",
    "        labels_df = pd.read_csv(csv_file)\n",
    "        self.labels = torch.from_numpy(labels_df[['R_idx', 'Az_idx', 'El_idx']].values[:limit].astype(np.float32)) # x, y, z\n",
    "        self.labels = self.labels\n",
    "\n",
    "        # Initialize lists to hold features\n",
    "        features_list = []\n",
    "\n",
    "        # Paths to real and imaginary folders\n",
    "        real_dir = os.path.join(data_dir, split)\n",
    "        imag_dir = os.path.join(data_dir, split)\n",
    "\n",
    "        # Load all real and imag data into memory\n",
    "        for i in range(1, limit + 1):\n",
    "            # Construct file names\n",
    "            real_file = f'real{i}.mat'\n",
    "            imag_file = f'imag{i}.mat'\n",
    "\n",
    "            real_path = os.path.join(real_dir, real_file)\n",
    "            imag_path = os.path.join(imag_dir, imag_file)\n",
    "\n",
    "            # Load .mat files\n",
    "            try:\n",
    "                real_data = scipy.io.loadmat(real_path)['Y_real']  # Adjust the key if different\n",
    "                imag_data = scipy.io.loadmat(imag_path)['Y_imag']  # Adjust the key if different\n",
    "            except KeyError as e:\n",
    "                raise KeyError(f\"Variable not found in {real_file} or {imag_file}: {e}\")\n",
    "            except FileNotFoundError as e:\n",
    "                raise FileNotFoundError(f\"File not found: {e}\")\n",
    "\n",
    "            # Verify the shape\n",
    "            if real_data.shape != (5, 21, 16):\n",
    "                raise ValueError(f\"Unexpected shape for {real_file}: {real_data.shape}\")\n",
    "            if imag_data.shape != (5, 21, 16):\n",
    "                raise ValueError(f\"Unexpected shape for {imag_file}: {imag_data.shape}\")\n",
    "\n",
    "            # Concatenate real and imaginary parts\n",
    "            features = np.concatenate([real_data, imag_data]).astype(np.float32)  # Shape: (3360,)\n",
    "            features_list.append(features)\n",
    "\n",
    "            # Optional: Print progress every 100 files\n",
    "            if i % 5000 == 0 or i == limit:\n",
    "                print(f'Loaded {i}/{limit} samples from {split} set.')\n",
    "\n",
    "        # Convert list to tensor\n",
    "        self.features = torch.from_numpy(np.array(features_list))  # Shape: (limit, 3360)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.limit\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c455b9-a025-4844-b7a0-788a3affcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "SCENARIO = 'num29'\n",
    "DATA_DIR = f'data/CVNN/{SCENARIO}'\n",
    "TRAIN_CSV = f'data/CVNN/{SCENARIO}/train.csv'\n",
    "TEST_CSV = f'data/CVNN/{SCENARIO}/test.csv'\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "train_dataset = RASPNetDataset(data_dir=DATA_DIR, csv_file=TRAIN_CSV, split='train', limit=20000)\n",
    "test_dataset = RASPNetDataset(data_dir=DATA_DIR, csv_file=TEST_CSV, split='test', limit=5000)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb5e41-6ba0-4777-a289-1456f685376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training and test dataset global constants\n",
    "if SCENARIO == 'num29':\n",
    "    coord_tr = torch.tensor([10851, 215, -5.4], device=device, dtype=torch.float32)\n",
    "    coord_ts = torch.tensor([10851, 215, -5.4], device=device, dtype=torch.float32)\n",
    "if SCENARIO == 'num35':\n",
    "    coord_tr = torch.tensor([11381, 215, -0.95], device=device, dtype=torch.float32)\n",
    "    coord_ts = torch.tensor([11381, 215, -0.95], device=device, dtype=torch.float32)\n",
    "if SCENARIO == 'num60':\n",
    "    coord_tr = torch.tensor([11073, 215, -5.3], device=device, dtype=torch.float32)\n",
    "    coord_ts = torch.tensor([11073, 215, -5.3], device=device, dtype=torch.float32)\n",
    "if SCENARIO == 'num62':\n",
    "    coord_tr = torch.tensor([11471, 215, -5.6], device=device, dtype=torch.float32)\n",
    "    coord_ts = torch.tensor([11471, 215, -5.6], device=device, dtype=torch.float32)\n",
    "if SCENARIO == 'num76':\n",
    "    coord_tr = torch.tensor([11388, 215, -6.15], device=device, dtype=torch.float32)\n",
    "    coord_ts = torch.tensor([11388, 215, -6.15], device=device, dtype=torch.float32)\n",
    "\n",
    "rng_res_tr = 59.9585 / 2          # Range resolution\n",
    "az_step_tr = 0.4                  # Azimuth step size\n",
    "el_step_tr = 0.01                 # Elevation step size\n",
    "scale_tr = [rng_res_tr, az_step_tr, el_step_tr]\n",
    "rng_res_ts = 59.9585 / 2          # Range resolution\n",
    "az_step_ts = 0.4                  # Azimuth step size\n",
    "el_step_ts = 0.01                 # Elevation step size\n",
    "scale_ts = [rng_res_ts, az_step_ts, el_step_ts]\n",
    "\n",
    "# Define the average Euclidean distance function\n",
    "def average_euclidean_distance(pred, target):\n",
    "    \"\"\"\n",
    "    Compute the average Euclidean distance between predictions and targets.\n",
    "\n",
    "    Parameters:\n",
    "    - pred (Tensor): Predicted Cartesian coordinates of shape (batch_size, 3).\n",
    "    - target (Tensor): True Cartesian coordinates of shape (batch_size, 3).\n",
    "\n",
    "    Returns:\n",
    "    - float: The average Euclidean distance over the batch.\n",
    "    \"\"\"\n",
    "    return torch.norm(pred - target, p=2, dim=1).mean().item()\n",
    "\n",
    "# Define the Spher2Cart_1D_torch function as previously modified\n",
    "def Spher2Cart_1D_torch(spherical, scale, coord):\n",
    "    \"\"\"\n",
    "    Convert spherical coordinates to Cartesian coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - spherical: Tensor of shape (batch_size, 3) containing [range, azimuth, elevation].\n",
    "    - scale: Tensor or list containing [rng_res, az_step, el_step].\n",
    "    - coord: Tensor or list containing [x0, y0, z0] to shift the coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - cartesian: Tensor of shape (batch_size, 3) containing [x, y, z].\n",
    "    \"\"\"\n",
    "    # If scale and coord are lists or numpy arrays, convert them to tensors\n",
    "    if not isinstance(scale, torch.Tensor): scale = torch.tensor(scale, device=spherical.device, dtype=torch.float32)\n",
    "    else: scale = scale.to(device=spherical.device, dtype=torch.float32)\n",
    "    if not isinstance(coord, torch.Tensor): coord = torch.tensor(coord, device=spherical.device, dtype=torch.float32)\n",
    "    else: coord = coord.to(device=spherical.device, dtype=torch.float32)\n",
    "\n",
    "    # Clone and detach to avoid modifying the original tensors\n",
    "    scale = scale.clone().detach(); coord = coord.clone().detach()\n",
    "\n",
    "    scaled = spherical * scale + coord  # Apply scaling and shifting\n",
    "    r = scaled[:, 0]; az_deg = scaled[:, 1]; el_deg = scaled[:, 2]\n",
    "\n",
    "    az = torch.deg2rad(az_deg); el = torch.deg2rad(el_deg)\n",
    "\n",
    "    hyp = torch.cos(el) * r\n",
    "    x = torch.cos(az) * hyp\n",
    "    y = -torch.sin(az) * hyp\n",
    "    z = torch.sin(el) * r\n",
    "\n",
    "    cartesian = torch.stack((x, y, z), dim=1)\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93919885-24f3-4538-a956-2e4fc4b4eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_based_hilbert_transform(real_features):\n",
    "    \"\"\"\n",
    "    Apply the FFT-based Hilbert transform to the real features to obtain the imaginary part.\n",
    "\n",
    "    Parameters:\n",
    "    - real_features (torch.Tensor): The input real features.\n",
    "\n",
    "    Returns:\n",
    "    - transformed_imag (torch.Tensor): The transformed imaginary features obtained from the Hilbert transform.\n",
    "    \"\"\"\n",
    "    # Perform FFT\n",
    "    fft_result = torch.fft.fft(real_features, dim=-1)\n",
    "\n",
    "    # Get the number of samples and create a tensor to hold the phase shifts\n",
    "    N = real_features.shape[-1]\n",
    "    phase_shift = torch.zeros_like(fft_result)\n",
    "\n",
    "    # Apply a -90 degree phase shift for positive frequencies (1 to N/2 - 1)\n",
    "    # and a +90 degree phase shift for negative frequencies (N/2 + 1 to N - 1)\n",
    "    if N % 2 == 0:\n",
    "        # Even number of samples\n",
    "        phase_shift[..., 1:N//2] = -1j  # Positive frequencies (excluding Nyquist)\n",
    "        phase_shift[..., N//2+1:] = 1j  # Negative frequencies\n",
    "    else:\n",
    "        # Odd number of samples\n",
    "        phase_shift[..., 1:(N+1)//2] = -1j  # Positive frequencies\n",
    "        phase_shift[..., (N+1)//2:] = 1j   # Negative frequencies\n",
    "\n",
    "    # Apply phase shift and perorm inverse FFT\n",
    "    shifted_fft_result = fft_result * phase_shift\n",
    "    transformed_imag = torch.fft.ifft(shifted_fft_result, dim=-1).real\n",
    "\n",
    "    return transformed_imag\n",
    "\n",
    "\n",
    "# Custom loss function for Analytic Neural Network\n",
    "def custom_loss(outputs, target, real_features, imag_features):\n",
    "    # Implementing hilbert consistency penalty + custom loss function\n",
    "    transformed_imag = fft_based_hilbert_transform(real_features)\n",
    "    consistency_penalty = nn.functional.mse_loss(transformed_imag, imag_features)\n",
    "    beta = 1e-3 # tradeoff parameter\n",
    "\n",
    "    return nn.MSELoss()(outputs, target) + beta*consistency_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c5a73-ab96-4571-9ef4-f7ca6277a59b",
   "metadata": {},
   "source": [
    "## Fully Connected Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4512d2-57a6-4be0-bc19-6d5969ae704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the neural network models\n",
    "# class SteinmetzNetwork(nn.Module):\n",
    "#     def __init__(self, dN, k, lN):\n",
    "#         super(SteinmetzNetwork, self).__init__()\n",
    "#         self.real_net = nn.Sequential(nn.Linear(dN, lN//2), nn.ReLU(), nn.Linear(lN//2, lN//2), nn.ReLU())\n",
    "#         self.imag_net = nn.Sequential(nn.Linear(dN, lN//2), nn.ReLU(), nn.Linear(lN//2, lN//2), nn.ReLU())\n",
    "#         self.regressor = nn.Sequential(nn.Linear(lN, k))\n",
    "\n",
    "#     def forward(self, real, imag):\n",
    "#         real_features = self.real_net(real)\n",
    "#         imag_features = self.imag_net(imag)\n",
    "        \n",
    "#         # Mean centering features as last step before concatenation\n",
    "#         # real_features = real_features - real_features.mean(dim=0, keepdim=True)\n",
    "#         imag_features = imag_features - imag_features.mean(dim=0, keepdim=True)\n",
    "        \n",
    "#         combined = torch.cat((real_features, imag_features), dim=1)\n",
    "#         output = self.regressor(combined)\n",
    "#         return output, real_features, imag_features\n",
    "\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self, dN, k, lN):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.net = nn.Sequential(nn.Linear(2*dN, lN//2), nn.ReLU(), nn.Linear(lN//2, lN), nn.ReLU(), nn.Linear(lN, k))\n",
    "\n",
    "#     def forward(self, real, imag):\n",
    "#         input = torch.cat((real, imag), dim=1)\n",
    "#         output = self.net(input)\n",
    "#         return output\n",
    "\n",
    "# class ComplexNeuralNetwork(nn.Module):\n",
    "#     def __init__(self, dN, k, lN):\n",
    "#         super(ComplexNeuralNetwork, self).__init__()\n",
    "#         self.fc1 = ComplexLinear(dN, lN//2)\n",
    "#         self.fc2 = ComplexLinear(lN//2, lN)\n",
    "#         self.fc3 = ComplexLinear(lN, k)\n",
    "\n",
    "#     def forward(self, real, imag):\n",
    "#         complex_tensor = torch.stack((real, imag), dim=-1)\n",
    "#         x = torch.view_as_complex(complex_tensor)\n",
    "#         x = complex_relu(self.fc1(x))\n",
    "#         x = complex_relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         output = torch.sqrt(torch.real(x)**2 + torch.imag(x)**2)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4b1d5b-db3a-4f6a-8b2e-052ae4f3eb0c",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28088ece-0860-4bd4-937d-556784be51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network models\n",
    "class SteinmetzNetwork(nn.Module):\n",
    "    def __init__(self, channels, height, width, k, lN):\n",
    "        super(SteinmetzNetwork, self).__init__()\n",
    "        self.conv_real1 = nn.Conv2d(channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv_real2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv_imag1 = nn.Conv2d(channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv_imag2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        flattened_size = 64 * (height // 4) * (width // 4)\n",
    "        self.fc1 = nn.Linear(flattened_size*2, lN)\n",
    "        self.fc2 = nn.Linear(lN, k)\n",
    "        \n",
    "    def forward(self, real, imag):\n",
    "        real = F.avg_pool2d(F.relu(self.conv_real1(real)), 2, 2)\n",
    "        real = F.avg_pool2d(F.relu(self.conv_real2(real)), 2, 2)\n",
    "        real = torch.flatten(real, 1)\n",
    "        imag = F.avg_pool2d(F.relu(self.conv_imag1(imag)), 2, 2)\n",
    "        imag = F.avg_pool2d(F.relu(self.conv_imag2(imag)), 2, 2)\n",
    "        imag = torch.flatten(imag, 1)\n",
    "        imag = imag - imag.mean(dim=0, keepdim=True)\n",
    "        x = torch.cat((real, imag), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        return output, real, imag\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, channels, height, width, k, lN):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels * 2, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        flattened_size = 64 * (height // 4) * (width // 4)\n",
    "        self.fc1 = nn.Linear(flattened_size, lN)\n",
    "        self.fc2 = nn.Linear(lN, k)\n",
    "        \n",
    "    def forward(self, real, imag):\n",
    "        x = torch.cat((real, imag), dim=1)\n",
    "        x = F.avg_pool2d(F.relu(self.conv1(x)), 2, 2)\n",
    "        x = F.avg_pool2d(F.relu(self.conv2(x)), 2, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "\n",
    "class ComplexNeuralNetwork(nn.Module):\n",
    "    def __init__(self, channels, height, width, k, lN):\n",
    "        super(ComplexNeuralNetwork, self).__init__()\n",
    "        self.conv1 = ComplexConv2d(in_channels=channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = ComplexConv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        flattened_size = 64 * (height // 4) * (width // 4)\n",
    "        self.fc1 = ComplexLinear(flattened_size, lN)\n",
    "        self.fc2 = ComplexLinear(lN, k)\n",
    "        \n",
    "    def forward(self, real, imag):\n",
    "        complex_tensor = torch.view_as_complex(torch.stack((real, imag), dim=-1))\n",
    "        x = complex_avg_pool2d(complex_relu(self.conv1(complex_tensor)), 2, 2)\n",
    "        x = complex_avg_pool2d(complex_relu(self.conv2(x)), 2, 2)\n",
    "        x = x.view(-1,64*5*4)\n",
    "        x = complex_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        output = torch.sqrt(torch.real(x)**2 + torch.imag(x)**2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd831db5-168f-4f20-865b-d74d9292ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function, and optimizer\n",
    "epochs = 70\n",
    "iterations = 5\n",
    "channels = 5; height = 21; width = 16\n",
    "dN = channels * height * width\n",
    "k = 2\n",
    "lN = 128  # Latent Dimensionality\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Move model to GPU if available\n",
    "train_error_rvnn = np.zeros((iterations, epochs))\n",
    "train_error_cvnn = np.zeros((iterations, epochs))\n",
    "train_error_steinmetz = np.zeros((iterations, epochs))\n",
    "train_error_analytic = np.zeros((iterations, epochs))\n",
    "test_error_rvnn = np.zeros((iterations, epochs))\n",
    "test_error_cvnn = np.zeros((iterations, epochs))\n",
    "test_error_steinmetz = np.zeros((iterations, epochs))\n",
    "test_error_analytic = np.zeros((iterations, epochs))\n",
    "\n",
    "# Initialize the model and other components as before\n",
    "for iter in range(iterations):\n",
    "    # Initialize models and optimizers\n",
    "    model_rvnn = NeuralNetwork(channels=channels, height=height, width=width, k=k, lN=lN).to(device)\n",
    "    model_cvnn = ComplexNeuralNetwork(channels=channels, height=height, width=width, k=k, lN=lN).to(device)\n",
    "    model_steinmetz = SteinmetzNetwork(channels=channels, height=height, width=width, k=k, lN=lN).to(device)\n",
    "    model_analytic = SteinmetzNetwork(channels=channels, height=height, width=width, k=k, lN=lN).to(device)\n",
    "\n",
    "    optimizer_rvnn = optim.Adam(model_rvnn.parameters(), lr=5e-2)\n",
    "    optimizer_cvnn = optim.Adam(model_cvnn.parameters(), lr=5e-2)\n",
    "    optimizer_steinmetz = optim.Adam(model_steinmetz.parameters(), lr=5e-2)\n",
    "    optimizer_analytic = optim.Adam(model_analytic.parameters(), lr=5e-2)\n",
    "\n",
    "    print(f'RVNN params: {sum(p.numel() for p in model_rvnn.parameters())}, \\\n",
    "        CVNN params: {sum(p.numel() for p in model_cvnn.parameters())}, \\\n",
    "        Steinmetz params: {sum(p.numel() for p in model_steinmetz.parameters())}, \\\n",
    "        Analytic params: {sum(p.numel() for p in model_analytic.parameters())}')\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        # Set models to training mode\n",
    "        model_rvnn.train(); model_cvnn.train(); model_steinmetz.train(); model_analytic.train()\n",
    "        train_distances_rvnn, train_distances_cvnn = [], []\n",
    "        train_distances_steinmetz, train_distances_analytic = [], []\n",
    "\n",
    "        for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs_rvnn = model_rvnn(features[:,:channels], features[:,channels:])\n",
    "            outputs_cvnn = model_cvnn(features[:,:channels], features[:,channels:])\n",
    "            outputs_steinmetz, _, _ = model_steinmetz(features[:,:channels], features[:,channels:])\n",
    "            outputs_analytic, real_feat, imag_feat = model_analytic(features[:,:channels], features[:,channels:])\n",
    "\n",
    "            # Neural network only predicts range and azimuth\n",
    "            outputs_rvnn = torch.cat((outputs_rvnn, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "            outputs_cvnn = torch.cat((outputs_cvnn, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "            outputs_steinmetz = torch.cat((outputs_steinmetz, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "            outputs_analytic = torch.cat((outputs_analytic, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "            # Transform outputs and labels to Cartesian coordinates\n",
    "            pred_cart_rvnn = Spher2Cart_1D_torch(outputs_rvnn, scale_tr, coord_tr)\n",
    "            pred_cart_cvnn = Spher2Cart_1D_torch(outputs_cvnn, scale_tr, coord_tr)\n",
    "            pred_cart_steinmetz = Spher2Cart_1D_torch(outputs_steinmetz, scale_tr, coord_tr)\n",
    "            pred_cart_analytic = Spher2Cart_1D_torch(outputs_analytic, scale_tr, coord_tr)\n",
    "\n",
    "            labels_cart = Spher2Cart_1D_torch(labels, scale_tr, coord_tr)\n",
    "\n",
    "            # Compute loss between transformed coordinates\n",
    "            loss_rvnn = criterion(pred_cart_rvnn, labels_cart)\n",
    "            loss_cvnn = criterion(pred_cart_cvnn, labels_cart)\n",
    "            loss_steinmetz = criterion(pred_cart_steinmetz, labels_cart)\n",
    "            loss_analytic = custom_loss(pred_cart_analytic, labels_cart, real_feat, imag_feat)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer_rvnn.zero_grad(); optimizer_cvnn.zero_grad(); optimizer_steinmetz.zero_grad(); optimizer_analytic.zero_grad()\n",
    "            loss_rvnn.backward(); loss_cvnn.backward(); loss_steinmetz.backward(); loss_analytic.backward()\n",
    "            optimizer_rvnn.step(); optimizer_cvnn.step(); optimizer_steinmetz.step(); optimizer_analytic.step()\n",
    "\n",
    "            # Compute and store Euclidean distances\n",
    "            distance_rvnn = average_euclidean_distance(pred_cart_rvnn, labels_cart)\n",
    "            distance_cvnn = average_euclidean_distance(pred_cart_cvnn, labels_cart)\n",
    "            distance_steinmetz = average_euclidean_distance(pred_cart_steinmetz, labels_cart)\n",
    "            distance_analytic = average_euclidean_distance(pred_cart_analytic, labels_cart)\n",
    "\n",
    "            train_distances_rvnn.append(distance_rvnn)\n",
    "            train_distances_cvnn.append(distance_cvnn)\n",
    "            train_distances_steinmetz.append(distance_steinmetz)\n",
    "            train_distances_analytic.append(distance_analytic)\n",
    "\n",
    "        # Evaluation on Test Set\n",
    "        model_rvnn.eval(); model_cvnn.eval(); model_steinmetz.eval(); model_analytic.eval()\n",
    "        test_distances_rvnn, test_distances_cvnn = [], []\n",
    "        test_distances_steinmetz, test_distances_analytic = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs_rvnn = model_rvnn(features[:,:channels], features[:,channels:])\n",
    "                outputs_cvnn = model_cvnn(features[:,:channels], features[:,channels:])\n",
    "                outputs_steinmetz, _, _ = model_steinmetz(features[:,:channels], features[:,channels:])\n",
    "                outputs_analytic, _, _ = model_analytic(features[:,:channels], features[:,channels:])\n",
    "\n",
    "                # Neural network only predicts range and azimuth\n",
    "                outputs_rvnn = torch.cat((outputs_rvnn, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "                outputs_cvnn = torch.cat((outputs_cvnn, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "                outputs_steinmetz = torch.cat((outputs_steinmetz, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "                outputs_analytic = torch.cat((outputs_analytic, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "                # Transform outputs and labels to Cartesian coordinates\n",
    "                pred_cart_rvnn = Spher2Cart_1D_torch(outputs_rvnn, scale_ts, coord_ts)\n",
    "                pred_cart_cvnn = Spher2Cart_1D_torch(outputs_cvnn, scale_ts, coord_ts)\n",
    "                pred_cart_steinmetz = Spher2Cart_1D_torch(outputs_steinmetz, scale_ts, coord_ts)\n",
    "                pred_cart_analytic = Spher2Cart_1D_torch(outputs_analytic, scale_ts, coord_ts)\n",
    "\n",
    "                labels_cart = Spher2Cart_1D_torch(labels, scale_ts, coord_ts)\n",
    "\n",
    "                # Compute loss between transformed coordinates\n",
    "                loss_rvnn = criterion(pred_cart_rvnn, labels_cart)\n",
    "                loss_cvnn = criterion(pred_cart_cvnn, labels_cart)\n",
    "                loss_steinmetz = criterion(pred_cart_steinmetz, labels_cart)\n",
    "                loss_analytic = criterion(pred_cart_analytic, labels_cart)\n",
    "\n",
    "                # Compute and store Euclidean distances\n",
    "                distance_rvnn = average_euclidean_distance(pred_cart_rvnn, labels_cart)\n",
    "                distance_cvnn = average_euclidean_distance(pred_cart_cvnn, labels_cart)\n",
    "                distance_steinmetz = average_euclidean_distance(pred_cart_steinmetz, labels_cart)\n",
    "                distance_analytic = average_euclidean_distance(pred_cart_analytic, labels_cart)\n",
    "\n",
    "                test_distances_rvnn.append(distance_rvnn)\n",
    "                test_distances_cvnn.append(distance_cvnn)\n",
    "                test_distances_steinmetz.append(distance_steinmetz)\n",
    "                test_distances_analytic.append(distance_analytic)\n",
    "\n",
    "        # Aggregate and store Average Euclidean Distances\n",
    "        train_error_rvnn[iter, epoch] = np.mean(train_distances_rvnn)\n",
    "        train_error_cvnn[iter, epoch] = np.mean(train_distances_cvnn)\n",
    "        train_error_steinmetz[iter, epoch] = np.mean(train_distances_steinmetz)\n",
    "        train_error_analytic[iter, epoch] = np.mean(train_distances_analytic)\n",
    "\n",
    "        test_error_rvnn[iter, epoch] = np.mean(test_distances_rvnn)\n",
    "        test_error_cvnn[iter, epoch] = np.mean(test_distances_cvnn)\n",
    "        test_error_steinmetz[iter, epoch] = np.mean(test_distances_steinmetz)\n",
    "        test_error_analytic[iter, epoch] = np.mean(test_distances_analytic)\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Iteration [{iter}/{iterations}], Epoch [{epoch}/{epochs}], RVNN MSE: {test_error_rvnn[iter,epoch]:.4f}, CVNN MSE: {test_error_cvnn[iter,epoch]:.4f}, Steinmetz MSE: {test_error_steinmetz[iter,epoch]:.4f}, Analytic MSE: {test_error_analytic[iter,epoch]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fe59d-07f5-48bd-ba32-56d8766a2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute means and 95% confidence intervals\n",
    "def compute_mean_and_CI(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std_error = np.std(data, axis=0) / np.sqrt(data.shape[0])  # Standard error of the mean\n",
    "    stdev = np.std(data, axis=0)\n",
    "    ci = 1.96 * std_error  # 95% CI for a normal distribution\n",
    "    return mean, ci, stdev\n",
    "\n",
    "X = test_error_rvnn[:,:]; Y = test_error_cvnn[:,:]\n",
    "Z = test_error_steinmetz[:,:]; A = test_error_analytic[:,:];\n",
    "mean_X, ci_X, stdev_X = compute_mean_and_CI(X)\n",
    "mean_Y, ci_Y, stdev_Y = compute_mean_and_CI(Y)\n",
    "mean_Z, ci_Z, stdev_Z = compute_mean_and_CI(Z)\n",
    "mean_A, ci_A, stdev_A = compute_mean_and_CI(A)\n",
    "\n",
    "print(mean_X[-1], mean_Y[-1], mean_Z[-1], mean_A[-1])\n",
    "print(stdev_X[-1], stdev_Y[-1], stdev_Z[-1], stdev_A[-1])\n",
    "\n",
    "# Step 3: Plot the results\n",
    "epochs_all = list(range(1, X.shape[1] + 1))\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "# Plotting for X\n",
    "plt.fill_between(epochs_all, mean_X - ci_X, mean_X + ci_X, color='blue', alpha=0.1, zorder=0)\n",
    "plt.plot(epochs_all, mean_X, 'b-', label=\"RVNN\", zorder=20)\n",
    "\n",
    "# Plotting for Y\n",
    "plt.fill_between(epochs_all, mean_Y - ci_Y, mean_Y + ci_Y, color='red', alpha=0.2, zorder=5)\n",
    "plt.plot(epochs_all, mean_Y, 'r-', label=\"CVNN\", zorder=25)\n",
    "\n",
    "# Plotting for Z\n",
    "plt.fill_between(epochs_all, mean_Z - ci_Z, mean_Z + ci_Z, color='orange', alpha=0.3, zorder=10)\n",
    "plt.plot(epochs_all, mean_Z, '-', color='orange', label=\"Steinmetz Neural Network\", zorder=30)\n",
    "\n",
    "# Plotting for A\n",
    "plt.fill_between(epochs_all, mean_A - ci_A, mean_A + ci_A, color='green', alpha=0.3, zorder=15)\n",
    "plt.plot(epochs_all, mean_A, 'g-', label=\"Analytic Neural Network\", zorder=35)\n",
    "\n",
    "# Additional plot settings\n",
    "plt.xlabel(\"Number of Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Average Euclidean Distance (m)\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.yscale('linear')\n",
    "plt.legend(prop={'size': 15},loc='center right',framealpha=0.7).set_zorder(50)\n",
    "plt.grid(True)\n",
    "plt.savefig(f'Results/RASPNet_{SCENARIO}_epochs.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "np.savetxt(f'Results/test_error_steinmetz.csv', test_error_steinmetz, delimiter=',')\n",
    "np.savetxt(f'Results/test_error_analytic.csv', test_error_analytic, delimiter=',')\n",
    "np.savetxt(f'Results/test_error_rvnn.csv', test_error_rvnn, delimiter=',')\n",
    "np.savetxt(f'Results/test_error_cvnn.csv', test_error_cvnn, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff534c-65f8-499c-8928-c57aad0df950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
