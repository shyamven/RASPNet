{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regression Network on Radar Dataset (Grand Canyon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Training labels: \n",
      "[[13.367  21.915   3.2009]\n",
      " [ 9.0069 21.278   3.8253]\n",
      " [11.133  21.523   4.6521]\n",
      " ...\n",
      " [20.181  24.18    2.4438]\n",
      " [ 5.8702 21.203   3.4267]\n",
      " [12.288  21.517   4.2458]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 35 # Grand Canyon\n",
    "names = os.listdir(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/')\n",
    "names = sorted(names, key = index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'data/EXAMPLES/num{scenario_idx}_Ground_Truth_25k.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "y_train = y[:int(0.9*len(names))]\n",
    "y_test = y[(int(0.9*len(names))+1):]\n",
    "training_names = names[:int(0.9*len(names))]\n",
    "test_names = names[(int(0.9*len(names))+1):]\n",
    "\n",
    "print('Training labels: ')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.45], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [11381, 215, -0.95] # Tensor corner\n",
    "rng_res_tr = 59.9585/2         # Range resolution\n",
    "az_step_tr = 0.4               # Azimuth step size\n",
    "el_step_tr = 0.01              # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11381, 215, -0.95] # Tensor corner\n",
    "rng_res_ts = 59.9585/2         # Range resolution\n",
    "az_step_ts = 0.4               # Azimuth step size\n",
    "el_step_ts = 0.01              # Elevation step size\n",
    "\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, normalize = True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind*bs, (ind+1)*bs):\n",
    "        try: temp = sio.loadmat(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/'+names[j])['P']\n",
    "        except: break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j,:])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500\n",
      "2499\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "#         self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "#         self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "#         self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(self.batchnorm1(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(self.batchnorm2(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "#         return output_reg\n",
    "    \n",
    "# from torchsummary import summary\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# model = Net()\n",
    "# model = model.to(device)\n",
    "# if device == 'cuda:0':\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "#     cudnn.benchmark = True\n",
    "# print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CART (Convolutional Adaptive Radar Transformer) and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 21,\n",
    "        seq_len: int = 26,\n",
    "        conv_channels: list = [32, 64],\n",
    "        kernel_size: int = 3,\n",
    "        pool_size: int = 2,\n",
    "        d_model: int = 64,\n",
    "        nhead: int = 4,\n",
    "        num_layers: int = 2,\n",
    "        dim_feedforward: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "        num_outputs: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # --- Local feature extractor (1D CNN) ---\n",
    "        self.conv1 = nn.Conv1d(in_channels, conv_channels[0], kernel_size, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm1d(conv_channels[0])\n",
    "        self.conv2 = nn.Conv1d(conv_channels[0], conv_channels[1], kernel_size, padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm1d(conv_channels[1])\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "\n",
    "        # compute reduced length after two poolings\n",
    "        reduced_len = seq_len // (pool_size**2)\n",
    "\n",
    "        # --- Token projection to transformer dimension ---\n",
    "        self.token_proj = nn.Linear(conv_channels[1], d_model)\n",
    "\n",
    "        # CLS token & positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, reduced_len + 1, d_model))\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, num_outputs)\n",
    "        )\n",
    "\n",
    "        # Initialize\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (batch, channels=in_channels, length=seq_len)\n",
    "        # 1) CNN blocks\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        # x: (batch, conv_channels[1], reduced_len)\n",
    "\n",
    "        # 2) prepare transformer tokens\n",
    "        x = x.permute(0, 2, 1)  # -> (batch, seq', channels)\n",
    "        x = self.token_proj(x)  # -> (batch, seq', d_model)\n",
    "\n",
    "        # prepend CLS token\n",
    "        bs = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(bs, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (batch, seq'+1, d_model)\n",
    "\n",
    "        # add positional embeddings\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # 3) Transformer\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # 4) classification head on CLS\n",
    "        cls_rep = x[:, 0]\n",
    "        out = self.mlp_head(cls_rep)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 20  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8400.27757915  8108.0750387   -184.78534814]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8383.52018487  8130.27230727  -187.19911291]\n",
      "Train Loss: 33.514321 ---- Test Loss: 11.273361\n",
      "Epoch 1/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8419.13348785  8095.65818446  -184.86392361]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8382.21611759  8134.73537293  -187.23392392]\n",
      "Train Loss: 11.478097 ---- Test Loss: 11.278524\n",
      "Epoch 2/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8402.67354598  8123.58225471  -184.98314782]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8382.89089141  8136.06718023  -187.25655374]\n",
      "Train Loss: 11.480702 ---- Test Loss: 11.258784\n",
      "Epoch 3/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8398.01589807  8120.76309379  -184.89913517]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8379.97063089  8145.2645784   -187.3256937 ]\n",
      "Train Loss: 11.481372 ---- Test Loss: 11.301373\n",
      "Epoch 4/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8417.93907791  8094.98537525  -184.84291604]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8388.38044507  8135.66290298  -187.31519651]\n",
      "Train Loss: 11.468509 ---- Test Loss: 11.246632\n",
      "Epoch 5/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8418.1922286   8089.88089183  -184.78981434]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8381.74823842  8145.37006894  -187.34730578]\n",
      "Train Loss: 11.469219 ---- Test Loss: 11.300784\n",
      "Epoch 6/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8421.52942115  8085.33742313  -184.77809163]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8383.31418225  8142.54799617  -187.33378746]\n",
      "Train Loss: 11.470987 ---- Test Loss: 11.282406\n",
      "Epoch 7/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8418.40585142  8085.45437544  -184.74371441]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8385.68196234  8137.13883082  -187.30062334]\n",
      "Train Loss: 11.470188 ---- Test Loss: 11.232734\n",
      "Epoch 8/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8420.11551614  8094.78722698  -184.86557372]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8385.12383647  8140.48213438  -187.33152883]\n",
      "Train Loss: 11.460125 ---- Test Loss: 11.262668\n",
      "Epoch 9/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8407.42438602  8106.80835432  -184.85283616]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8391.68630384  8132.75484785  -187.32079519]\n",
      "Train Loss: 11.467055 ---- Test Loss: 11.220011\n",
      "Epoch 10/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8399.22231055  8109.50909742  -184.78909584]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8386.36331424  8139.17496163  -187.33119112]\n",
      "Train Loss: 11.464821 ---- Test Loss: 11.247389\n",
      "Epoch 11/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8402.49165149  8126.02709277  -185.00797639]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8384.38806808  8143.73727874  -187.35941778]\n",
      "Train Loss: 11.465407 ---- Test Loss: 11.287507\n",
      "Epoch 12/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8406.27417708  8108.40092287  -184.85722997]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8390.03662458  8136.24243731  -187.34072144]\n",
      "Train Loss: 11.468295 ---- Test Loss: 11.243790\n",
      "Epoch 13/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8424.17254     8081.99562931  -184.77165   ]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8386.77888348  8141.48345888  -187.36174423]\n",
      "Train Loss: 11.462024 ---- Test Loss: 11.258822\n",
      "Epoch 14/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8403.94551173  8111.32767763  -184.8628712 ]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8388.55599598  8139.82900232  -187.36371555]\n",
      "Train Loss: 11.465315 ---- Test Loss: 11.255115\n",
      "Epoch 15/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8395.87610337  8122.27527873  -184.89143073]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8389.93172545  8137.17590328  -187.34993145]\n",
      "Train Loss: 11.462769 ---- Test Loss: 11.235559\n",
      "Epoch 16/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8407.02442161  8112.12090522  -184.906655  ]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8391.85277011  8134.98135688  -187.34755093]\n",
      "Train Loss: 11.461653 ---- Test Loss: 11.225479\n",
      "Epoch 17/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8406.45362945  8106.00386264  -184.83293755]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8388.60066723  8140.50239781  -187.37174653]\n",
      "Train Loss: 11.459188 ---- Test Loss: 11.255877\n",
      "Epoch 18/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8410.99153781  8100.88060256  -184.82838629]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8388.42458401  8140.24064685  -187.36679899]\n",
      "Train Loss: 11.465098 ---- Test Loss: 11.260746\n",
      "Epoch 19/20\n",
      "----------\n",
      "[-8438.4946267   8154.65388483  -185.73258569]\n",
      "[-8404.59906284  8110.23577263  -184.85831315]\n",
      "[-8451.09155208  7987.77764771  -186.40186841]\n",
      "[-8390.70804382  8137.32715931  -187.36055237]\n",
      "Train Loss: 11.463058 ---- Test Loss: 11.246234\n",
      "1886.5972719192505\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20      # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-'*10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names)//bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:,0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names)//bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:,0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "\n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "        \n",
    "        epoch_loss_train = running_loss_train*x_train.size()[0]/len(training_names)\n",
    "        epoch_loss_test = running_loss_test*x_test.size()[0]/len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch%5==0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 1.425280182110686\n",
      "Localization Error (m) = 180.1413887216811\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical),3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    cartesian[:,0] = np.multiply(np.cos(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,1] = np.multiply(-np.sin(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,2] = np.multiply(np.sin(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range,az,el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test),3))\n",
    "labels_test_reg = np.zeros((len(y_test),3))\n",
    "\n",
    "for i in range(0, len(y_test)//bs):  \n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, True)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs*i : bs*i + bs] = (labels_test[:,0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs*i : bs*i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs*i : bs*i + bs, 2] = labels_test_reg[bs*i : bs*i + bs, 2]\n",
    "    \n",
    "azim_tot = 0\n",
    "for i in np.arange(0,len(out_test_reg),1):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i,1] - labels_test_reg[i,1])\n",
    "    \n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "\n",
    "sum_tot = 0\n",
    "for i in np.arange(0,len(new_data) - (len(new_data) % bs),1):\n",
    "    sum_tot += np.linalg.norm(new_data[i,:] - new_labels_data[i,:])\n",
    "    \n",
    "# def reject_outliers(data, m=2):\n",
    "#     return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "# \n",
    "# azims = (np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) - np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]))[:,1]\n",
    "# azims = reject_outliers(azims)\n",
    "# mse = np.mean(azims**2)\n",
    "# bias_sq = np.mean(azims)**2\n",
    "# var = np.var(azims)\n",
    "# print(mse, bias_sq, var)\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
