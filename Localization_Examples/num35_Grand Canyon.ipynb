{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regression Network on Radar Dataset (Grand Canyon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Training labels: \n",
      "[[13.367  21.915   3.2009]\n",
      " [ 9.0069 21.278   3.8253]\n",
      " [11.133  21.523   4.6521]\n",
      " ...\n",
      " [19.973  24.298   2.3648]\n",
      " [ 6.4552 24.221   5.9275]\n",
      " [ 5.6511 22.427   5.2918]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 35 # Grand Canyon\n",
    "names = os.listdir(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names = sorted(names, key = index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'num{scenario_idx}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "y_train = y[:int(0.9*len(names))]\n",
    "y_test = y[(int(0.9*len(names))+1):]\n",
    "training_names = names[:int(0.9*len(names))]\n",
    "test_names = names[(int(0.9*len(names))+1):]\n",
    "\n",
    "print('Training labels: ')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.45], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [11381, 215, -0.95] # Tensor corner\n",
    "rng_res_tr = 59.9585/2         # Range resolution\n",
    "az_step_tr = 0.4               # Azimuth step size\n",
    "el_step_tr = 0.01              # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11381, 215, -0.95] # Tensor corner\n",
    "rng_res_ts = 59.9585/2         # Range resolution\n",
    "az_step_ts = 0.4               # Azimuth step size\n",
    "el_step_ts = 0.01              # Elevation step size\n",
    "\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, normalize = True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind*bs, (ind+1)*bs):\n",
    "        try: temp = sio.loadmat(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/'+names[j])['P']\n",
    "        except: break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j,:])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 32, 24]           2,048\n",
      "       BatchNorm1d-2               [-1, 32, 24]              64\n",
      "            Conv1d-3               [-1, 64, 10]           6,208\n",
      "       BatchNorm1d-4               [-1, 64, 10]             128\n",
      "            Linear-5                   [-1, 20]           6,420\n",
      "            Linear-6                    [-1, 2]              42\n",
      "               Net-7                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 14,910\n",
      "Trainable params: 14,910\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "        self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "        return output_reg\n",
    "    \n",
    "from torchsummary import summary\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_input = torch.randn(1,5,26,dtype=torch.float).to(device)\n",
    "# starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "# repetitions = 10000\n",
    "# timings=np.zeros((repetitions,1))\n",
    "# #GPU-WARM-UP\n",
    "# for _ in range(10):\n",
    "#    _ = model(dummy_input)\n",
    "# # MEASURE PERFORMANCE\n",
    "# with torch.no_grad():\n",
    "#     for rep in range(repetitions):\n",
    "#         starter.record()\n",
    "#         _ = model(dummy_input)\n",
    "#         ender.record()\n",
    "#         # WAIT FOR GPU SYNC\n",
    "#         torch.cuda.synchronize()\n",
    "#         curr_time = starter.elapsed_time(ender)\n",
    "#         timings[rep] = curr_time\n",
    "# mean_syn = np.sum(timings) / repetitions\n",
    "# std_syn = np.std(timings)\n",
    "# print(mean_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 20  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8398.14865427  8126.83359139  -181.55075593]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8456.81075344  8030.68838163  -190.63884485]\n",
      "Train Loss: 31.482071 ---- Test Loss: 13.339676\n",
      "Epoch 1/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8400.45257875  8118.74247751  -181.48910357]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8462.4467245   8020.82809959  -190.59474639]\n",
      "Train Loss: 11.569948 ---- Test Loss: 13.916877\n",
      "Epoch 2/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8399.21282376  8119.25785657  -181.48081975]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8414.13931416  8084.46702605  -190.74165916]\n",
      "Train Loss: 11.536996 ---- Test Loss: 14.179494\n",
      "Epoch 3/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8399.70078974  8117.69812935  -181.46943078]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8427.53403579  8068.29587866  -190.71670804]\n",
      "Train Loss: 11.533243 ---- Test Loss: 14.530443\n",
      "Epoch 4/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8401.147875    8114.27036762  -181.44859843]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8409.09745061  8093.98038721  -190.79004784]\n",
      "Train Loss: 11.530924 ---- Test Loss: 15.031218\n",
      "Epoch 5/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8408.83961087  8106.04532045  -181.44586188]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8395.58490409  8112.58044155  -190.84211925]\n",
      "Train Loss: 11.525394 ---- Test Loss: 14.392076\n",
      "Epoch 6/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8401.18306231  8114.2345122   -181.44860465]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8391.86561769  8116.41742926  -190.84200243]\n",
      "Train Loss: 11.524399 ---- Test Loss: 13.676053\n",
      "Epoch 7/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8407.44613348  8106.87750147  -181.43925055]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8400.97348375  8103.1945544   -190.79892641]\n",
      "Train Loss: 11.521725 ---- Test Loss: 13.697562\n",
      "Epoch 8/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8407.0226803   8107.61379913  -181.44245518]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8386.55509378  8126.16396782  -190.89044671]\n",
      "Train Loss: 11.521608 ---- Test Loss: 14.108698\n",
      "Epoch 9/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8398.89416739  8116.13759114  -181.44357298]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8379.81238755  8132.78917116  -190.88671491]\n",
      "Train Loss: 11.524653 ---- Test Loss: 13.867303\n",
      "Epoch 10/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8403.75188078  8111.6351347   -181.44926414]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8375.30568507  8141.66599643  -190.93497181]\n",
      "Train Loss: 11.526187 ---- Test Loss: 13.720236\n",
      "Epoch 11/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8406.44603852  8108.87786225  -181.44963985]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8380.8957545   8137.76751709  -190.95610514]\n",
      "Train Loss: 11.524902 ---- Test Loss: 14.085764\n",
      "Epoch 12/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8398.99529281  8116.56362421  -181.44930189]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8366.37632232  8156.13869274  -190.99540637]\n",
      "Train Loss: 11.522134 ---- Test Loss: 13.659711\n",
      "Epoch 13/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8407.37606471  8107.30360468  -181.44306182]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8383.42535445  8134.56179238  -190.94927799]\n",
      "Train Loss: 11.517489 ---- Test Loss: 13.875025\n",
      "Epoch 14/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8405.00362932  8110.0160388   -181.44578994]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8359.85844773  8167.17751703  -191.04518524]\n",
      "Train Loss: 11.517009 ---- Test Loss: 13.605285\n",
      "Epoch 15/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8401.29430168  8113.43146318  -181.44118105]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8369.21597631  8152.98615416  -190.99268392]\n",
      "Train Loss: 11.514518 ---- Test Loss: 13.782297\n",
      "Epoch 16/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8400.60896935  8114.26165517  -181.44248271]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8354.03106686  8173.62423094  -191.05074229]\n",
      "Train Loss: 11.513503 ---- Test Loss: 13.716890\n",
      "Epoch 17/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8412.62497447  8101.00718354  -181.43390534]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8377.5170757   8144.58055078  -190.99410075]\n",
      "Train Loss: 11.513020 ---- Test Loss: 14.051232\n",
      "Epoch 18/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8406.6426657   8107.31904768  -181.43502714]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8371.4617659   8152.68709955  -191.015571  ]\n",
      "Train Loss: 11.508175 ---- Test Loss: 13.866599\n",
      "Epoch 19/20\n",
      "----------\n",
      "[-8300.09505833  8161.49402752  -180.83632894]\n",
      "[-8406.38183634  8107.52575173  -181.43433967]\n",
      "[-8389.76299523  8386.6006751   -193.91412799]\n",
      "[-8389.47749758  8127.35356516  -190.93828725]\n",
      "Train Loss: 11.509799 ---- Test Loss: 13.993163\n",
      "1190.988319158554\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20      # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-'*10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names)//bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:,0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names)//bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:,0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "\n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "        \n",
    "        epoch_loss_train = running_loss_train*x_train.size()[0]/len(training_names)\n",
    "        epoch_loss_test = running_loss_test*x_test.size()[0]/len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch%5==0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_accuracy = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 2.0671823791748074\n",
      "Localization Error (m) = 220.44750662960487\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical),3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    cartesian[:,0] = np.multiply(np.cos(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,1] = np.multiply(-np.sin(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,2] = np.multiply(np.sin(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range,az,el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test),3))\n",
    "labels_test_reg = np.zeros((len(y_test),3))\n",
    "\n",
    "for i in range(0, len(y_test)//bs):  \n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, True)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs*i : bs*i + bs] = (labels_test[:,0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs*i : bs*i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs*i : bs*i + bs, 2] = labels_test_reg[bs*i : bs*i + bs, 2]\n",
    "    \n",
    "azim_tot = 0\n",
    "for i in np.arange(0,len(out_test_reg),1):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i,1] - labels_test_reg[i,1])\n",
    "    \n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "\n",
    "sum_tot = 0\n",
    "for i in np.arange(0,len(new_data) - (len(new_data) % bs),1):\n",
    "    sum_tot += np.linalg.norm(new_data[i,:] - new_labels_data[i,:])\n",
    "    \n",
    "# def reject_outliers(data, m=2):\n",
    "#     return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "# \n",
    "# azims = (np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) - np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]))[:,1]\n",
    "# azims = reject_outliers(azims)\n",
    "# mse = np.mean(azims**2)\n",
    "# bias_sq = np.mean(azims)**2\n",
    "# var = np.var(azims)\n",
    "# print(mse, bias_sq, var)\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
