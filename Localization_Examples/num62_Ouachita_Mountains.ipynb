{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Regression Network on Radar Dataset (Ouachita Mountains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "Training labels: \n",
      "[[ 0.19278 19.175    5.3732 ]\n",
      " [11.244   23.269   24.522  ]\n",
      " [-0.2566  20.21     9.9473 ]\n",
      " ...\n",
      " [ 7.7966  22.974   29.859  ]\n",
      " [15.579   24.133   17.291  ]\n",
      " [ 6.6286  23.169   29.05   ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 62 # Ouachita Mountains\n",
    "names = os.listdir(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/')\n",
    "names = sorted(names, key = index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'data/EXAMPLES/num{scenario_idx}_Ground_Truth_25k.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "y_train = y[:int(0.8*len(names))]\n",
    "y_test = y[(int(0.8*len(names))+1):]\n",
    "training_names = names[:int(0.8*len(names))]\n",
    "test_names = names[(int(0.8*len(names))+1):]\n",
    "\n",
    "print('Training labels: ')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.45], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [11471, 215, -5.6] # Tensor corner\n",
    "rng_res_tr = 59.9585/2         # Range resolution\n",
    "az_step_tr = 0.4               # Azimuth step size\n",
    "el_step_tr = 0.01              # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11471, 215, -5.6] # Tensor corner\n",
    "rng_res_ts = 59.9585/2         # Range resolution\n",
    "az_step_ts = 0.4               # Azimuth step size\n",
    "el_step_ts = 0.01              # Elevation step size\n",
    "\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, normalize = True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind*bs, (ind+1)*bs):\n",
    "        try: temp = sio.loadmat(f'data/EXAMPLES/num{scenario_idx}_NAMF_DATA_25k/'+names[j])['P']\n",
    "        except: break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j,:])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "4999\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "#         self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "#         self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "#         self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(self.batchnorm1(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(self.batchnorm2(x))\n",
    "#         x = F.max_pool1d(x, 2)\n",
    "        \n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "#         return output_reg\n",
    "    \n",
    "# from torchsummary import summary\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# model = Net()\n",
    "# model = model.to(device)\n",
    "# if device == 'cuda:0':\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "#     cudnn.benchmark = True\n",
    "# print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CART (Convolutional Adaptive Radar Transformer) and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 21,\n",
    "        seq_len: int = 26,\n",
    "        d_model: int = 64,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dim_feedforward: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        num_outputs: int = 2\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # 1) Patch embedding: each of the seq_len timeâ€‘steps is a token of dim in_features\n",
    "        self.patch_embed = nn.Linear(in_features, d_model)\n",
    "\n",
    "        # 2) CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "\n",
    "        # 3) Positional embeddings for (seq_len + 1) tokens\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, seq_len + 1, d_model))\n",
    "\n",
    "        # 4) Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # so input shape is (batch, seq, d_model)\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # 5) Classification / regression head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model // 2, num_outputs)\n",
    "        )\n",
    "\n",
    "        # initialize\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (batch_size, in_features=21, seq_len=26)\n",
    "        returns: (batch_size, num_outputs=2)\n",
    "        \"\"\"\n",
    "        bs = x.size(0)\n",
    "        # reshape to (batch, seq_len, in_features)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # patch embedding -> (batch, seq_len, d_model)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # prepend cls token -> (batch, seq_len+1, d_model)\n",
    "        cls_tokens = self.cls_token.expand(bs, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # take CLS representation\n",
    "        cls_rep = x[:, 0]  # (batch, d_model)\n",
    "\n",
    "        # head to outputs\n",
    "        out = self.mlp_head(cls_rep)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 50  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8329.13596781  7857.47363627 -1063.07111602]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8317.04118405  7836.74176966 -1062.07233825]\n",
      "Train Loss: 49.810909 ---- Test Loss: 7.793518\n",
      "Epoch 1/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8230.65418693  7957.79446704 -1062.89176568]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8219.84888649  7949.78776706 -1062.79327137]\n",
      "Train Loss: 3.833410 ---- Test Loss: 6.830496\n",
      "Epoch 2/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8287.28283574  7924.6965201  -1064.55170967]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8240.96115946  7944.07969453 -1063.83633809]\n",
      "Train Loss: 3.555828 ---- Test Loss: 5.585723\n",
      "Epoch 3/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8288.161396    7919.89397565 -1064.30257848]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8246.47472167  7943.6981056  -1064.18072071]\n",
      "Train Loss: 3.511878 ---- Test Loss: 5.356457\n",
      "Epoch 4/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8247.89248309  7966.82376843 -1064.62514263]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8245.18931759  7947.22021226 -1064.32182807]\n",
      "Train Loss: 3.486416 ---- Test Loss: 4.543832\n",
      "Epoch 5/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8252.95750692  7963.08234851 -1064.72219759]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8240.98719476  7945.02894354 -1063.89931065]\n",
      "Train Loss: 3.474732 ---- Test Loss: 5.022276\n",
      "Epoch 6/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8245.62203098  7968.88037157 -1064.60622053]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8237.7898491   7956.41320881 -1064.42017425]\n",
      "Train Loss: 3.470259 ---- Test Loss: 3.957525\n",
      "Epoch 7/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8268.96232235  7944.51719637 -1064.59712966]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8235.69988224  7952.59417165 -1064.03388256]\n",
      "Train Loss: 3.458166 ---- Test Loss: 3.542857\n",
      "Epoch 8/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8264.62479683  7947.73324609 -1064.51371547]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.89641105  7952.44260806 -1064.43846199]\n",
      "Train Loss: 3.453991 ---- Test Loss: 3.889589\n",
      "Epoch 9/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8250.02042403  7960.44952857 -1064.35624557]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.26657191  7950.39448079 -1064.26416809]\n",
      "Train Loss: 3.452383 ---- Test Loss: 3.487201\n",
      "Epoch 10/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8264.48618475  7952.22059203 -1064.79325715]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8237.00155799  7952.84417228 -1064.13705154]\n",
      "Train Loss: 3.447166 ---- Test Loss: 3.696834\n",
      "Epoch 11/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8246.00249821  7967.01946512 -1064.51156805]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8238.74671138  7954.62133048 -1064.36846021]\n",
      "Train Loss: 3.447594 ---- Test Loss: 3.898897\n",
      "Epoch 12/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8270.71232051  7939.33685057 -1064.38118046]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8238.35641373  7949.29215106 -1063.9983879 ]\n",
      "Train Loss: 3.440053 ---- Test Loss: 4.464895\n",
      "Epoch 13/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8264.87017091  7954.10160466 -1064.9400352 ]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8237.96627344  7951.91805378 -1064.1417756 ]\n",
      "Train Loss: 3.438568 ---- Test Loss: 3.491959\n",
      "Epoch 14/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8265.43101671  7950.61178976 -1064.75291763]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.61393507  7949.24276204 -1064.2130897 ]\n",
      "Train Loss: 3.438558 ---- Test Loss: 3.395750\n",
      "Epoch 15/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8256.50823693  7960.74291425 -1064.8086849 ]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8235.47129706  7956.18263273 -1064.25029917]\n",
      "Train Loss: 3.436088 ---- Test Loss: 3.399265\n",
      "Epoch 16/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8263.02523     7940.2062989  -1063.92237154]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.92692353  7946.08644997 -1064.09731302]\n",
      "Train Loss: 3.437161 ---- Test Loss: 3.383642\n",
      "Epoch 17/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8262.83696864  7941.65653646 -1064.00306444]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.18415576  7947.48217543 -1064.13765015]\n",
      "Train Loss: 3.432895 ---- Test Loss: 3.392561\n",
      "Epoch 18/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8268.17235144  7943.00493942 -1064.44697289]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.67447402  7942.53723199 -1063.8515099 ]\n",
      "Train Loss: 3.434397 ---- Test Loss: 3.393072\n",
      "Epoch 19/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8265.47443598  7950.93072298 -1064.77634999]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8248.17176374  7939.307927   -1064.01131906]\n",
      "Train Loss: 3.428581 ---- Test Loss: 3.385783\n",
      "Epoch 20/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8257.13516571  7955.01542575 -1064.48158936]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.37219359  7942.38756035 -1063.82162747]\n",
      "Train Loss: 3.432231 ---- Test Loss: 3.389524\n",
      "Epoch 21/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8255.29984879  7946.0648995  -1063.78245368]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.95893686  7941.39292671 -1063.72982764]\n",
      "Train Loss: 3.429281 ---- Test Loss: 3.414600\n",
      "Epoch 22/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8264.94794186  7945.23369245 -1064.37450362]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8243.10216309  7941.00022553 -1063.78102162]\n",
      "Train Loss: 3.431098 ---- Test Loss: 3.385520\n",
      "Epoch 23/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8245.49971831  7963.6973879  -1064.26371177]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8244.11706445  7941.16987207 -1063.85989294]\n",
      "Train Loss: 3.427245 ---- Test Loss: 3.427194\n",
      "Epoch 24/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8250.35931373  7958.7522848  -1064.26948239]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.2462098   7940.80985294 -1063.71145507]\n",
      "Train Loss: 3.428918 ---- Test Loss: 3.383796\n",
      "Epoch 25/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8250.96357802  7959.7975981  -1064.37723614]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.69520006  7945.31507144 -1064.0320525 ]\n",
      "Train Loss: 3.427566 ---- Test Loss: 3.396380\n",
      "Epoch 26/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8248.09550038  7963.92338505 -1064.45164337]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8243.39691092  7942.66987249 -1063.90841401]\n",
      "Train Loss: 3.426408 ---- Test Loss: 3.380130\n",
      "Epoch 27/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8248.14802844  7955.78357006 -1063.93037614]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.03247829  7941.42209724 -1063.66970493]\n",
      "Train Loss: 3.421041 ---- Test Loss: 3.581597\n",
      "Epoch 28/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8252.32257077  7962.35513248 -1064.63289748]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8247.29262415  7938.88020012 -1063.92488375]\n",
      "Train Loss: 3.422117 ---- Test Loss: 3.374211\n",
      "Epoch 29/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8251.4116543   7961.09349105 -1064.49070798]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8247.2507465   7938.37739268 -1063.88967186]\n",
      "Train Loss: 3.422262 ---- Test Loss: 3.369103\n",
      "Epoch 30/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8255.76098512  7955.52481715 -1064.42253058]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8246.95812442  7935.67649205 -1063.69600975]\n",
      "Train Loss: 3.426265 ---- Test Loss: 3.947680\n",
      "Epoch 31/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8264.56797942  7935.61825156 -1063.73058779]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8230.21840606  7948.93367737 -1063.43109043]\n",
      "Train Loss: 3.848967 ---- Test Loss: 3.411282\n",
      "Epoch 32/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8242.56683021  7976.3220716  -1064.88261045]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8231.98978917  7959.15917317 -1064.20988316]\n",
      "Train Loss: 3.459880 ---- Test Loss: 3.403489\n",
      "Epoch 33/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8260.35627847  7941.20992505 -1063.80829214]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8236.02661597  7950.62328364 -1063.9284989 ]\n",
      "Train Loss: 3.442807 ---- Test Loss: 3.387568\n",
      "Epoch 34/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8271.17018067  7943.22806811 -1064.66204792]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8245.12047158  7944.32124049 -1064.13025928]\n",
      "Train Loss: 3.435252 ---- Test Loss: 3.397153\n",
      "Epoch 35/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8257.64997553  7961.12110733 -1064.90936404]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8238.00319434  7951.93313785 -1064.14521811]\n",
      "Train Loss: 3.429789 ---- Test Loss: 3.404126\n",
      "Epoch 36/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8254.03951925  7963.59069747 -1064.82725879]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.87841272  7944.29710039 -1063.97865556]\n",
      "Train Loss: 3.428875 ---- Test Loss: 3.366621\n",
      "Epoch 37/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8268.92803091  7951.06511484 -1065.01609727]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.80259758  7944.20675603 -1063.90083777]\n",
      "Train Loss: 3.428189 ---- Test Loss: 3.366451\n",
      "Epoch 38/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8267.70571344  7945.76485272 -1064.59326562]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8243.52938323  7944.8012648  -1064.0547344 ]\n",
      "Train Loss: 3.425975 ---- Test Loss: 3.365512\n",
      "Epoch 39/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8261.28543161  7950.78705715 -1064.48685352]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8245.10974558  7938.90799113 -1063.78052206]\n",
      "Train Loss: 3.426072 ---- Test Loss: 3.367668\n",
      "Epoch 40/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8254.87341131  7959.87469328 -1064.64347535]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.36032379  7942.91609421 -1063.85491851]\n",
      "Train Loss: 3.424877 ---- Test Loss: 3.367792\n",
      "Epoch 41/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8255.07019368  7960.24730121 -1064.68063866]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8242.26849663  7945.29843389 -1064.00242704]\n",
      "Train Loss: 3.423200 ---- Test Loss: 3.362519\n",
      "Epoch 42/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8263.04248396  7953.67817795 -1064.79051869]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8245.44772692  7939.46891931 -1063.83930948]\n",
      "Train Loss: 3.423489 ---- Test Loss: 3.368700\n",
      "Epoch 43/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8263.39627221  7948.88863812 -1064.50586846]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.37132859  7940.34362617 -1063.62283483]\n",
      "Train Loss: 3.422048 ---- Test Loss: 3.369174\n",
      "Epoch 44/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8262.85962385  7955.48770219 -1064.89480022]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.0543963   7942.91614204 -1063.7675284 ]\n",
      "Train Loss: 3.422196 ---- Test Loss: 3.371099\n",
      "Epoch 45/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8257.71899615  7957.62474863 -1064.68870868]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8244.36058208  7939.40616065 -1063.76248254]\n",
      "Train Loss: 3.423326 ---- Test Loss: 3.370721\n",
      "Epoch 46/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8261.8493718   7949.49357521 -1064.44131129]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8241.55798447  7941.88807283 -1063.73492493]\n",
      "Train Loss: 3.420463 ---- Test Loss: 3.368293\n",
      "Epoch 47/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8258.04769746  7956.03132651 -1064.6080389 ]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8245.21886679  7937.5909408  -1063.70293016]\n",
      "Train Loss: 3.421699 ---- Test Loss: 3.366466\n",
      "Epoch 48/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8273.88301204  7940.45366709 -1064.66535198]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8244.05118034  7939.47488229 -1063.74620032]\n",
      "Train Loss: 3.420133 ---- Test Loss: 3.367724\n",
      "Epoch 49/50\n",
      "----------\n",
      "[-8263.79447029  7958.09617352 -1065.12529474]\n",
      "[-8267.08558735  7940.98958424 -1064.24459012]\n",
      "[-8252.27557442  7945.33781512 -1064.67476422]\n",
      "[-8244.30434296  7936.68282861 -1063.58316279]\n",
      "Train Loss: 3.421973 ---- Test Loss: 3.365714\n",
      "3800.862854719162\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20      # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-'*10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names)//bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:,0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1,], [rng_res_tr,az_step_tr,el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names)//bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:,0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "\n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "        \n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1,], [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "        \n",
    "        epoch_loss_train = running_loss_train*x_train.size()[0]/len(training_names)\n",
    "        epoch_loss_test = running_loss_test*x_test.size()[0]/len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch%5==0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 0.296981071176029\n",
      "Localization Error (m) = 57.865952131498965\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical),3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    cartesian[:,0] = np.multiply(np.cos(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,1] = np.multiply(-np.sin(np.radians(spherical[:,1])), hypotenuse)\n",
    "    cartesian[:,2] = np.multiply(np.sin(np.radians(spherical[:,2])), spherical[:,0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range,az,el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test),3))\n",
    "labels_test_reg = np.zeros((len(y_test),3))\n",
    "\n",
    "for i in range(0, len(y_test)//bs):  \n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, True)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs*i : bs*i + bs] = (labels_test[:,0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs*i : bs*i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs*i : bs*i + bs, 2] = labels_test_reg[bs*i : bs*i + bs, 2]\n",
    "    \n",
    "azim_tot = 0\n",
    "for i in np.arange(0,len(out_test_reg),1):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i,1] - labels_test_reg[i,1])\n",
    "    \n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) + coord_ts)\n",
    "\n",
    "sum_tot = 0\n",
    "for i in np.arange(0,len(new_data) - (len(new_data) % bs),1):\n",
    "    sum_tot += np.linalg.norm(new_data[i,:] - new_labels_data[i,:])\n",
    "    \n",
    "# def reject_outliers(data, m=2):\n",
    "#     return data[abs(data - np.mean(data)) < m * np.std(data)]\n",
    "# \n",
    "# azims = (np.multiply(out_test_reg, [rng_res_ts,az_step_ts,el_step_ts]) - np.multiply(labels_test_reg, [rng_res_ts,az_step_ts,el_step_ts]))[:,1]\n",
    "# azims = reject_outliers(azims)\n",
    "# mse = np.mean(azims**2)\n",
    "# bias_sq = np.mean(azims)**2\n",
    "# var = np.var(azims)\n",
    "# print(mse, bias_sq, var)\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
