{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Example on Radar Dataset (Ouachita Mountains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Training labels:\n",
      "[[ 8.3355 13.325  19.525 ]\n",
      " [18.156  17.388  32.775 ]\n",
      " [ 8.777  24.73   18.582 ]\n",
      " ...\n",
      " [15.785  15.992  29.641 ]\n",
      " [ 3.1736 10.716  12.27  ]\n",
      " [ 4.5552 21.035  14.234 ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 29 # Bonneville Salt Flats\n",
    "names = os.listdir(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names = sorted(names, key=index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'num{scenario_idx}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "scenario_idx_test = 62 # Ouachita Mountains\n",
    "names_test = os.listdir(f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names_test = sorted(names_test, key=index)\n",
    "y_test = pd.read_csv(f'num{scenario_idx_test}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names_test = y_test.columns[4:7]\n",
    "y_test = y_test[col_names_test].to_numpy()\n",
    "\n",
    "# Define fine-tuning name and labels\n",
    "few_shot_count = 64\n",
    "names_FS = names_test[:few_shot_count]\n",
    "y_FS = y_test[:few_shot_count]\n",
    "\n",
    "# Create training and testing datasets\n",
    "y_train = y[:int(0.9 * len(names))]\n",
    "y_test = y_test[int(0.9 * len(names_test)) - 1:]\n",
    "training_names = names[:int(0.9 * len(names))]\n",
    "test_names = names_test[int(0.9 * len(names_test)) - 1:]\n",
    "\n",
    "print('Training labels:')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.4], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [10851, 215, -5.4]  # Tensor corner\n",
    "rng_res_tr = 59.9585 / 2        # Range resolution\n",
    "az_step_tr = 0.4                # Azimuth step size\n",
    "el_step_tr = 0.01               # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11471, 215, -5.6]  # Tensor corner\n",
    "rng_res_ts = 59.9585 / 2        # Range resolution\n",
    "az_step_ts = 0.4                # Azimuth step size\n",
    "el_step_ts = 0.01               # Elevation step size\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, directory, normalize=True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind * bs, (ind + 1) * bs):\n",
    "        try:\n",
    "            temp = sio.loadmat(directory + names[j])['P']\n",
    "        except:\n",
    "            break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j, :])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 32, 24]           2,048\n",
      "       BatchNorm1d-2               [-1, 32, 24]              64\n",
      "            Conv1d-3               [-1, 64, 10]           6,208\n",
      "       BatchNorm1d-4               [-1, 64, 10]             128\n",
      "            Linear-5                   [-1, 20]           6,420\n",
      "            Linear-6                    [-1, 2]              42\n",
      "               Net-7                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 14,910\n",
      "Trainable params: 14,910\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "        self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "        return output_reg\n",
    "    \n",
    "from torchsummary import summary\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 50  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8205.89466346  7729.9688152  -1006.03124537]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8673.48676275  7538.50656094 -1067.20095188]\n",
      "Train Loss: 8.731725 ---- Test Loss: 67.093027\n",
      "Epoch 1/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8195.82266503  7745.72584678 -1006.34250361]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8628.45446684  7610.61948904 -1068.46614085]\n",
      "Train Loss: 0.185318 ---- Test Loss: 58.223908\n",
      "Epoch 2/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.49127336  7759.01678775 -1007.26569128]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8612.02345273  7641.13144918 -1069.2007458 ]\n",
      "Train Loss: 0.130282 ---- Test Loss: 56.654466\n",
      "Epoch 3/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8201.53093511  7752.35814815 -1007.1192642 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8594.87771448  7660.26664837 -1069.19175311]\n",
      "Train Loss: 0.113513 ---- Test Loss: 54.894583\n",
      "Epoch 4/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8200.84741535  7752.20790759 -1007.06572667]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8561.57910421  7687.63502869 -1068.58161491]\n",
      "Train Loss: 0.111029 ---- Test Loss: 51.384093\n",
      "Epoch 5/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8192.87392552  7756.9769039  -1006.84130968]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8536.06841936  7707.77890512 -1068.07283187]\n",
      "Train Loss: 0.114072 ---- Test Loss: 47.993399\n",
      "Epoch 6/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8187.7583711   7762.00151374 -1006.81829568]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8504.55875506  7743.39076473 -1068.12652893]\n",
      "Train Loss: 0.117516 ---- Test Loss: 44.356668\n",
      "Epoch 7/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8188.11505888  7765.53809131 -1007.0585475 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8489.82291993  7756.24928865 -1067.92011473]\n",
      "Train Loss: 0.116891 ---- Test Loss: 42.546054\n",
      "Epoch 8/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8189.58668876  7770.04368669 -1007.43054023]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8483.51697303  7770.30873987 -1068.36928666]\n",
      "Train Loss: 0.115292 ---- Test Loss: 40.972617\n",
      "Epoch 9/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8188.69865735  7777.34578511 -1007.8216969 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8483.95941857  7777.45549149 -1068.84796871]\n",
      "Train Loss: 0.118276 ---- Test Loss: 39.143529\n",
      "Epoch 10/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8189.6730862   7780.02085721 -1008.04915328]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8490.51713655  7774.86982525 -1069.13477979]\n",
      "Train Loss: 0.114674 ---- Test Loss: 38.194920\n",
      "Epoch 11/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8190.75370607  7780.45359545 -1008.14566671]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8496.689951    7772.77909261 -1069.42656309]\n",
      "Train Loss: 0.100556 ---- Test Loss: 37.714718\n",
      "Epoch 12/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8191.96048236  7779.01433522 -1008.1353033 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8502.86826555  7768.83384011 -1069.60280954]\n",
      "Train Loss: 0.093059 ---- Test Loss: 37.349484\n",
      "Epoch 13/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8191.10941649  7779.41740465 -1008.10500086]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8502.14443824  7769.8568808  -1069.61727477]\n",
      "Train Loss: 0.093102 ---- Test Loss: 37.161097\n",
      "Epoch 14/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8191.69408679  7778.62259338 -1008.09399195]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8502.99882561  7769.36114726 -1069.64479217]\n",
      "Train Loss: 0.093090 ---- Test Loss: 36.623631\n",
      "Epoch 15/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8191.91638333  7777.4398005  -1008.03569965]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8497.62067234  7774.32745225 -1069.58739453]\n",
      "Train Loss: 0.090134 ---- Test Loss: 36.207855\n",
      "Epoch 16/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8192.39392118  7776.08013716 -1007.98306935]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8497.45619902  7774.34347482 -1069.57712954]\n",
      "Train Loss: 0.086529 ---- Test Loss: 35.829159\n",
      "Epoch 17/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8193.02082564  7774.98681198 -1007.95648229]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8496.93462496  7775.16752518 -1069.59305346]\n",
      "Train Loss: 0.084347 ---- Test Loss: 35.497467\n",
      "Epoch 18/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8193.81386157  7773.10441226 -1007.8921976 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8499.23507579  7773.33720855 -1069.63594951]\n",
      "Train Loss: 0.082790 ---- Test Loss: 35.250802\n",
      "Epoch 19/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8194.5587201   7771.68692615 -1007.85337164]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8498.18860868  7778.01359472 -1069.85740132]\n",
      "Train Loss: 0.080370 ---- Test Loss: 34.661667\n",
      "Epoch 20/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8195.33259515  7769.94193233 -1007.79633474]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8496.52997569  7781.93210296 -1069.98953163]\n",
      "Train Loss: 0.078196 ---- Test Loss: 34.259957\n",
      "Epoch 21/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.35258053  7769.0356849  -1007.80675406]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8498.42820523  7782.00798009 -1070.1242951 ]\n",
      "Train Loss: 0.076820 ---- Test Loss: 34.021844\n",
      "Epoch 22/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.87386624  7768.71404119 -1007.82077187]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8500.87802421  7781.67607818 -1070.27128325]\n",
      "Train Loss: 0.075327 ---- Test Loss: 33.854395\n",
      "Epoch 23/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.70255997  7769.12561064 -1007.83494215]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8498.67645976  7788.66921132 -1070.55915975]\n",
      "Train Loss: 0.073879 ---- Test Loss: 33.209796\n",
      "Epoch 24/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.74330236  7768.91786782 -1007.82482778]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8498.76509211  7790.26347665 -1070.66526506]\n",
      "Train Loss: 0.072296 ---- Test Loss: 33.034696\n",
      "Epoch 25/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.03119348  7768.95638887 -1007.84583924]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8499.26683809  7790.37976239 -1070.70691128]\n",
      "Train Loss: 0.070814 ---- Test Loss: 32.646366\n",
      "Epoch 26/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.79223342  7769.62916826 -1007.87166391]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8498.87398383  7793.3654186  -1070.8673911 ]\n",
      "Train Loss: 0.069359 ---- Test Loss: 32.538312\n",
      "Epoch 27/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.99505841  7769.4770321  -1007.87546061]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8500.47202324  7794.3738608  -1071.04006651]\n",
      "Train Loss: 0.067734 ---- Test Loss: 32.410358\n",
      "Epoch 28/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.42399676  7769.26921041 -1007.89048477]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8502.86492436  7792.9625266  -1071.11530611]\n",
      "Train Loss: 0.066237 ---- Test Loss: 32.236559\n",
      "Epoch 29/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.72881865  7768.75126107 -1007.87843393]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8504.81971689  7792.51832102 -1071.22127526]\n",
      "Train Loss: 0.064994 ---- Test Loss: 32.017025\n",
      "Epoch 30/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.87441324  7768.10213337 -1007.84801967]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8507.52888668  7794.31314607 -1071.51938041]\n",
      "Train Loss: 0.064077 ---- Test Loss: 31.864186\n",
      "Epoch 31/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8198.28902315  7768.00771673 -1007.86908195]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8507.39993002  7794.9180683  -1071.54850058]\n",
      "Train Loss: 0.063025 ---- Test Loss: 31.517915\n",
      "Epoch 32/50\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.81612099  7768.24910298 -1007.85326491]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8506.23277849  7796.34327211 -1071.55801079]\n",
      "Train Loss: 0.061774 ---- Test Loss: 31.059137\n",
      "Epoch 33/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.42319634  7768.67607146 -1007.85402222]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8507.64462748  7795.97984028 -1071.63186969]\n",
      "Train Loss: 0.060901 ---- Test Loss: 30.747501\n",
      "Epoch 34/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.76255035  7768.22879488 -1007.84854826]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8507.87631973  7796.82243039 -1071.70059908]\n",
      "Train Loss: 0.060055 ---- Test Loss: 30.409379\n",
      "Epoch 35/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.3003067   7768.87909916 -1007.8585254 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8509.26904553  7796.80552408 -1071.7948961 ]\n",
      "Train Loss: 0.059098 ---- Test Loss: 30.191112\n",
      "Epoch 36/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.14927495  7768.94688517 -1007.85290404]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8509.64964877  7798.14735058 -1071.90514217]\n",
      "Train Loss: 0.058141 ---- Test Loss: 29.946426\n",
      "Epoch 37/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.41242671  7770.12702631 -1007.87763088]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8511.03303323  7797.68049746 -1071.97057348]\n",
      "Train Loss: 0.057001 ---- Test Loss: 29.816796\n",
      "Epoch 38/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.70169063  7769.64570728 -1007.86681511]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8513.66328408  7796.18613115 -1072.05696175]\n",
      "Train Loss: 0.055993 ---- Test Loss: 29.819453\n",
      "Epoch 39/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.69163593  7769.68901622 -1007.86882274]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8517.53745833  7792.59523205 -1072.09720084]\n",
      "Train Loss: 0.055132 ---- Test Loss: 29.878783\n",
      "Epoch 40/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.70324543  7769.3206544  -1007.91173002]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8516.20494796  7794.17264037 -1072.10479896]\n",
      "Train Loss: 0.054544 ---- Test Loss: 29.581350\n",
      "Epoch 41/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.67688543  7770.04160596 -1007.88951399]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8518.74155865  7792.53062365 -1072.17565655]\n",
      "Train Loss: 0.053936 ---- Test Loss: 29.316737\n",
      "Epoch 42/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.81490611  7769.5787562  -1007.87003748]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8520.5490973   7790.88454609 -1072.19635971]\n",
      "Train Loss: 0.053073 ---- Test Loss: 29.177885\n",
      "Epoch 43/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8197.01124347  7769.33924552 -1007.86805018]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8522.65591251  7789.29129979 -1072.24093586]\n",
      "Train Loss: 0.052359 ---- Test Loss: 29.001911\n",
      "Epoch 44/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.81342993  7769.66417962 -1007.87518615]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8523.25293242  7788.2711188  -1072.21795115]\n",
      "Train Loss: 0.051837 ---- Test Loss: 28.844184\n",
      "Epoch 45/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.54806407  7769.95803295 -1007.87604003]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8520.58032331  7791.20662118 -1072.21868347]\n",
      "Train Loss: 0.051297 ---- Test Loss: 28.595996\n",
      "Epoch 46/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.55213564  7769.65272285 -1007.8575597 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8523.19318246  7787.76128421 -1072.18191694]\n",
      "Train Loss: 0.050746 ---- Test Loss: 28.593248\n",
      "Epoch 47/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.37866565  7770.43176924 -1007.8941544 ]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8522.70928794  7787.40160396 -1072.12621063]\n",
      "Train Loss: 0.050204 ---- Test Loss: 28.532469\n",
      "Epoch 48/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.68311061  7769.80298575 -1007.87526747]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8523.58716436  7785.20869234 -1072.04904506]\n",
      "Train Loss: 0.049636 ---- Test Loss: 28.510026\n",
      "Epoch 49/50\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.58252562  7770.33386941 -1007.90134628]\n",
      "[-8209.36050705  7974.67245585 -1062.87089418]\n",
      "[-8522.81163911  7786.4327588  -1072.07253868]\n",
      "Train Loss: 0.048983 ---- Test Loss: 28.356645\n",
      "4113.241749286652\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 6.152806818050189\n",
      "Localization Error (m) = 511.4853873152036\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Regression CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.conv1.weight.requires_grad = False\n",
    "model.module.conv1.bias.requires_grad = False\n",
    "model.module.batchnorm1.weight.requires_grad = False\n",
    "model.module.batchnorm1.bias.requires_grad = False\n",
    "\n",
    "model.module.conv2.weight.requires_grad = False\n",
    "model.module.conv2.bias.requires_grad = False\n",
    "model.module.batchnorm2.weight.requires_grad = False\n",
    "model.module.batchnorm2.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8342.33102062  7299.63588013 -1000.00141727]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8486.41324285  7815.04882915 -1071.67841562]\n",
      "Train Loss: 21.412037 ---- Test Loss: 26.475146\n",
      "Epoch 1/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8326.1469946   7314.88980451  -999.81082865]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8461.55583635  7835.91001774 -1071.29674204]\n",
      "Train Loss: 19.837910 ---- Test Loss: 24.557225\n",
      "Epoch 2/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8309.4573078   7330.33690282  -999.60154979]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8438.30445479  7855.92490089 -1070.97903045]\n",
      "Train Loss: 18.338196 ---- Test Loss: 22.710969\n",
      "Epoch 3/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8292.26205663  7345.97573213  -999.3737682 ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8419.37979373  7870.46156041 -1070.61471125]\n",
      "Train Loss: 16.920042 ---- Test Loss: 20.947931\n",
      "Epoch 4/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8274.89707622  7361.65331591  -999.14121901]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8402.64263078  7883.0862405  -1070.28150038]\n",
      "Train Loss: 15.599827 ---- Test Loss: 19.225507\n",
      "Epoch 5/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8257.42557607  7377.34572369  -998.90679976]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8384.960906    7894.5809463  -1069.81589171]\n",
      "Train Loss: 14.380770 ---- Test Loss: 17.560201\n",
      "Epoch 6/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8239.90449999  7393.03492378  -998.67329756]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8367.59962023  7905.74494007 -1069.35420745]\n",
      "Train Loss: 13.267578 ---- Test Loss: 15.999395\n",
      "Epoch 7/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8222.39269906  7408.70362587  -998.44363308]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8349.93893999  7917.7826912  -1068.93142809]\n",
      "Train Loss: 12.259922 ---- Test Loss: 14.594726\n",
      "Epoch 8/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8204.9532165   7424.33578355  -998.22104294]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8331.5921191   7930.75616382 -1068.52592908]\n",
      "Train Loss: 11.357395 ---- Test Loss: 13.349113\n",
      "Epoch 9/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8187.65605702  7439.9157775   -998.00921331]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8314.16656602  7942.61746004 -1068.11486212]\n",
      "Train Loss: 10.558078 ---- Test Loss: 12.228959\n",
      "Epoch 10/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8170.57795307  7455.42687572  -997.81216685]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8297.6076334   7954.32572778 -1067.75550264]\n",
      "Train Loss: 9.858420 ---- Test Loss: 11.261183\n",
      "Epoch 11/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8153.80083949  7470.84888175  -997.63401357]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8281.76926753  7965.20551137 -1067.39423716]\n",
      "Train Loss: 9.253246 ---- Test Loss: 10.412179\n",
      "Epoch 12/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8137.40878885  7486.15771288  -997.47871705]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8266.0355572   7976.08214387 -1067.04262798]\n",
      "Train Loss: 8.734425 ---- Test Loss: 9.678014\n",
      "Epoch 13/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8121.48267304  7501.32101958  -997.34947025]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8251.33159387  7986.47067516 -1066.73105753]\n",
      "Train Loss: 8.292630 ---- Test Loss: 9.062601\n",
      "Epoch 14/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8106.10064176  7516.29827607  -997.24873147]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8238.44214676  7995.99553682 -1066.48708127]\n",
      "Train Loss: 7.918140 ---- Test Loss: 8.542351\n",
      "Epoch 15/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8091.33309385  7531.0411784   -997.17791609]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8227.10584248  8005.6354899  -1066.35598126]\n",
      "Train Loss: 7.600129 ---- Test Loss: 8.105703\n",
      "Epoch 16/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8077.24196398  7545.49439576  -997.13739915]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8217.3952369   8014.26347915 -1066.26912172]\n",
      "Train Loss: 7.328535 ---- Test Loss: 7.742823\n",
      "Epoch 17/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8063.88041592  7559.59731818  -997.12660782]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8209.32921781  8021.56232983 -1066.20658146]\n",
      "Train Loss: 7.093642 ---- Test Loss: 7.445872\n",
      "Epoch 18/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8051.2905813   7573.28722748  -997.14407534]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8202.42043294  8028.22120511 -1066.18023022]\n",
      "Train Loss: 6.887599 ---- Test Loss: 7.195419\n",
      "Epoch 19/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8039.5009846   7586.49713973  -997.18714819]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8196.99024044  8033.62074056 -1066.17081952]\n",
      "Train Loss: 6.702163 ---- Test Loss: 6.985879\n",
      "Epoch 20/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8028.52863187  7599.16574504  -997.252749  ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8193.180988    8037.36827816 -1066.16188548]\n",
      "Train Loss: 6.533076 ---- Test Loss: 6.805644\n",
      "Epoch 21/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8018.3746126   7611.22325937  -997.33622319]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8190.80734678  8039.93129196 -1066.17126048]\n",
      "Train Loss: 6.374578 ---- Test Loss: 6.646881\n",
      "Epoch 22/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8009.03059379  7622.6169853   -997.43336058]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8189.75139871  8041.29849437 -1066.19023693]\n",
      "Train Loss: 6.222154 ---- Test Loss: 6.505867\n",
      "Epoch 23/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-8000.47685827  7633.30120686  -997.539653  ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8189.89982862  8041.48942848 -1066.21250178]\n",
      "Train Loss: 6.073324 ---- Test Loss: 6.376783\n",
      "Epoch 24/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7992.68261355  7643.23996902  -997.65049725]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8191.10785562  8040.60169854 -1066.2348073 ]\n",
      "Train Loss: 5.926272 ---- Test Loss: 6.254806\n",
      "Epoch 25/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7985.60830491  7652.40242595  -997.76106408]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8193.25001983  8038.70800059 -1066.25361813]\n",
      "Train Loss: 5.780056 ---- Test Loss: 6.137207\n",
      "Epoch 26/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7979.20736764  7660.77356447  -997.86708679]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8196.22998936  8035.85681036 -1066.26579199]\n",
      "Train Loss: 5.633918 ---- Test Loss: 6.021814\n",
      "Epoch 27/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7973.42864761  7668.34888239  -997.96469169]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8199.90486042  8032.17200102 -1066.27002481]\n",
      "Train Loss: 5.487904 ---- Test Loss: 5.906983\n",
      "Epoch 28/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7968.21708473  7675.13738326  -998.05063188]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8204.17545111  8027.69759056 -1066.2627277 ]\n",
      "Train Loss: 5.342488 ---- Test Loss: 5.792370\n",
      "Epoch 29/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7963.5161356   7681.15929111  -998.12230265]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8208.9015633   8022.53964753 -1066.24162046]\n",
      "Train Loss: 5.198500 ---- Test Loss: 5.677527\n",
      "Epoch 30/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7959.27020546  7686.44682637  -998.17794698]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8213.98701249  8016.78755493 -1066.20624528]\n",
      "Train Loss: 5.057070 ---- Test Loss: 5.562835\n",
      "Epoch 31/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7955.42473819  7691.04414387  -998.21665551]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8219.3187266  8010.5503791 -1066.1562772]\n",
      "Train Loss: 4.919903 ---- Test Loss: 5.449536\n",
      "Epoch 32/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7951.92755018  7695.00517427  -998.23831524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8224.83938222  8003.91961779 -1066.09393098]\n",
      "Train Loss: 4.788298 ---- Test Loss: 5.338945\n",
      "Epoch 33/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7948.73382216  7698.39457862  -998.24398961]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8230.48969888  7996.99543543 -1066.02182763]\n",
      "Train Loss: 4.663462 ---- Test Loss: 5.232251\n",
      "Epoch 34/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7945.79992442  7701.28198472  -998.2351531 ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8236.19857025  7989.88853039 -1065.9424513 ]\n",
      "Train Loss: 4.546553 ---- Test Loss: 5.130881\n",
      "Epoch 35/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7943.08709584  7703.74204993  -998.21393007]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8241.89778426  7982.71692857 -1065.85891619]\n",
      "Train Loss: 4.438734 ---- Test Loss: 5.036019\n",
      "Epoch 36/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7940.55830035  7705.8550948   -998.18292779]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8247.52545596  7975.58480936 -1065.77382614]\n",
      "Train Loss: 4.340886 ---- Test Loss: 4.948609\n",
      "Epoch 37/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7938.1810405   7707.69790803  -998.14483832]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8253.06189099  7968.55473554 -1065.68988478]\n",
      "Train Loss: 4.253170 ---- Test Loss: 4.869352\n",
      "Epoch 38/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7935.92591164  7709.34540392  -998.10244619]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8258.42839678  7961.76251636 -1065.61055605]\n",
      "Train Loss: 4.175628 ---- Test Loss: 4.798792\n",
      "Epoch 39/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7933.7669794   7710.86830515  -998.05850511]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8263.57704795  7955.28750251 -1065.53768447]\n",
      "Train Loss: 4.107949 ---- Test Loss: 4.737160\n",
      "Epoch 40/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7931.67979339  7712.33243501  -998.01556327]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8268.46972439  7949.19888128 -1065.47309573]\n",
      "Train Loss: 4.049501 ---- Test Loss: 4.683924\n",
      "Epoch 41/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7929.6430757   7713.79336529  -997.97573475]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8273.07418666  7943.55637875 -1065.41838611]\n",
      "Train Loss: 3.999440 ---- Test Loss: 4.638610\n",
      "Epoch 42/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7927.63681496  7715.3007224   -997.94084595]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8277.36196791  7938.40848319 -1065.37467288]\n",
      "Train Loss: 3.956757 ---- Test Loss: 4.600607\n",
      "Epoch 43/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7925.64265446  7716.89424671  -997.91221207]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8281.31023674  7933.78898414 -1065.34250247]\n",
      "Train Loss: 3.920357 ---- Test Loss: 4.569346\n",
      "Epoch 44/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7923.64398129  7718.60387607  -997.89064739]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.89585253  7929.72041098 -1065.3216782 ]\n",
      "Train Loss: 3.889457 ---- Test Loss: 4.544107\n",
      "Epoch 45/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7921.62597928  7720.4485598   -997.87639338]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8288.1057373   7926.2104631  -1065.31173099]\n",
      "Train Loss: 3.862758 ---- Test Loss: 4.524169\n",
      "Epoch 46/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7919.575775    7722.43783465  -997.86922656]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8290.9311576   7923.25859557 -1065.31196271]\n",
      "Train Loss: 3.839284 ---- Test Loss: 4.508855\n",
      "Epoch 47/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7917.48283728  7724.57159029  -997.86846877]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8293.36553256  7920.85266144 -1065.3210855 ]\n",
      "Train Loss: 3.818290 ---- Test Loss: 4.497599\n",
      "Epoch 48/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7915.33837605  7726.84400168  -997.87319528]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8295.41034961  7918.96442944 -1065.33733318]\n",
      "Train Loss: 3.799293 ---- Test Loss: 4.489838\n",
      "Epoch 49/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7913.13640019  7729.23858457  -997.88199069]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8297.07098273  7917.55930712 -1065.35880502]\n",
      "Train Loss: 3.781665 ---- Test Loss: 4.485068\n",
      "Epoch 50/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7910.87321887  7731.7326372   -997.89319559]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8298.35382208  7916.5980191  -1065.38337982]\n",
      "Train Loss: 3.765201 ---- Test Loss: 4.482814\n",
      "Epoch 51/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7908.54855174  7734.2985227   -997.90505809]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8299.27322647  7916.03435882 -1065.40903788]\n",
      "Train Loss: 3.749694 ---- Test Loss: 4.482637\n",
      "Epoch 52/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7906.1654682   7736.90525298  -997.91582887]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8299.84867628  7915.81857519 -1065.43388539]\n",
      "Train Loss: 3.734928 ---- Test Loss: 4.484143\n",
      "Epoch 53/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7903.72970229  7739.52111417  -997.92388191]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8300.10313973  7915.90002098 -1065.45621272]\n",
      "Train Loss: 3.720844 ---- Test Loss: 4.486966\n",
      "Epoch 54/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7901.25109008  7742.11342764  -997.92779155]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8300.06359205  7916.22404975 -1065.47432839]\n",
      "Train Loss: 3.707434 ---- Test Loss: 4.490770\n",
      "Epoch 55/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7898.74277686  7744.65087267  -997.92642754]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8299.75978924  7916.74730247 -1065.48745492]\n",
      "Train Loss: 3.694709 ---- Test Loss: 4.495244\n",
      "Epoch 56/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7896.22161874  7747.1040965   -997.91901926]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8299.22437245  7917.424088   -1065.49486111]\n",
      "Train Loss: 3.682695 ---- Test Loss: 4.500115\n",
      "Epoch 57/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7893.70641999  7749.44791756  -997.90518139]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8298.49175259  7918.2127083  -1065.49619129]\n",
      "Train Loss: 3.671417 ---- Test Loss: 4.505167\n",
      "Epoch 58/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7891.21967334  7751.66015701  -997.88495246]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8297.59869182  7919.07496288 -1065.49147155]\n",
      "Train Loss: 3.660897 ---- Test Loss: 4.510223\n",
      "Epoch 59/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7888.78519934  7753.72387498  -997.85878458]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8296.59040677  7919.98261516 -1065.48193391]\n",
      "Train Loss: 3.651228 ---- Test Loss: 4.515231\n",
      "Epoch 60/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7886.43246786  7755.62832612  -997.82788248]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8295.50201313  7920.90729226 -1065.46812114]\n",
      "Train Loss: 3.642463 ---- Test Loss: 4.520062\n",
      "Epoch 61/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7884.18618081  7757.36492452  -997.79327881]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8294.36811459  7921.82478288 -1065.45080675]\n",
      "Train Loss: 3.634447 ---- Test Loss: 4.524610\n",
      "Epoch 62/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7882.07047017  7758.9288904   -997.75620914]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8293.2207755   7922.71606184 -1065.43092457]\n",
      "Train Loss: 3.627129 ---- Test Loss: 4.528781\n",
      "Epoch 63/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7880.10784804  7760.31925019  -997.71804518]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8292.09136891  7923.57539549 -1065.41021335]\n",
      "Train Loss: 3.620379 ---- Test Loss: 4.532605\n",
      "Epoch 64/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7878.31924772  7761.54052838  -997.68040524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8291.00408142  7924.37984563 -1065.38882381]\n",
      "Train Loss: 3.614155 ---- Test Loss: 4.535872\n",
      "Epoch 65/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7876.72071154  7762.59327824  -997.64434252]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8289.98092872  7925.1206718  -1065.36767096]\n",
      "Train Loss: 3.608359 ---- Test Loss: 4.538505\n",
      "Epoch 66/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7875.32555798  7763.48323615  -997.61106433]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8289.03938285  7925.79176975 -1065.34753326]\n",
      "Train Loss: 3.602851 ---- Test Loss: 4.540433\n",
      "Epoch 67/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7874.14277744  7764.21766786  -997.58159817]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8288.19212091  7926.38985095 -1065.3290477 ]\n",
      "Train Loss: 3.597513 ---- Test Loss: 4.541602\n",
      "Epoch 68/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7873.17693999  7764.80444356  -997.5567275 ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8287.44678286  7926.91294014 -1065.31259739]\n",
      "Train Loss: 3.592240 ---- Test Loss: 4.541970\n",
      "Epoch 69/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7872.42725034  7765.25254762  -997.53696352]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8286.80657514  7927.36116119 -1065.29840282]\n",
      "Train Loss: 3.586947 ---- Test Loss: 4.541516\n",
      "Epoch 70/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7871.88961757  7765.56997958  -997.52254511]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8286.27040476  7927.73559549 -1065.28645771]\n",
      "Train Loss: 3.581574 ---- Test Loss: 4.540245\n",
      "Epoch 71/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7871.55455044  7765.7656055   -997.5134209 ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8285.83246692  7928.03915403 -1065.27655745]\n",
      "Train Loss: 3.576090 ---- Test Loss: 4.538177\n",
      "Epoch 72/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7871.40915791  7765.84782168  -997.50929291]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8285.48414231  7928.27501065 -1065.26832585]\n",
      "Train Loss: 3.570487 ---- Test Loss: 4.535350\n",
      "Epoch 73/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7871.43715042  7765.82455714  -997.50961657]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8285.21326826  7928.44780673 -1065.2612436 ]\n",
      "Train Loss: 3.564783 ---- Test Loss: 4.531812\n",
      "Epoch 74/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7871.61950772  7765.70414165  -997.51369832]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8285.00609788  7928.5623561  -1065.25469654]\n",
      "Train Loss: 3.559011 ---- Test Loss: 4.527615\n",
      "Epoch 75/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7871.93534958  7765.49430616  -997.5206879 ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.84650652  7928.62542249 -1065.24803642]\n",
      "Train Loss: 3.553213 ---- Test Loss: 4.522822\n",
      "Epoch 76/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7872.36226882  7765.20375537  -997.52969908]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.72912643  7928.64899046 -1065.24167248]\n",
      "Train Loss: 3.547452 ---- Test Loss: 4.517597\n",
      "Epoch 77/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7872.88327988  7764.84343989  -997.54033629]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.63589288  7928.64071313 -1065.23488377]\n",
      "Train Loss: 3.541839 ---- Test Loss: 4.511997\n",
      "Epoch 78/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7873.47492768  7764.42227395  -997.55165935]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.54970286  7928.60852914 -1065.22703232]\n",
      "Train Loss: 3.536329 ---- Test Loss: 4.506074\n",
      "Epoch 79/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7874.11533247  7763.9492819   -997.562836  ]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.45535702  7928.56071885 -1065.21762988]\n",
      "Train Loss: 3.530955 ---- Test Loss: 4.499887\n",
      "Epoch 80/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7874.78394124  7763.43397392  -997.57314954]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.33821581  7928.50715437 -1065.20632804]\n",
      "Train Loss: 3.525738 ---- Test Loss: 4.493500\n",
      "Epoch 81/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7875.46242867  7762.88640684  -997.58206054]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8284.18679206  7928.45734201 -1065.19296648]\n",
      "Train Loss: 3.520689 ---- Test Loss: 4.486988\n",
      "Epoch 82/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7876.13478098  7762.31705893  -997.58920424]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8283.99129908  7928.42135742 -1065.17753562]\n",
      "Train Loss: 3.515815 ---- Test Loss: 4.480433\n",
      "Epoch 83/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7876.78681839  7761.73759824  -997.59440854]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8283.74502009  7928.40883117 -1065.16020338]\n",
      "Train Loss: 3.511114 ---- Test Loss: 4.473921\n",
      "Epoch 84/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7877.40869644  7761.15838423  -997.59769654]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8283.44302625  7928.4294127  -1065.141259  ]\n",
      "Train Loss: 3.506577 ---- Test Loss: 4.467549\n",
      "Epoch 85/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7877.99133888  7760.59115313  -997.59922752]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8283.0832114   7928.49164753 -1065.12111038]\n",
      "Train Loss: 3.502197 ---- Test Loss: 4.461414\n",
      "Epoch 86/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7878.5297447   7760.04566013  -997.59929687]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8282.66516201  7928.60316432 -1065.10022033]\n",
      "Train Loss: 3.497963 ---- Test Loss: 4.455609\n",
      "Epoch 87/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7879.02022526  7759.53171099  -997.59828735]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8282.19143947  7928.7689442  -1065.07908136]\n",
      "Train Loss: 3.493863 ---- Test Loss: 4.450218\n",
      "Epoch 88/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7879.46175453  7759.05697812  -997.59661766]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8281.6649412   7928.99323389 -1065.05816155]\n",
      "Train Loss: 3.489886 ---- Test Loss: 4.445308\n",
      "Epoch 89/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7879.85413431  7758.62829383  -997.59470651]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8281.09746311  7929.2811925  -1065.03858481]\n",
      "Train Loss: 3.486052 ---- Test Loss: 4.441005\n",
      "Epoch 90/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7880.20157788  7758.25145935  -997.59319095]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8280.49368654  7929.63159331 -1065.02058748]\n",
      "Train Loss: 3.482323 ---- Test Loss: 4.437329\n",
      "Epoch 91/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7880.50612001  7757.92868315  -997.59234069]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8279.85861017  7930.04119033 -1065.0042975 ]\n",
      "Train Loss: 3.478689 ---- Test Loss: 4.434277\n",
      "Epoch 92/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7880.76981311  7757.66012448  -997.59229702]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8279.19802991  7930.50412067 -1064.98972807]\n",
      "Train Loss: 3.475147 ---- Test Loss: 4.431823\n",
      "Epoch 93/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7880.99558831  7757.44370835  -997.59311645]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8278.51806591  7931.01250806 -1064.97678468]\n",
      "Train Loss: 3.471695 ---- Test Loss: 4.429919\n",
      "Epoch 94/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7881.18637158  7757.27549553  -997.59473734]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8277.82409952  7931.55776922 -1064.96527756]\n",
      "Train Loss: 3.468335 ---- Test Loss: 4.428503\n",
      "Epoch 95/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7881.3451004   7757.15011231  -997.59700811]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8277.12201258  7932.12968632 -1064.95494525]\n",
      "Train Loss: 3.465068 ---- Test Loss: 4.427496\n",
      "Epoch 96/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7881.47474054  7757.06118017  -997.59971558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8276.41714944  7932.71794894 -1064.94548397]\n",
      "Train Loss: 3.461897 ---- Test Loss: 4.426812\n",
      "Epoch 97/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7881.5784099   7757.00123097  -997.60258745]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8275.71507468  7933.31139972 -1064.93654993]\n",
      "Train Loss: 3.458824 ---- Test Loss: 4.426358\n",
      "Epoch 98/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7881.65910211  7756.96288106  -997.60534885]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8275.02087348  7933.89990093 -1064.92783241]\n",
      "Train Loss: 3.455849 ---- Test Loss: 4.426041\n",
      "Epoch 99/100\n",
      "----------\n",
      "[-7973.8157493   7783.385959   -1005.21113055]\n",
      "[-7881.71954117  7756.93910122  -997.60773009]\n",
      "[-8202.70901443  7975.78315676 -1062.79789273]\n",
      "[-8274.33922898  7934.47398584 -1064.91903654]\n",
      "Train Loss: 3.452971 ---- Test Loss: 4.425775\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch_size\n",
    "num_epoch = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "main(names_FS, test_names, bs, num_epoch, y_FS, y_test, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 0.7389588330748224\n",
      "Localization Error (m) = 97.71598280344698\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
