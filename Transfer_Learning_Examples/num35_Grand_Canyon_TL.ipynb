{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Example on Radar Dataset (Grand Canyon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Training labels:\n",
      "[[ 8.3355 13.325  19.525 ]\n",
      " [18.156  17.388  32.775 ]\n",
      " [ 8.777  24.73   18.582 ]\n",
      " ...\n",
      " [15.785  15.992  29.641 ]\n",
      " [ 3.1736 10.716  12.27  ]\n",
      " [ 4.5552 21.035  14.234 ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 29 # Bonneville Salt Flats\n",
    "names = os.listdir(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names = sorted(names, key=index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'num{scenario_idx}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "scenario_idx_test = 35 # Grand Canyon\n",
    "names_test = os.listdir(f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names_test = sorted(names_test, key=index)\n",
    "y_test = pd.read_csv(f'num{scenario_idx_test}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names_test = y_test.columns[4:7]\n",
    "y_test = y_test[col_names_test].to_numpy()\n",
    "\n",
    "# Define fine-tuning name and labels\n",
    "few_shot_count = 64\n",
    "names_FS = names_test[:few_shot_count]\n",
    "y_FS = y_test[:few_shot_count]\n",
    "\n",
    "# Create training and testing datasets\n",
    "y_train = y[:int(0.9 * len(names))]\n",
    "y_test = y_test[int(0.9 * len(names_test)) - 1:]\n",
    "training_names = names[:int(0.9 * len(names))]\n",
    "test_names = names_test[int(0.9 * len(names_test)) - 1:]\n",
    "\n",
    "print('Training labels:')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.4], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [10851, 215, -5.4]   # Tensor corner\n",
    "rng_res_tr = 59.9585 / 2        # Range resolution\n",
    "az_step_tr = 0.4                # Azimuth step size\n",
    "el_step_tr = 0.01               # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11381, 215, -0.95]  # Tensor corner\n",
    "rng_res_ts = 59.9585 / 2        # Range resolution\n",
    "az_step_ts = 0.4                # Azimuth step size\n",
    "el_step_ts = 0.01               # Elevation step size\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, directory, normalize=True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind * bs, (ind + 1) * bs):\n",
    "        try:\n",
    "            temp = sio.loadmat(directory + names[j])['P']\n",
    "        except:\n",
    "            break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j, :])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 32, 24]           2,048\n",
      "       BatchNorm1d-2               [-1, 32, 24]              64\n",
      "            Conv1d-3               [-1, 64, 10]           6,208\n",
      "       BatchNorm1d-4               [-1, 64, 10]             128\n",
      "            Linear-5                   [-1, 20]           6,420\n",
      "            Linear-6                    [-1, 2]              42\n",
      "               Net-7                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 14,910\n",
      "Trainable params: 14,910\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "        self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "        return output_reg\n",
    "    \n",
    "from torchsummary import summary\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 10  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8219.14628559  7731.56993467 -1006.99025773]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-9081.69542488  7213.07695345  -187.8823746 ]\n",
      "Train Loss: 11.360616 ---- Test Loss: 86.396618\n",
      "Epoch 1/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8194.42767714  7746.51625189 -1006.30048578]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8967.39812062  7305.75192641  -187.38063815]\n",
      "Train Loss: 0.207086 ---- Test Loss: 69.206835\n",
      "Epoch 2/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8190.37833086  7751.3793355  -1006.33617578]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8891.52041431  7318.53298694  -186.56078202]\n",
      "Train Loss: 0.136451 ---- Test Loss: 68.116810\n",
      "Epoch 3/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8189.49821671  7754.39110007 -1006.46390652]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8855.40051635  7322.97342986  -186.15520101]\n",
      "Train Loss: 0.112454 ---- Test Loss: 68.461064\n",
      "Epoch 4/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8186.00358332  7758.87010193 -1006.51239958]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8828.3532192   7355.02115532  -186.14963125]\n",
      "Train Loss: 0.101975 ---- Test Loss: 62.509594\n",
      "Epoch 5/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8186.72899565  7758.13751375 -1006.51441483]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8807.99744119  7391.95379241  -186.28044861]\n",
      "Train Loss: 0.099540 ---- Test Loss: 56.431868\n",
      "Epoch 6/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8189.39275904  7754.07822651 -1006.437876  ]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8780.8563648   7431.19540902  -186.35390832]\n",
      "Train Loss: 0.102561 ---- Test Loss: 52.524009\n",
      "Epoch 7/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8190.44006594  7755.13646503 -1006.57067156]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8756.35387326  7449.59321687  -186.24407738]\n",
      "Train Loss: 0.107593 ---- Test Loss: 52.251046\n",
      "Epoch 8/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8190.99200857  7758.68962709 -1006.82446595]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8741.43816556  7461.02552023  -186.18028302]\n",
      "Train Loss: 0.107370 ---- Test Loss: 52.804389\n",
      "Epoch 9/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8192.08296624  7760.47636733 -1007.00479904]\n",
      "[-8519.12322118  8442.27271505  -194.29731319]\n",
      "[-8732.5365339   7467.1206673   -186.13477704]\n",
      "Train Loss: 0.099412 ---- Test Loss: 53.466667\n",
      "677.616847038269\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 7.547080267597325\n",
      "Localization Error (m) = 641.2023282819082\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Regression CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.conv1.weight.requires_grad = False\n",
    "model.module.conv1.bias.requires_grad = False\n",
    "model.module.batchnorm1.weight.requires_grad = False\n",
    "model.module.batchnorm1.bias.requires_grad = False\n",
    "\n",
    "model.module.conv2.weight.requires_grad = False\n",
    "model.module.conv2.bias.requires_grad = False\n",
    "model.module.batchnorm2.weight.requires_grad = False\n",
    "model.module.batchnorm2.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8182.04147984  7381.26913786 -1034.22520792]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8786.21228455  7370.55097882  -177.47689676]\n",
      "Train Loss: 22.160118 ---- Test Loss: 68.896415\n",
      "Epoch 1/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8169.29734079  7409.03395757 -1035.08621328]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8826.32401825  7335.56965905  -177.60641232]\n",
      "Train Loss: 19.933058 ---- Test Loss: 71.555292\n",
      "Epoch 2/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8156.37613407  7436.94331186 -1035.95129817]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8858.64637498  7324.57792346  -177.88294614]\n",
      "Train Loss: 17.933056 ---- Test Loss: 68.775848\n",
      "Epoch 3/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8143.28511797  7464.89749339 -1036.81479212]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8878.58386272  7341.68445265  -178.28942432]\n",
      "Train Loss: 16.169889 ---- Test Loss: 65.308708\n",
      "Epoch 4/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8130.02915012  7492.76710842 -1037.66889275]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8886.52375998  7376.49742239  -178.72775097]\n",
      "Train Loss: 14.650675 ---- Test Loss: 61.231278\n",
      "Epoch 5/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8116.61011915  7520.38480807 -1038.50307397]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8877.27047587  7421.93224218  -179.06777075]\n",
      "Train Loss: 13.378574 ---- Test Loss: 56.349343\n",
      "Epoch 6/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8103.02563906  7547.53921805 -1039.30357708]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8858.47597144  7469.64833368  -179.31985205]\n",
      "Train Loss: 12.351089 ---- Test Loss: 50.759255\n",
      "Epoch 7/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8089.27046845  7573.97114723 -1040.05323173]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8835.24905538  7520.87435746  -179.55804281]\n",
      "Train Loss: 11.558281 ---- Test Loss: 45.682181\n",
      "Epoch 8/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8075.34492314  7599.37805036 -1040.73228417]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8812.13325829  7573.70963952  -179.81766762]\n",
      "Train Loss: 10.981455 ---- Test Loss: 41.282989\n",
      "Epoch 9/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8061.26978484  7623.42923893 -1041.32036348]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8784.28041391  7627.08029575  -180.03141094]\n",
      "Train Loss: 10.593203 ---- Test Loss: 37.470224\n",
      "Epoch 10/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8047.10619469  7645.7954233  -1041.79972227]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8755.58756244  7677.31720958  -180.20796695]\n",
      "Train Loss: 10.359465 ---- Test Loss: 34.304070\n",
      "Epoch 11/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8032.97308313  7666.17983727 -1042.15843187]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8727.72834848  7723.23827033  -180.35418437]\n",
      "Train Loss: 10.243467 ---- Test Loss: 31.705255\n",
      "Epoch 12/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8019.04847355  7684.3444503  -1042.39218835]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8698.40344685  7768.18974967  -180.47719859]\n",
      "Train Loss: 10.209988 ---- Test Loss: 29.653962\n",
      "Epoch 13/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8005.55518028  7700.11976933 -1042.50402395]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8671.77768698  7806.95462986  -180.57090767]\n",
      "Train Loss: 10.228507 ---- Test Loss: 27.971030\n",
      "Epoch 14/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7992.73510598  7713.40558061 -1042.50268014]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8647.60314561  7842.46116533  -180.66171418]\n",
      "Train Loss: 10.274572 ---- Test Loss: 26.638344\n",
      "Epoch 15/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7980.82378188  7724.16596077 -1042.4006386 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8624.45974782  7875.33314786  -180.73920681]\n",
      "Train Loss: 10.329856 ---- Test Loss: 25.656269\n",
      "Epoch 16/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7970.02946435  7732.42497843 -1042.21250761]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8603.7341946   7903.93959101  -180.80168852]\n",
      "Train Loss: 10.381596 ---- Test Loss: 25.003567\n",
      "Epoch 17/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7960.52166499  7738.26157499 -1041.95398279]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8584.20762613  7929.43433641  -180.84674958]\n",
      "Train Loss: 10.421855 ---- Test Loss: 24.566768\n",
      "Epoch 18/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7952.42392409  7741.80618188 -1041.64118988]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8566.73297461  7951.31215936  -180.87834989]\n",
      "Train Loss: 10.446700 ---- Test Loss: 24.316444\n",
      "Epoch 19/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7945.81365994  7743.23495984 -1041.29032675]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8551.42621909  7970.08151109  -180.90271746]\n",
      "Train Loss: 10.455327 ---- Test Loss: 24.157526\n",
      "Epoch 20/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7940.72512679  7742.76286994 -1040.91741496]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8537.8513224   7986.28255959  -180.92027392]\n",
      "Train Loss: 10.449113 ---- Test Loss: 24.096810\n",
      "Epoch 21/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7937.15478361  7740.63498491 -1040.53807798]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8526.194788    7999.99812517  -180.93374442]\n",
      "Train Loss: 10.430643 ---- Test Loss: 24.087951\n",
      "Epoch 22/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7935.06597921  7737.11742317 -1040.167234  ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8516.42079285  8011.48226799  -180.94519583]\n",
      "Train Loss: 10.402834 ---- Test Loss: 24.124844\n",
      "Epoch 23/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7934.39450658  7732.48699342 -1039.81875393]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8509.20767406  8020.02122326  -180.95451621]\n",
      "Train Loss: 10.368247 ---- Test Loss: 24.190950\n",
      "Epoch 24/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7935.05367506  7727.01847798 -1039.50492978]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8503.8081575   8026.74107795  -180.96508371]\n",
      "Train Loss: 10.328737 ---- Test Loss: 24.258705\n",
      "Epoch 25/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7936.93539389  7720.97707266 -1039.23601513]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8500.03600198  8032.09212861  -180.97950182]\n",
      "Train Loss: 10.285432 ---- Test Loss: 24.315801\n",
      "Epoch 26/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7939.9133905   7714.60814231 -1039.0197389 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8497.66442893  8036.47368648  -180.99941226]\n",
      "Train Loss: 10.238979 ---- Test Loss: 24.392650\n",
      "Epoch 27/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7943.84588268  7708.13113079 -1038.86106224]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8496.55972539  8039.94113072  -181.02386921]\n",
      "Train Loss: 10.189942 ---- Test Loss: 24.498244\n",
      "Epoch 28/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7948.577693    7701.73632894 -1038.76209304]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8496.5049734   8042.75839497  -181.05322258]\n",
      "Train Loss: 10.139147 ---- Test Loss: 24.638709\n",
      "Epoch 29/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7953.94515876  7695.58215554 -1038.72223018]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8497.41531332  8044.93125738  -181.08657034]\n",
      "Train Loss: 10.087882 ---- Test Loss: 24.806835\n",
      "Epoch 30/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7959.77896929  7689.7976173  -1038.73851355]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8498.9683838   8046.88064942  -181.12476411]\n",
      "Train Loss: 10.037927 ---- Test Loss: 24.998464\n",
      "Epoch 31/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7965.90929842  7684.48264366 -1038.80599534]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8500.99373134  8048.63554175  -181.16619576]\n",
      "Train Loss: 9.991374 ---- Test Loss: 25.218618\n",
      "Epoch 32/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7972.1699911   7679.71010658 -1038.91816222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8503.3591296   8050.17487103  -181.20915545]\n",
      "Train Loss: 9.950361 ---- Test Loss: 25.463505\n",
      "Epoch 33/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7978.40127445  7675.52899939 -1039.06733602]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8505.76228214  8051.94761267  -181.255023  ]\n",
      "Train Loss: 9.916780 ---- Test Loss: 25.737388\n",
      "Epoch 34/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7984.45294746  7671.96615494 -1039.24501306]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8508.15808866  8053.87468252  -181.30244999]\n",
      "Train Loss: 9.891981 ---- Test Loss: 26.032691\n",
      "Epoch 35/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.18768349  7669.02758938 -1039.4421874 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8510.46069993  8055.91554439  -181.35004023]\n",
      "Train Loss: 9.876574 ---- Test Loss: 26.345038\n",
      "Epoch 36/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7995.48252762  7666.70132021 -1039.64964737]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8512.52873213  8058.0979293   -181.39649971]\n",
      "Train Loss: 9.870319 ---- Test Loss: 26.666188\n",
      "Epoch 37/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8000.23216741  7664.9591812  -1039.85832558]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8514.24249509  8060.43069879  -181.44057777]\n",
      "Train Loss: 9.872131 ---- Test Loss: 26.986068\n",
      "Epoch 38/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8004.34855484  7663.76103263 -1040.0595554 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8515.50556444  8062.90739404  -181.48112287]\n",
      "Train Loss: 9.880182 ---- Test Loss: 27.295050\n",
      "Epoch 39/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8007.76568271  7663.05531598 -1040.2454369 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8516.24636132  8065.50957863  -181.51713606]\n",
      "Train Loss: 9.892145 ---- Test Loss: 27.584682\n",
      "Epoch 40/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8010.4389251   7662.78441318 -1040.40914417]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8516.41901276  8068.21037425  -181.54781839]\n",
      "Train Loss: 9.905465 ---- Test Loss: 27.845796\n",
      "Epoch 41/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8012.34630287  7662.88717074 -1040.5451764 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8516.00209811  8070.97924648  -181.57260757]\n",
      "Train Loss: 9.917701 ---- Test Loss: 28.070519\n",
      "Epoch 42/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8013.48864711  7663.30244207 -1040.64959833]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8514.99910308  8073.78392039  -181.59120322]\n",
      "Train Loss: 9.926817 ---- Test Loss: 28.253246\n",
      "Epoch 43/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8013.88868632  7663.97143351 -1040.72012842]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8513.42689595  8076.60896403  -181.60363497]\n",
      "Train Loss: 9.931435 ---- Test Loss: 28.391950\n",
      "Epoch 44/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8013.59014135  7664.83948347 -1040.75618959]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8511.31931005  8079.44695536  -181.61020853]\n",
      "Train Loss: 9.930950 ---- Test Loss: 28.486303\n",
      "Epoch 45/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8012.65386037  7665.85872239 -1040.75881577]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8508.76334891  8082.23162626  -181.61119905]\n",
      "Train Loss: 9.925543 ---- Test Loss: 28.540390\n",
      "Epoch 46/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8011.15602072  7666.9878318  -1040.73051023]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8505.83566303  8084.94287037  -181.60725598]\n",
      "Train Loss: 9.916075 ---- Test Loss: 28.558851\n",
      "Epoch 47/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8009.18537693  7668.19110928 -1040.67499451]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8502.62159257  8087.56607639  -181.59918359]\n",
      "Train Loss: 9.903879 ---- Test Loss: 28.547686\n",
      "Epoch 48/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8006.83768549  7669.44076217 -1040.59697594]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8499.21335943  8090.08981745  -181.58789587]\n",
      "Train Loss: 9.890521 ---- Test Loss: 28.514079\n",
      "Epoch 49/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8004.21457566  7670.71384404 -1040.50186986]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8495.70559542  8092.50616719  -181.57436963]\n",
      "Train Loss: 9.877544 ---- Test Loss: 28.465752\n",
      "Epoch 50/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-8001.41886513  7671.9921105  -1040.39547095]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8492.19201661  8094.80971423  -181.55959673]\n",
      "Train Loss: 9.866254 ---- Test Loss: 28.409942\n",
      "Epoch 51/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7998.55161736  7673.26107085 -1040.28369146]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8488.76306856  8096.99575507  -181.54453847]\n",
      "Train Loss: 9.857557 ---- Test Loss: 28.353371\n",
      "Epoch 52/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7995.70907883  7674.50870001 -1040.17226995]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8485.50160268  8099.06112345  -181.5300863 ]\n",
      "Train Loss: 9.851894 ---- Test Loss: 28.301792\n",
      "Epoch 53/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7992.97978568  7675.72449746 -1040.0665149 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8482.48151271  8101.00178223  -181.51702104]\n",
      "Train Loss: 9.849232 ---- Test Loss: 28.259873\n",
      "Epoch 54/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.44243165  7676.89823745 -1039.97108021]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8479.76454052  8102.81340735  -181.50598372]\n",
      "Train Loss: 9.849142 ---- Test Loss: 28.231161\n",
      "Epoch 55/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7988.16355584  7678.01914771 -1039.88975689]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.4002805   8104.48940577  -181.49745478]\n",
      "Train Loss: 9.850931 ---- Test Loss: 28.217680\n",
      "Epoch 56/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7986.19601109  7679.0754222  -1039.82533941]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8475.42326232  8106.02240665  -181.4917377 ]\n",
      "Train Loss: 9.853781 ---- Test Loss: 28.220364\n",
      "Epoch 57/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7984.57772059  7680.05462419 -1039.77956966]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8473.85381312  8107.4035371   -181.48896123]\n",
      "Train Loss: 9.856893 ---- Test Loss: 28.239079\n",
      "Epoch 58/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7983.33215098  7680.9424701  -1039.75309149]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8472.69735418  8108.62280335  -181.48907581]\n",
      "Train Loss: 9.859612 ---- Test Loss: 28.272715\n",
      "Epoch 59/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7982.46782445  7681.72397252 -1039.74549348]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8471.94603638  8109.67042396  -181.49188635]\n",
      "Train Loss: 9.861488 ---- Test Loss: 28.319133\n",
      "Epoch 60/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7981.97872908  7682.38484413 -1039.75542914]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8471.57922127  8110.53685424  -181.49705795]\n",
      "Train Loss: 9.862316 ---- Test Loss: 28.375812\n",
      "Epoch 61/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7981.84685431  7682.91065893 -1039.78073447]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8471.56552542  8111.21453306  -181.50415751]\n",
      "Train Loss: 9.862100 ---- Test Loss: 28.439982\n",
      "Epoch 62/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7982.04278285  7683.28927132 -1039.81862569]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8471.864737    8111.69778557  -181.51267406]\n",
      "Train Loss: 9.861018 ---- Test Loss: 28.508278\n",
      "Epoch 63/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7982.52842956  7683.51059971 -1039.86587016]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8472.42933583  8111.98537753  -181.52206302]\n",
      "Train Loss: 9.859344 ---- Test Loss: 28.577209\n",
      "Epoch 64/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7983.25769495  7683.56952613 -1039.91901886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8473.20788939  8112.07946096  -181.53177271]\n",
      "Train Loss: 9.857388 ---- Test Loss: 28.643342\n",
      "Epoch 65/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7984.18127337  7683.46378537 -1039.97459336]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8474.14608126  8111.98765618  -181.54127802]\n",
      "Train Loss: 9.855421 ---- Test Loss: 28.703482\n",
      "Epoch 66/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7985.24657343  7683.19718051 -1040.02928891]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8475.18959742  8111.72236708  -181.55010513]\n",
      "Train Loss: 9.853657 ---- Test Loss: 28.754841\n",
      "Epoch 67/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7986.40077138  7682.77905157 -1040.08014537]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8476.28613666  8111.30163011  -181.55786298]\n",
      "Train Loss: 9.852221 ---- Test Loss: 28.795221\n",
      "Epoch 68/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7987.59364123  7682.22352243 -1040.12468886]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.38679043  8110.74805616  -181.56424719]\n",
      "Train Loss: 9.851156 ---- Test Loss: 28.823128\n",
      "Epoch 69/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7988.77774616  7681.5505027  -1040.16100918]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8478.4483782   8110.08846519  -181.56906222]\n",
      "Train Loss: 9.850441 ---- Test Loss: 28.837863\n",
      "Epoch 70/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.91123032  7680.7846809  -1040.18788275]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8479.43292346  8109.35402442  -181.57221694]\n",
      "Train Loss: 9.850019 ---- Test Loss: 28.839523\n",
      "Epoch 71/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.9579876   7679.95444812 -1040.20471379]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8480.31037953  8108.57744004  -181.57372506]\n",
      "Train Loss: 9.849813 ---- Test Loss: 28.828999\n",
      "Epoch 72/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7991.88955025  7679.09083577 -1040.21159315]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8481.05795767  8107.79295257  -181.57369764]\n",
      "Train Loss: 9.849754 ---- Test Loss: 28.807860\n",
      "Epoch 73/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7992.68416823  7678.22641474 -1040.20916467]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8481.66017932  8107.03400067  -181.57231869]\n",
      "Train Loss: 9.849785 ---- Test Loss: 28.778252\n",
      "Epoch 74/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7993.32801586  7677.39358051 -1040.19859584]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8482.10926989  8106.33251155  -181.56984216]\n",
      "Train Loss: 9.849866 ---- Test Loss: 28.742689\n",
      "Epoch 75/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7993.81442895  7676.62299609 -1040.1814255 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8482.40367221  8105.71726206  -181.56655782]\n",
      "Train Loss: 9.849972 ---- Test Loss: 28.703898\n",
      "Epoch 76/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7994.14320224  7675.94250639 -1040.1594463 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8482.54854724  8105.21198164  -181.56277666]\n",
      "Train Loss: 9.850084 ---- Test Loss: 28.664632\n",
      "Epoch 77/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7994.31996674  7675.37572361 -1040.13457109]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8482.5540674   8104.83502197  -181.55880838]\n",
      "Train Loss: 9.850189 ---- Test Loss: 28.627480\n",
      "Epoch 78/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7994.35554648  7674.94080031 -1040.10871006]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8482.43370593  8104.59924019  -181.55494096]\n",
      "Train Loss: 9.850267 ---- Test Loss: 28.594712\n",
      "Epoch 79/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7994.26462022  7674.64976421 -1040.08363715]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8482.20534042  8104.50960159  -181.55142745]\n",
      "Train Loss: 9.850298 ---- Test Loss: 28.568190\n",
      "Epoch 80/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7994.06506085  7674.50788995 -1040.06090455]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8481.88852318  8104.56446522  -181.54846916]\n",
      "Train Loss: 9.850267 ---- Test Loss: 28.549240\n",
      "Epoch 81/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7993.77633614  7674.51400971 -1040.04175456]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8481.50302124  8104.75678429  -181.54621205]\n",
      "Train Loss: 9.850163 ---- Test Loss: 28.538644\n",
      "Epoch 82/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7993.41911875  7674.66014199 -1040.02706881]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8481.06987427  8105.07209298  -181.54473711]\n",
      "Train Loss: 9.849983 ---- Test Loss: 28.536610\n",
      "Epoch 83/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7993.01387697  7674.93259413 -1040.01734421]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8480.6088191   8105.49166192  -181.54406511]\n",
      "Train Loss: 9.849740 ---- Test Loss: 28.542783\n",
      "Epoch 84/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7992.58044845  7675.31237026 -1040.01269029]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8480.13908531  8105.99195972  -181.54415978]\n",
      "Train Loss: 9.849459 ---- Test Loss: 28.556355\n",
      "Epoch 85/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7992.13782039  7675.77598048 -1040.01286662]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8479.67723081  8106.54733491  -181.5449322 ]\n",
      "Train Loss: 9.849172 ---- Test Loss: 28.576054\n",
      "Epoch 86/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7991.70198806  7676.29829078 -1040.01732284]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8479.23777555  8107.13105077  -181.54625895]\n",
      "Train Loss: 9.848913 ---- Test Loss: 28.600341\n",
      "Epoch 87/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7991.28784004  7676.85146619 -1040.02525747]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8478.8335635   8107.71519678  -181.54798517]\n",
      "Train Loss: 9.848709 ---- Test Loss: 28.627456\n",
      "Epoch 88/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.90724141  7677.4084464  -1040.03571409]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8478.47386523  8108.27449217  -181.5499441 ]\n",
      "Train Loss: 9.848581 ---- Test Loss: 28.655583\n",
      "Epoch 89/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.56930137  7677.94361074 -1040.04764276]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8478.16629662  8108.78531079  -181.55196806]\n",
      "Train Loss: 9.848536 ---- Test Loss: 28.682957\n",
      "Epoch 90/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.28131574  7678.43327405 -1040.05999621]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.9149995   8109.2286973   -181.55390044]\n",
      "Train Loss: 9.848563 ---- Test Loss: 28.707954\n",
      "Epoch 91/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.04729455  7678.85816099 -1040.0717913 ]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.72260525  8109.5892206   -181.55560543]\n",
      "Train Loss: 9.848639 ---- Test Loss: 28.729231\n",
      "Epoch 92/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.86974061  7679.20266103 -1040.08218113]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.58909265  8109.85690966  -181.55697598]\n",
      "Train Loss: 9.848741 ---- Test Loss: 28.745770\n",
      "Epoch 93/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.74887168  7679.45653882 -1040.09051383]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.51303756  8110.02693734  -181.55794438]\n",
      "Train Loss: 9.848838 ---- Test Loss: 28.756945\n",
      "Epoch 94/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.68253663  7679.61506018 -1040.09633523]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.49074117  8110.09986885  -181.55847525]\n",
      "Train Loss: 9.848909 ---- Test Loss: 28.762506\n",
      "Epoch 95/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.66768827  7679.67836231 -1040.09944764]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.5177988   8110.08035248  -181.55856904]\n",
      "Train Loss: 9.848940 ---- Test Loss: 28.762607\n",
      "Epoch 96/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.69970645  7679.65121309 -1040.09984838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.58831423  8109.97831376  -181.558266  ]\n",
      "Train Loss: 9.848927 ---- Test Loss: 28.757751\n",
      "Epoch 97/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.77321754  7679.54277538 -1040.09776988]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.69561163  8109.806393    -181.55762674]\n",
      "Train Loss: 9.848874 ---- Test Loss: 28.748720\n",
      "Epoch 98/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7989.88190965  7679.36581465 -1040.09361555]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.83304996  8109.57942628  -181.55673575]\n",
      "Train Loss: 9.848797 ---- Test Loss: 28.736510\n",
      "Epoch 99/100\n",
      "----------\n",
      "[-8030.11680344  7623.28012548 -1039.18831649]\n",
      "[-7990.01898224  7679.13528809 -1040.08789834]\n",
      "[-8239.59111431  8056.30503705  -178.33329667]\n",
      "[-8477.99271053  8109.31470243  -181.55568949]\n",
      "Train Loss: 9.848711 ---- Test Loss: 28.722237\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch_size\n",
    "num_epoch = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "main(names_FS, test_names, bs, num_epoch, y_FS, y_test, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 3.342892600922403\n",
      "Localization Error (m) = 337.3033336528585\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
