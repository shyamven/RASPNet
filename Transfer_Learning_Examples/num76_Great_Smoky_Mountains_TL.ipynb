{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Example on Radar Dataset (Great Smoky Mountains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Training labels:\n",
      "[[ 8.3355 13.325  19.525 ]\n",
      " [18.156  17.388  32.775 ]\n",
      " [ 8.777  24.73   18.582 ]\n",
      " ...\n",
      " [15.785  15.992  29.641 ]\n",
      " [ 3.1736 10.716  12.27  ]\n",
      " [ 4.5552 21.035  14.234 ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 29 # Bonneville Salt Flats\n",
    "names = os.listdir(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names = sorted(names, key=index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'num{scenario_idx}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "scenario_idx_test = 76 # Great Smoky Mountains\n",
    "names_test = os.listdir(f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names_test = sorted(names_test, key=index)\n",
    "y_test = pd.read_csv(f'num{scenario_idx_test}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names_test = y_test.columns[4:7]\n",
    "y_test = y_test[col_names_test].to_numpy()\n",
    "\n",
    "# Define fine-tuning name and labels\n",
    "few_shot_count = 64\n",
    "names_FS = names_test[:few_shot_count]\n",
    "y_FS = y_test[:few_shot_count]\n",
    "\n",
    "# Create training and testing datasets\n",
    "y_train = y[:int(0.9 * len(names))]\n",
    "y_test = y_test[int(0.9 * len(names_test)) - 1:]\n",
    "training_names = names[:int(0.9 * len(names))]\n",
    "test_names = names_test[int(0.9 * len(names_test)) - 1:]\n",
    "\n",
    "print('Training labels:')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.4], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [10851, 215, -5.4]   # Tensor corner\n",
    "rng_res_tr = 59.9585 / 2        # Range resolution\n",
    "az_step_tr = 0.4                # Azimuth step size\n",
    "el_step_tr = 0.01               # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11388, 215, -6.15]  # Tensor corner\n",
    "rng_res_ts = 59.9585 / 2        # Range resolution\n",
    "az_step_ts = 0.4                # Azimuth step size\n",
    "el_step_ts = 0.01               # Elevation step size\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, directory, normalize=True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind * bs, (ind + 1) * bs):\n",
    "        try:\n",
    "            temp = sio.loadmat(directory + names[j])['P']\n",
    "        except:\n",
    "            break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j, :])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 32, 24]           2,048\n",
      "       BatchNorm1d-2               [-1, 32, 24]              64\n",
      "            Conv1d-3               [-1, 64, 10]           6,208\n",
      "       BatchNorm1d-4               [-1, 64, 10]             128\n",
      "            Linear-5                   [-1, 20]           6,420\n",
      "            Linear-6                    [-1, 2]              42\n",
      "               Net-7                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 14,910\n",
      "Trainable params: 14,910\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "        self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "        return output_reg\n",
    "    \n",
    "from torchsummary import summary\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 10  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8207.26235776  7733.10778354 -1006.31216813]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8801.11340659  7387.31225422 -1134.4771778 ]\n",
      "Train Loss: 8.487845 ---- Test Loss: 64.618457\n",
      "Epoch 1/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8205.59041975  7727.03001869 -1005.83167212]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8789.46357174  7407.7753226  -1134.89738028]\n",
      "Train Loss: 0.190858 ---- Test Loss: 63.072532\n",
      "Epoch 2/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8205.22340794  7726.19132579 -1005.7565183 ]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8784.78876847  7416.32199312 -1135.08864916]\n",
      "Train Loss: 0.134331 ---- Test Loss: 58.491567\n",
      "Epoch 3/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8208.47782392  7721.71008594 -1005.69392866]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8797.67291338  7400.38480342 -1135.04741968]\n",
      "Train Loss: 0.115166 ---- Test Loss: 58.155142\n",
      "Epoch 4/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8205.8071481   7724.74942585 -1005.70624125]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8797.65479404  7395.06030899 -1134.70772102]\n",
      "Train Loss: 0.112591 ---- Test Loss: 58.082277\n",
      "Epoch 5/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.23534838  7736.20983697 -1005.78618287]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8791.18864065  7403.82321378 -1134.77622567]\n",
      "Train Loss: 0.117338 ---- Test Loss: 56.346408\n",
      "Epoch 6/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8194.12999055  7742.25587353 -1006.02003316]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8769.95458825  7422.06178268 -1134.33593095]\n",
      "Train Loss: 0.122423 ---- Test Loss: 53.972414\n",
      "Epoch 7/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.75645471  7745.24034607 -1006.37331177]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8753.68832027  7448.72786279 -1134.81492413]\n",
      "Train Loss: 0.122568 ---- Test Loss: 51.898384\n",
      "Epoch 8/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8196.67504909  7754.34581933 -1006.92628308]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8744.04016013  7471.52219102 -1135.55030725]\n",
      "Train Loss: 0.125339 ---- Test Loss: 49.235225\n",
      "Epoch 9/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8195.38462084  7761.15574283 -1007.26040538]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8731.10768911  7496.60430711 -1136.19152722]\n",
      "Train Loss: 0.124375 ---- Test Loss: 47.261231\n",
      "661.4941940307617\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 8.37928448728461\n",
      "Localization Error (m) = 691.347446648395\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Regression CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.conv1.weight.requires_grad = False\n",
    "model.module.conv1.bias.requires_grad = False\n",
    "model.module.batchnorm1.weight.requires_grad = False\n",
    "model.module.batchnorm1.bias.requires_grad = False\n",
    "\n",
    "model.module.conv2.weight.requires_grad = False\n",
    "model.module.conv2.bias.requires_grad = False\n",
    "model.module.batchnorm2.weight.requires_grad = False\n",
    "model.module.batchnorm2.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7920.53664558  7806.68647488  -951.78932896]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8712.19533732  7481.03148732 -1127.96127404]\n",
      "Train Loss: 24.690163 ---- Test Loss: 47.718513\n",
      "Epoch 1/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7904.51969873  7813.41984623  -951.21855363]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8684.00799282  7487.68242512 -1126.28865401]\n",
      "Train Loss: 23.312172 ---- Test Loss: 46.488462\n",
      "Epoch 2/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7889.87712386  7820.05226781  -950.72721716]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8652.16414354  7509.63447894 -1125.33382938]\n",
      "Train Loss: 22.059099 ---- Test Loss: 44.359572\n",
      "Epoch 3/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7877.16607978  7826.7241956   -950.35725994]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8620.40347791  7536.68340015 -1124.72664265]\n",
      "Train Loss: 20.916197 ---- Test Loss: 41.729415\n",
      "Epoch 4/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7866.65676259  7833.62743768  -950.13623105]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8587.4674508   7569.67028252 -1124.43302309]\n",
      "Train Loss: 19.866230 ---- Test Loss: 38.828023\n",
      "Epoch 5/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7858.44405846  7840.78118184  -950.07064476]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8552.61409098  7608.53433645 -1124.4008333 ]\n",
      "Train Loss: 18.893639 ---- Test Loss: 35.920823\n",
      "Epoch 6/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7852.41507769  7847.97039227  -950.14029515]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8519.70549971  7647.3985162  -1124.53418513]\n",
      "Train Loss: 17.988997 ---- Test Loss: 33.097277\n",
      "Epoch 7/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7848.35436142  7854.85635424  -950.31131394]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8487.88476798  7687.36084499 -1124.84137932]\n",
      "Train Loss: 17.148853 ---- Test Loss: 30.451543\n",
      "Epoch 8/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7846.07799426  7861.07555257  -950.5502741 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8459.5044416   7724.66383436 -1125.24419823]\n",
      "Train Loss: 16.371996 ---- Test Loss: 28.035932\n",
      "Epoch 9/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7845.45185394  7866.28334487  -950.82794436]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8434.4865811   7759.06358147 -1125.71562396]\n",
      "Train Loss: 15.656253 ---- Test Loss: 25.887544\n",
      "Epoch 10/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7846.34294141  7870.17659799  -951.11773622]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8411.86118452  7791.01134911 -1126.21105474]\n",
      "Train Loss: 14.997794 ---- Test Loss: 24.016718\n",
      "Epoch 11/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7848.61632058  7872.50300737  -951.39610618]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8391.3675601   7819.72268513 -1126.65579095]\n",
      "Train Loss: 14.391035 ---- Test Loss: 22.419045\n",
      "Epoch 12/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7852.13181136  7873.07161747  -951.64300926]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8372.80310674  7845.67547404 -1127.06395783]\n",
      "Train Loss: 13.829103 ---- Test Loss: 21.058318\n",
      "Epoch 13/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7856.75740555  7871.76992667  -951.84375179]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8356.81692492  7867.92904718 -1127.41591589]\n",
      "Train Loss: 13.304964 ---- Test Loss: 19.901261\n",
      "Epoch 14/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7862.38282906  7868.58273656  -951.99094672]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8343.7998355   7886.47168968 -1127.73565481]\n",
      "Train Loss: 12.812773 ---- Test Loss: 18.932437\n",
      "Epoch 15/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7868.92614728  7863.59480583  -952.08505484]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.99057366  7901.78525474 -1127.99874891]\n",
      "Train Loss: 12.348850 ---- Test Loss: 18.127501\n",
      "Epoch 16/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7876.32534214  7856.98078864  -952.13323646]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8324.45894467  7913.85515499 -1128.20733906]\n",
      "Train Loss: 11.911963 ---- Test Loss: 17.458213\n",
      "Epoch 17/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7884.53093369  7848.98781059  -952.14781309]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8318.21805013  7922.96417524 -1128.38004182]\n",
      "Train Loss: 11.502928 ---- Test Loss: 16.893390\n",
      "Epoch 18/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7893.50658965  7839.9357202   -952.14627716]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8313.92340122  7929.74544093 -1128.53424959]\n",
      "Train Loss: 11.123875 ---- Test Loss: 16.410466\n",
      "Epoch 19/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7903.18551315  7830.14234898  -952.14409274]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8311.14751913  7934.93596801 -1128.68896882]\n",
      "Train Loss: 10.775438 ---- Test Loss: 15.991112\n",
      "Epoch 20/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7913.48356527  7819.94240933  -952.1565971 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8309.88660029  7938.62696037 -1128.84979762]\n",
      "Train Loss: 10.457903 ---- Test Loss: 15.620199\n",
      "Epoch 21/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7924.28609034  7809.65376339  -952.19613855]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8309.63983767  7941.51064276 -1129.02795454]\n",
      "Train Loss: 10.171463 ---- Test Loss: 15.287206\n",
      "Epoch 22/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7935.44012393  7799.55136685  -952.27001882]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8310.38817308  7943.59054695 -1129.22225178]\n",
      "Train Loss: 9.915203 ---- Test Loss: 14.981426\n",
      "Epoch 23/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7946.76529242  7789.86635393  -952.38110009]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8311.93767034  7945.07292451 -1129.43288497]\n",
      "Train Loss: 9.688078 ---- Test Loss: 14.697614\n",
      "Epoch 24/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7958.06047778  7780.77991001  -952.52785375]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8314.09003817  7946.19222744 -1129.66168475]\n",
      "Train Loss: 9.488741 ---- Test Loss: 14.432157\n",
      "Epoch 25/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7969.11395521  7772.42276805  -952.7049699 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8316.55756386  7947.17756949 -1129.90377828]\n",
      "Train Loss: 9.315575 ---- Test Loss: 14.184148\n",
      "Epoch 26/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7979.71600695  7764.87777489  -952.9043106 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8319.05154887  7948.18929818 -1130.14954984]\n",
      "Train Loss: 9.166627 ---- Test Loss: 13.952311\n",
      "Epoch 27/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7989.67198376  7758.18452053  -953.11601912]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8321.33800555  7949.29887641 -1130.38722889]\n",
      "Train Loss: 9.039578 ---- Test Loss: 13.736258\n",
      "Epoch 28/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7998.81425482  7752.34843875  -953.32982918]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8323.25320489  7950.52434814 -1130.60640597]\n",
      "Train Loss: 8.931775 ---- Test Loss: 13.535432\n",
      "Epoch 29/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8007.01345094  7747.3467509   -953.53614003]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8324.76642877  7951.82501533 -1130.80213417]\n",
      "Train Loss: 8.840402 ---- Test Loss: 13.349901\n",
      "Epoch 30/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8014.18833962  7743.14121964  -953.72740861]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8325.83190796  7953.16652218 -1130.96883077]\n",
      "Train Loss: 8.762769 ---- Test Loss: 13.179606\n",
      "Epoch 31/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8020.3129103   7739.68567043  -953.89905151]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8326.38121614  7954.60837424 -1131.10567548]\n",
      "Train Loss: 8.696593 ---- Test Loss: 13.024196\n",
      "Epoch 32/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8025.41583611  7736.93320132  -954.04985221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8326.44976597  7956.15273643 -1131.21533739]\n",
      "Train Loss: 8.640176 ---- Test Loss: 12.882621\n",
      "Epoch 33/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8029.57402433  7734.83676055  -954.1816026 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8326.13618562  7957.80610266 -1131.30527177]\n",
      "Train Loss: 8.592516 ---- Test Loss: 12.753633\n",
      "Epoch 34/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.93037226  7733.36098559  -954.30089983]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8325.54503839  7959.5670243  -1131.38281655]\n",
      "Train Loss: 8.553005 ---- Test Loss: 12.636240\n",
      "Epoch 35/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8035.62960983  7732.45461278  -954.41354872]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8324.80664974  7961.39162333 -1131.45425595]\n",
      "Train Loss: 8.520977 ---- Test Loss: 12.529365\n",
      "Epoch 36/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8037.83265342  7732.05821077  -954.52589773]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8324.05239946  7963.26611406 -1131.52798575]\n",
      "Train Loss: 8.495828 ---- Test Loss: 12.431944\n",
      "Epoch 37/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.70360655  7732.10430512  -954.64403693]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8323.41273823  7965.14252483 -1131.61000789]\n",
      "Train Loss: 8.476755 ---- Test Loss: 12.343210\n",
      "Epoch 38/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8041.39216648  7732.50998714  -954.77226689]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8322.9894481   7966.96646668 -1131.70384669]\n",
      "Train Loss: 8.462744 ---- Test Loss: 12.262545\n",
      "Epoch 39/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8043.02555376  7733.17928167  -954.91273546]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8322.85966706  7968.64866218 -1131.80890267]\n",
      "Train Loss: 8.452679 ---- Test Loss: 12.190077\n",
      "Epoch 40/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8044.70066291  7734.00529514  -955.06507695]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8323.06779471  7970.10961381 -1131.92291477]\n",
      "Train Loss: 8.445452 ---- Test Loss: 12.126403\n",
      "Epoch 41/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8046.47913312  7734.87702363  -955.22650964]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8323.6278237   7971.26824275 -1132.04135812]\n",
      "Train Loss: 8.440045 ---- Test Loss: 12.072377\n",
      "Epoch 42/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8048.38747862  7735.68732866  -955.39231605]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8324.52974976  7972.05824764 -1132.15901333]\n",
      "Train Loss: 8.435648 ---- Test Loss: 12.028975\n",
      "Epoch 43/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8050.42834414  7736.34564667  -955.5572911 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8325.73552264  7972.43596287 -1132.2702152 ]\n",
      "Train Loss: 8.431511 ---- Test Loss: 11.996851\n",
      "Epoch 44/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8052.57419697  7736.78476038  -955.71575434]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8327.1871299   7972.38645762 -1132.36984076]\n",
      "Train Loss: 8.427065 ---- Test Loss: 11.976361\n",
      "Epoch 45/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8054.7739264   7736.96060534  -955.86194483]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8328.82029137  7971.90864078 -1132.45326732]\n",
      "Train Loss: 8.421939 ---- Test Loss: 11.967262\n",
      "Epoch 46/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8056.96684494  7736.86350235  -955.9915498 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8330.57295841  7971.04211588 -1132.51879699]\n",
      "Train Loss: 8.415968 ---- Test Loss: 11.968586\n",
      "Epoch 47/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8059.09503303  7736.52384561  -956.10280173]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.36295141  7969.86680731 -1132.5660399 ]\n",
      "Train Loss: 8.409077 ---- Test Loss: 11.978765\n",
      "Epoch 48/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8061.09782326  7736.00143344  -956.19550314]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8334.1275411   7968.48024296 -1132.59717603]\n",
      "Train Loss: 8.401371 ---- Test Loss: 11.995658\n",
      "Epoch 49/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8062.92129734  7735.38259826  -956.27144106]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8335.8082506   7966.98764101 -1132.61520148]\n",
      "Train Loss: 8.393045 ---- Test Loss: 12.016906\n",
      "Epoch 50/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8064.51563485  7734.76236787  -956.33316561]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8337.36165552  7965.50497481 -1132.62490244]\n",
      "Train Loss: 8.384275 ---- Test Loss: 12.039739\n",
      "Epoch 51/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8065.84647605  7734.24935646  -956.38498225]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8338.75509246  7964.13721504 -1132.63107488]\n",
      "Train Loss: 8.375220 ---- Test Loss: 12.061489\n",
      "Epoch 52/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8066.88789863  7733.94498923  -956.43128907]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8339.95448924  7962.97987205 -1132.63776717]\n",
      "Train Loss: 8.365925 ---- Test Loss: 12.079867\n",
      "Epoch 53/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8067.62131267  7733.93110882  -956.47577716]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8340.9362245   7962.09594692 -1132.64756176]\n",
      "Train Loss: 8.356361 ---- Test Loss: 12.093233\n",
      "Epoch 54/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8068.03222575  7734.26255921  -956.52079429]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8341.68125639  7961.51714397 -1132.66124399]\n",
      "Train Loss: 8.346482 ---- Test Loss: 12.100781\n",
      "Epoch 55/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8068.11094434  7734.9618908   -956.56707673]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8342.17514979  7961.24370188 -1132.67779515]\n",
      "Train Loss: 8.336279 ---- Test Loss: 12.102536\n",
      "Epoch 56/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8067.85328907  7736.01859968  -956.61374925]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8342.4107311   7961.24756774 -1132.69479768]\n",
      "Train Loss: 8.325809 ---- Test Loss: 12.099271\n",
      "Epoch 57/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8067.26208722  7737.39363209  -956.65868349]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8342.39148645  7961.4786025  -1132.70909758]\n",
      "Train Loss: 8.315203 ---- Test Loss: 12.092256\n",
      "Epoch 58/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8066.34894505  7739.02641714  -956.69902216]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8342.12891191  7961.87180196 -1132.71710485]\n",
      "Train Loss: 8.304569 ---- Test Loss: 12.083066\n",
      "Epoch 59/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8065.13366124  7740.83984532  -956.73143499]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8341.64755389  7962.36186074 -1132.71613783]\n",
      "Train Loss: 8.294104 ---- Test Loss: 12.073234\n",
      "Epoch 60/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8063.6534556   7742.75736226  -956.75369854]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8341.00237101  7962.89550093 -1132.70649137]\n",
      "Train Loss: 8.283961 ---- Test Loss: 12.063857\n",
      "Epoch 61/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8061.9774891   7744.71651956  -956.76639305]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8340.24150986  7963.41802946 -1132.68787901]\n",
      "Train Loss: 8.274525 ---- Test Loss: 12.055974\n",
      "Epoch 62/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8060.16420031  7746.65425248  -956.76939419]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8339.42058844  7963.88761993 -1132.66141588]\n",
      "Train Loss: 8.265639 ---- Test Loss: 12.050227\n",
      "Epoch 63/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8058.27975412  7748.52131489  -956.76386731]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8338.59753988  7964.2769841  -1132.62936583]\n",
      "Train Loss: 8.257366 ---- Test Loss: 12.046863\n",
      "Epoch 64/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8056.3928099   7750.28409839  -956.75205291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8337.82808512  7964.57202865 -1132.59472913]\n",
      "Train Loss: 8.249731 ---- Test Loss: 12.045780\n",
      "Epoch 65/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8054.56830392  7751.92445949  -956.7368741 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8337.15908676  7964.76996615 -1132.56064303]\n",
      "Train Loss: 8.242723 ---- Test Loss: 12.046656\n",
      "Epoch 66/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8052.86256335  7753.43564655  -956.72139542]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.62528034  7964.8744869  -1132.52982271]\n",
      "Train Loss: 8.236296 ---- Test Loss: 12.049071\n",
      "Epoch 67/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8051.317809    7754.81886063  -956.70828158]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.2454662   7964.89296032 -1132.50410162]\n",
      "Train Loss: 8.230391 ---- Test Loss: 12.052647\n",
      "Epoch 68/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8049.95866186  7756.07966713  -956.69937079]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.02047845  7964.83491022 -1132.48418405]\n",
      "Train Loss: 8.224937 ---- Test Loss: 12.057120\n",
      "Epoch 69/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8048.79062621  7757.22568713  -956.69544566]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8335.93554717  7964.70917066 -1132.46962004]\n",
      "Train Loss: 8.219888 ---- Test Loss: 12.062375\n",
      "Epoch 70/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8047.80199186  7758.2642787   -956.69621362]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8335.95489435  7964.52042116 -1132.45818627]\n",
      "Train Loss: 8.215203 ---- Test Loss: 12.068508\n",
      "Epoch 71/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8046.95707779  7759.19890564  -956.69967554]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.04197199  7964.28003322 -1132.4480591 ]\n",
      "Train Loss: 8.210821 ---- Test Loss: 12.075531\n",
      "Epoch 72/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8046.22050276  7760.03909165  -956.7042126 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.15884148  7964.00165813 -1132.43747077]\n",
      "Train Loss: 8.206783 ---- Test Loss: 12.083366\n",
      "Epoch 73/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8045.55269987  7760.79941095  -956.70824988]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.27824113  7963.71827662 -1132.42672314]\n",
      "Train Loss: 8.203068 ---- Test Loss: 12.091378\n",
      "Epoch 74/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8044.92731698  7761.5121425   -956.71207993]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.36716532  7963.44825086 -1132.41471782]\n",
      "Train Loss: 8.199696 ---- Test Loss: 12.099317\n",
      "Epoch 75/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8044.3095216   7762.19683436  -956.71471782]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.39993569  7963.21142162 -1132.40097689]\n",
      "Train Loss: 8.196559 ---- Test Loss: 12.106829\n",
      "Epoch 76/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8043.672662    7762.87389177  -956.71573445]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.36039095  7963.02780203 -1132.38570997]\n",
      "Train Loss: 8.193606 ---- Test Loss: 12.113481\n",
      "Epoch 77/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8042.99987327  7763.56447546  -956.71534925]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.24189739  7962.91610106 -1132.36971499]\n",
      "Train Loss: 8.190791 ---- Test Loss: 12.118795\n",
      "Epoch 78/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8042.2840451   7764.28840548  -956.71430336]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8336.04712459  7962.89117517 -1132.35418955]\n",
      "Train Loss: 8.188076 ---- Test Loss: 12.122336\n",
      "Epoch 79/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8041.52723494  7765.06194302  -956.71369143]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8335.78570126  7962.9615792  -1132.34039831]\n",
      "Train Loss: 8.185429 ---- Test Loss: 12.123777\n",
      "Epoch 80/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8040.73934756  7765.89424225  -956.71466906]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8335.47159588  7963.12812807 -1132.32938936]\n",
      "Train Loss: 8.182830 ---- Test Loss: 12.122958\n",
      "Epoch 81/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.93439439  7766.78684279  -956.71819193]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8335.12229267  7963.3818373  -1132.32179573]\n",
      "Train Loss: 8.180264 ---- Test Loss: 12.119944\n",
      "Epoch 82/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.12875827  7767.73157784  -956.7247842 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8334.75383764  7963.7066201  -1132.31766632]\n",
      "Train Loss: 8.177725 ---- Test Loss: 12.115011\n",
      "Epoch 83/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8038.33872286  7768.71190838  -956.73446538]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8334.38207729  7964.07898273 -1132.31653301]\n",
      "Train Loss: 8.175218 ---- Test Loss: 12.108624\n",
      "Epoch 84/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8037.5797551   7769.70346599  -956.74673813]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8334.02014537  7964.47220205 -1132.3175154 ]\n",
      "Train Loss: 8.172745 ---- Test Loss: 12.101364\n",
      "Epoch 85/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8036.86391154  7770.67899331  -956.76072233]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8333.67982276  7964.85825984 -1132.31954866]\n",
      "Train Loss: 8.170313 ---- Test Loss: 12.093834\n",
      "Epoch 86/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8036.20209662  7771.60969564  -956.77537454]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8333.36962551  7965.21344625 -1132.32162804]\n",
      "Train Loss: 8.167925 ---- Test Loss: 12.086583\n",
      "Epoch 87/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8035.60133183  7772.47150423  -956.78969274]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8333.09760841  7965.518492   -1132.32301708]\n",
      "Train Loss: 8.165580 ---- Test Loss: 12.080015\n",
      "Epoch 88/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8035.06777351  7773.24571931  -956.80294067]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.86850107  7965.76347672 -1132.32337762]\n",
      "Train Loss: 8.163277 ---- Test Loss: 12.074359\n",
      "Epoch 89/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8034.60566171  7773.9221397   -956.81476971]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.68560846  7965.94624958 -1132.32279768]\n",
      "Train Loss: 8.161011 ---- Test Loss: 12.069651\n",
      "Epoch 90/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8034.21644607  7774.49987821  -956.82521398]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.55094222  7966.07128133 -1132.32172306]\n",
      "Train Loss: 8.158785 ---- Test Loss: 12.065763\n",
      "Epoch 91/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.89922008  7774.98633473  -956.83465624]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.45125318  7966.13376279 -1132.31888628]\n",
      "Train Loss: 8.156644 ---- Test Loss: 12.062945\n",
      "Epoch 92/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.63957364  7775.37749984  -956.84197015]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.38521402  7966.15344917 -1132.31553384]\n",
      "Train Loss: 8.154549 ---- Test Loss: 12.060729\n",
      "Epoch 93/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.43388862  7775.6961768   -956.84828937]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.35035494  7966.15303887 -1132.31303105]\n",
      "Train Loss: 8.152506 ---- Test Loss: 12.058588\n",
      "Epoch 94/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.27749369  7775.9682976   -956.85486946]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.34072894  7966.15466657 -1132.3124581 ]\n",
      "Train Loss: 8.150520 ---- Test Loss: 12.056045\n",
      "Epoch 95/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.16283796  7776.21866767  -956.86272216]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.34752536  7966.17602994 -1132.31439075]\n",
      "Train Loss: 8.148594 ---- Test Loss: 12.052752\n",
      "Epoch 96/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.07895662  7776.46754649  -956.87237896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.35914984  7966.22791808 -1132.31873815]\n",
      "Train Loss: 8.146729 ---- Test Loss: 12.048567\n",
      "Epoch 97/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.01189834  7776.72815956  -956.88376916]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.36227942  7966.31479584 -1132.3248575 ]\n",
      "Train Loss: 8.144920 ---- Test Loss: 12.043524\n",
      "Epoch 98/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.9461083   7777.00617159  -956.89627352]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.34378055  7966.43493855 -1132.33169931]\n",
      "Train Loss: 8.143171 ---- Test Loss: 12.037817\n",
      "Epoch 99/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.86596034  7777.30179043  -956.90894365]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8332.29236592  7966.58328973 -1132.3381191 ]\n",
      "Train Loss: 8.141479 ---- Test Loss: 12.031713\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch_size\n",
    "num_epoch = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "main(names_FS, test_names, bs, num_epoch, y_FS, y_test, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 1.782160221439709\n",
      "Localization Error (m) = 176.62557058374003\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
