{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Example on Radar Dataset (Great Smoky Mountains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the labels and creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Training labels:\n",
      "[[ 8.3355 13.325  19.525 ]\n",
      " [18.156  17.388  32.775 ]\n",
      " [ 8.777  24.73   18.582 ]\n",
      " ...\n",
      " [15.785  15.992  29.641 ]\n",
      " [ 3.1736 10.716  12.27  ]\n",
      " [ 4.5552 21.035  14.234 ]]\n"
     ]
    }
   ],
   "source": [
    "def index(string):\n",
    "    s = re.findall(\"[0-9]\", string)\n",
    "    return int(''.join(s))\n",
    "\n",
    "scenario_idx = 29 # Bonneville Salt Flats\n",
    "names = os.listdir(f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names = sorted(names, key=index)\n",
    "print(len(names))\n",
    "y = pd.read_csv(f'num{scenario_idx}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names = y.columns[4:7]\n",
    "y = y[col_names].to_numpy()\n",
    "\n",
    "scenario_idx_test = 76 # Great Smoky Mountains\n",
    "names_test = os.listdir(f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/')\n",
    "names_test = sorted(names_test, key=index)\n",
    "y_test = pd.read_csv(f'num{scenario_idx_test}_Ground_Truth_20_rng30_1000_100k_m.csv')\n",
    "col_names_test = y_test.columns[4:7]\n",
    "y_test = y_test[col_names_test].to_numpy()\n",
    "\n",
    "# Define fine-tuning name and labels\n",
    "few_shot_count = 64\n",
    "names_FS = names_test[:few_shot_count]\n",
    "y_FS = y_test[:few_shot_count]\n",
    "\n",
    "# Create training and testing datasets\n",
    "y_train = y[:int(0.9 * len(names))]\n",
    "y_test = y_test[int(0.9 * len(names_test)) - 1:]\n",
    "training_names = names[:int(0.9 * len(names))]\n",
    "test_names = names_test[int(0.9 * len(names_test)) - 1:]\n",
    "\n",
    "print('Training labels:')\n",
    "print(y_train)\n",
    "\n",
    "# Tensor Corners\n",
    "##################################################################################\n",
    "# num29: [10851, 215, -5.4], num60: [11073, 215, -5.3], num62: [11471, 215, -5.6]\n",
    "# num76: [11388, 215, -6.15], num35: [11381, 215, -0.95]\n",
    "##################################################################################\n",
    "\n",
    "# Training dataset global constants\n",
    "coord_tr = [10851, 215, -5.4]   # Tensor corner\n",
    "rng_res_tr = 59.9585 / 2        # Range resolution\n",
    "az_step_tr = 0.4                # Azimuth step size\n",
    "el_step_tr = 0.01               # Elevation step size\n",
    "\n",
    "# Test dataset global constants\n",
    "coord_ts = [11388, 215, -6.15]  # Tensor corner\n",
    "rng_res_ts = 59.9585 / 2        # Range resolution\n",
    "az_step_ts = 0.4                # Azimuth step size\n",
    "el_step_ts = 0.01               # Elevation step size\n",
    "\n",
    "def Drawing_Batch(names, label, bs, ind, directory, normalize=True):\n",
    "    x = []\n",
    "    labels = []\n",
    "    \n",
    "    for j in range(ind * bs, (ind + 1) * bs):\n",
    "        try:\n",
    "            temp = sio.loadmat(directory + names[j])['P']\n",
    "        except:\n",
    "            break\n",
    "        if normalize:\n",
    "            Anorm = temp - np.min(temp.flatten())\n",
    "            temp = np.divide(Anorm, np.max(Anorm.flatten()))\n",
    "        x.append(temp)\n",
    "        labels.append(label[j, :])\n",
    "        \n",
    "    x = torch.FloatTensor(np.array(x))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "    return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a regression CNN and instantiating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 32, 24]           2,048\n",
      "       BatchNorm1d-2               [-1, 32, 24]              64\n",
      "            Conv1d-3               [-1, 64, 10]           6,208\n",
      "       BatchNorm1d-4               [-1, 64, 10]             128\n",
      "            Linear-5                   [-1, 20]           6,420\n",
      "            Linear-6                    [-1, 2]              42\n",
      "               Net-7                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 14,910\n",
      "Trainable params: 14,910\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(21, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64 * 5, 20)  # Adjust input size based on the output of conv layers and max pooling\n",
    "        self.fc2_reg = nn.Linear(20, 2)  # Adjusted output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        output_reg = self.fc2_reg(x)  # (bs, 2)\n",
    "        \n",
    "        return output_reg\n",
    "    \n",
    "from torchsummary import summary\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = Net()\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "print(summary(model,(21,26)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128  # batch_size\n",
    "num_epoch = 10  # number of epochs\n",
    "PATH = './ckpt_model.pth'   # forsaving the model\n",
    "criterion = nn.MSELoss()\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define a Loss function and optimizer; Using GPU or CPU\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "model = model.to(device)\n",
    "if device == 'cuda:0':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "def Spher2Cart_1D(spherical):\n",
    "    cartesian = np.zeros(3)\n",
    "    hypotenuse = np.cos(np.radians(spherical[2]))*spherical[0]\n",
    "    cartesian[0] = np.cos(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[1] = -np.sin(np.radians(spherical[1]))*hypotenuse\n",
    "    cartesian[2] = np.sin(np.radians(spherical[2]))*spherical[0]\n",
    "    return cartesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8201.78965758  7752.09649026 -1007.12000372]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8673.90116691  7484.90196981 -1131.15709508]\n",
      "Train Loss: 7.494222 ---- Test Loss: 41.460526\n",
      "Epoch 1/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8190.91999719  7758.68701287 -1006.81964003]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8720.74150625  7430.90880575 -1131.19767986]\n",
      "Train Loss: 0.218877 ---- Test Loss: 54.987772\n",
      "Epoch 2/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8195.53088076  7748.54268886 -1006.4962598 ]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8777.59706943  7377.55012236 -1132.07939997]\n",
      "Train Loss: 0.148049 ---- Test Loss: 60.940435\n",
      "Epoch 3/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8199.69832416  7736.50556766 -1006.02905223]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8804.46039359  7362.15771869 -1133.13557497]\n",
      "Train Loss: 0.125275 ---- Test Loss: 61.030189\n",
      "Epoch 4/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8200.19485873  7728.47305988 -1005.56950668]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8816.16216403  7355.91884976 -1133.62740162]\n",
      "Train Loss: 0.122205 ---- Test Loss: 59.875853\n",
      "Epoch 5/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8194.88981419  7733.43534648 -1005.52891706]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8820.15441533  7351.21510273 -1133.63268979]\n",
      "Train Loss: 0.129559 ---- Test Loss: 60.821772\n",
      "Epoch 6/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8187.55393371  7750.60918348 -1006.10587823]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8807.46342317  7397.06838847 -1135.57670598]\n",
      "Train Loss: 0.138631 ---- Test Loss: 55.425508\n",
      "Epoch 7/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8182.62516521  7768.19378487 -1006.86628556]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8795.97773402  7420.05624828 -1136.17068494]\n",
      "Train Loss: 0.142279 ---- Test Loss: 52.436051\n",
      "Epoch 8/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8182.77764212  7768.80860521 -1006.91392983]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8787.44065239  7441.26570132 -1136.87866789]\n",
      "Train Loss: 0.126095 ---- Test Loss: 50.650165\n",
      "Epoch 9/10\n",
      "----------\n",
      "[-8187.45955885  7784.83170223 -1008.20172776]\n",
      "[-8182.50938153  7768.40185613 -1006.87157655]\n",
      "[-8305.41473486  8082.7572811  -1144.22594828]\n",
      "[-8788.22468607  7447.20115686 -1137.31651446]\n",
      "Train Loss: 0.108613 ---- Test Loss: 50.066348\n",
      "632.1163589954376\n"
     ]
    }
   ],
   "source": [
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "start = time.time()\n",
    "main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 7.236562654693393\n",
      "Localization Error (m) = 601.5837993848022\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Regression CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.conv1.weight.requires_grad = False\n",
    "model.module.conv1.bias.requires_grad = False\n",
    "model.module.batchnorm1.weight.requires_grad = False\n",
    "model.module.batchnorm1.bias.requires_grad = False\n",
    "\n",
    "model.module.conv2.weight.requires_grad = False\n",
    "model.module.conv2.bias.requires_grad = False\n",
    "model.module.batchnorm2.weight.requires_grad = False\n",
    "model.module.batchnorm2.bias.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8086.80003282  7779.37301071  -960.35345043]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8733.5095606   7429.46069382 -1126.2616375 ]\n",
      "Train Loss: 23.463051 ---- Test Loss: 45.568877\n",
      "Epoch 1/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8052.6115664   7808.18439468  -959.9617904 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8719.36172319  7454.31316165 -1126.7882673 ]\n",
      "Train Loss: 21.562136 ---- Test Loss: 44.080161\n",
      "Epoch 2/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.21554544  7835.30774769  -959.53276358]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8697.7510198   7484.21687488 -1127.08929463]\n",
      "Train Loss: 19.887175 ---- Test Loss: 41.385051\n",
      "Epoch 3/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7987.56295577  7859.75791712  -959.06353462]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8669.49963237  7521.94851784 -1127.4126501 ]\n",
      "Train Loss: 18.432178 ---- Test Loss: 38.087341\n",
      "Epoch 4/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7958.44158334  7880.8511538   -958.55800915]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8633.9853704   7567.98130818 -1127.75538792]\n",
      "Train Loss: 17.181892 ---- Test Loss: 34.705958\n",
      "Epoch 5/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7932.34829852  7898.23833806  -958.02186521]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8598.82833619  7613.13114271 -1128.09570818]\n",
      "Train Loss: 16.115299 ---- Test Loss: 31.514586\n",
      "Epoch 6/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7909.4577049   7911.92301707  -957.46253859]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8561.71950636  7659.15804356 -1128.37845993]\n",
      "Train Loss: 15.210331 ---- Test Loss: 28.601735\n",
      "Epoch 7/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7889.77223347  7922.0801684   -956.88789909]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8523.93218528  7704.40558297 -1128.5902474 ]\n",
      "Train Loss: 14.446072 ---- Test Loss: 26.041183\n",
      "Epoch 8/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7873.25919128  7928.90756017  -956.3056848 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8487.13226683  7747.16256479 -1128.73837635]\n",
      "Train Loss: 13.802404 ---- Test Loss: 23.849403\n",
      "Epoch 9/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7859.89318516  7932.5840683   -955.72349977]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8452.70852992  7786.06460464 -1128.82872997]\n",
      "Train Loss: 13.259552 ---- Test Loss: 22.006192\n",
      "Epoch 10/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7849.64211309  7933.28669448  -955.14894559]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8421.36398163  7820.69264427 -1128.87796561]\n",
      "Train Loss: 12.798408 ---- Test Loss: 20.493639\n",
      "Epoch 11/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7842.44586318  7931.21579711  -954.58982865]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8393.15583877  7851.23341692 -1128.8964536 ]\n",
      "Train Loss: 12.401310 ---- Test Loss: 19.272636\n",
      "Epoch 12/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7838.2031642   7926.61198939  -954.05435027]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8369.72638745  7875.81038922 -1128.86986471]\n",
      "Train Loss: 12.052815 ---- Test Loss: 18.295005\n",
      "Epoch 13/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7836.76842052  7919.7600867   -953.55109211]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8350.77576366  7895.09531729 -1128.81549211]\n",
      "Train Loss: 11.740183 ---- Test Loss: 17.502864\n",
      "Epoch 14/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7837.95061965  7910.98550637  -953.08864559]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8334.43871923  7911.68998548 -1128.77156391]\n",
      "Train Loss: 11.453564 ---- Test Loss: 16.861083\n",
      "Epoch 15/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7841.52164177  7900.64176985  -952.67525104]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8320.92518498  7925.40806628 -1128.73814997]\n",
      "Train Loss: 11.185850 ---- Test Loss: 16.335087\n",
      "Epoch 16/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7847.22277609  7889.09854506  -952.31835891]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8310.09088093  7936.52947392 -1128.72200127]\n",
      "Train Loss: 10.932344 ---- Test Loss: 15.893071\n",
      "Epoch 17/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7854.77197612  7876.72733598  -952.02410069]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8301.7006254   7945.48048028 -1128.73389479]\n",
      "Train Loss: 10.690311 ---- Test Loss: 15.510662\n",
      "Epoch 18/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7863.87597021  7863.88246247  -951.79678671]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8295.80612921  7952.21592766 -1128.77340293]\n",
      "Train Loss: 10.458544 ---- Test Loss: 15.170932\n",
      "Epoch 19/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7874.23444664  7850.89195682  -951.63855264]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8291.86177008  7957.52442931 -1128.85472635]\n",
      "Train Loss: 10.236996 ---- Test Loss: 14.865524\n",
      "Epoch 20/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7885.55144893  7838.04617992  -951.54933022]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8289.67558387  7961.58562509 -1128.97608754]\n",
      "Train Loss: 10.026441 ---- Test Loss: 14.581950\n",
      "Epoch 21/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7897.53747419  7825.59852962  -951.5270106 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8288.98914541  7964.69031266 -1129.13873159]\n",
      "Train Loss: 9.828192 ---- Test Loss: 14.313773\n",
      "Epoch 22/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7909.91696863  7813.76330822  -951.56778079]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8289.3651459   7967.24082497 -1129.33895254]\n",
      "Train Loss: 9.643807 ---- Test Loss: 14.057232\n",
      "Epoch 23/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7922.4307873   7802.71785699  -951.66642597]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8290.63906326  7969.43332145 -1129.57840632]\n",
      "Train Loss: 9.474839 ---- Test Loss: 13.812126\n",
      "Epoch 24/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7934.83978555  7792.60482787  -951.81671713]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8292.55384693  7971.4220018  -1129.84937075]\n",
      "Train Loss: 9.322605 ---- Test Loss: 13.578611\n",
      "Epoch 25/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7946.92846717  7783.5322742   -952.01167679]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.74586616  7973.45199231 -1130.14277929]\n",
      "Train Loss: 9.188026 ---- Test Loss: 13.357198\n",
      "Epoch 26/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7958.50696349  7775.57486583  -952.24381307]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.02785871  7975.67854116 -1130.45593911]\n",
      "Train Loss: 9.071526 ---- Test Loss: 13.148929\n",
      "Epoch 27/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7969.41458045  7768.77236843  -952.50528545]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8299.25357274  7978.17144694 -1130.78324511]\n",
      "Train Loss: 8.972967 ---- Test Loss: 12.955199\n",
      "Epoch 28/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7979.52027206  7763.13102316  -952.78805352]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8301.30677651  7980.86562704 -1131.11203798]\n",
      "Train Loss: 8.891661 ---- Test Loss: 12.776386\n",
      "Epoch 29/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7988.72535167  7758.62247699  -953.084011  ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8303.1163411   7983.76365569 -1131.43746025]\n",
      "Train Loss: 8.826373 ---- Test Loss: 12.612780\n",
      "Epoch 30/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-7996.96229878  7755.18609935  -953.38507604]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8304.59661734  7986.85739261 -1131.75290067]\n",
      "Train Loss: 8.775393 ---- Test Loss: 12.464853\n",
      "Epoch 31/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8004.19572215  7752.73150921  -953.68341786]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8305.69793752  7990.06302689 -1132.0491474 ]\n",
      "Train Loss: 8.736578 ---- Test Loss: 12.332056\n",
      "Epoch 32/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8010.4182346  7751.1454658  -953.9716226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8306.40868441  7993.3010468  -1132.31997627]\n",
      "Train Loss: 8.707473 ---- Test Loss: 12.213678\n",
      "Epoch 33/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8015.64940276  7750.29825865  -954.24301021]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8306.69258534  7996.52749019 -1132.5598395 ]\n",
      "Train Loss: 8.685439 ---- Test Loss: 12.108832\n",
      "Epoch 34/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.93066026  7750.05326084  -954.4918856 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8306.60050758  7999.63614927 -1132.76511495]\n",
      "Train Loss: 8.667862 ---- Test Loss: 12.016105\n",
      "Epoch 35/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8023.3212501   7750.27651197  -954.71385067]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8306.18230727  8002.54203025 -1132.93354716]\n",
      "Train Loss: 8.652351 ---- Test Loss: 11.934167\n",
      "Epoch 36/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8025.89351599  7750.84491206  -954.90599203]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8305.43175345  8005.23160007 -1133.06377783]\n",
      "Train Loss: 8.636969 ---- Test Loss: 11.861801\n",
      "Epoch 37/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8027.72851625  7751.65334099  -955.06702489]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8304.39544653  8007.66732585 -1133.15654761]\n",
      "Train Loss: 8.620366 ---- Test Loss: 11.797980\n",
      "Epoch 38/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8028.91241948  7752.61823927  -955.19727593]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8303.1571689   8009.78219915 -1133.21321059]\n",
      "Train Loss: 8.601870 ---- Test Loss: 11.741968\n",
      "Epoch 39/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8029.53236296  7753.67966261  -955.29854437]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8301.75513006  8011.61169102 -1133.23888137]\n",
      "Train Loss: 8.581432 ---- Test Loss: 11.693157\n",
      "Epoch 40/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8029.674604    7754.80060188  -955.3739435 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8300.22453283  8013.19336058 -1133.23860468]\n",
      "Train Loss: 8.559523 ---- Test Loss: 11.651125\n",
      "Epoch 41/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8029.42281109  7755.96436794  -955.42763737]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8298.61176093  8014.56346447 -1133.21812675]\n",
      "Train Loss: 8.536937 ---- Test Loss: 11.615522\n",
      "Epoch 42/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8028.85536077  7757.17245272  -955.46454581]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.96196517  8015.76504655 -1133.1835697 ]\n",
      "Train Loss: 8.514603 ---- Test Loss: 11.585846\n",
      "Epoch 43/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8028.04616443  7758.43917334  -955.49007624]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8295.31746255  8016.84553342 -1133.14115492]\n",
      "Train Loss: 8.493390 ---- Test Loss: 11.561470\n",
      "Epoch 44/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8027.06297744  7759.78903829  -955.50986282]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8293.71492215  8017.85849326 -1133.0971247 ]\n",
      "Train Loss: 8.473955 ---- Test Loss: 11.541565\n",
      "Epoch 45/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8025.9689612   7761.24959666  -955.52943728]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8292.18703054  8018.85562209 -1133.05731332]\n",
      "Train Loss: 8.456662 ---- Test Loss: 11.525134\n",
      "Epoch 46/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8024.82122803  7762.84880381  -955.55398278]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8290.76385622  8019.88228397 -1133.02693879]\n",
      "Train Loss: 8.441545 ---- Test Loss: 11.511063\n",
      "Epoch 47/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8023.67209658  7764.610036    -955.58811437]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8289.47127024  8020.97554643 -1133.01035689]\n",
      "Train Loss: 8.428347 ---- Test Loss: 11.498249\n",
      "Epoch 48/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8022.56936572  7766.54738744  -955.6356158 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8288.33352082  8022.15896714 -1133.01088651]\n",
      "Train Loss: 8.416607 ---- Test Loss: 11.485720\n",
      "Epoch 49/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8021.55629347  7768.6637754   -955.69932485]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8287.37124619  8023.44322375 -1133.03071185]\n",
      "Train Loss: 8.405772 ---- Test Loss: 11.472733\n",
      "Epoch 50/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8020.67116952  7770.94905753  -955.78099444]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8286.60295962  8024.82411656 -1133.07085022]\n",
      "Train Loss: 8.395303 ---- Test Loss: 11.458874\n",
      "Epoch 51/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.9480625  7773.3786575  -955.881256 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8286.04439973  8026.28555773 -1133.13131002]\n",
      "Train Loss: 8.384776 ---- Test Loss: 11.444062\n",
      "Epoch 52/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.41542839  7775.91591155  -955.99967312]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8285.70396245  8027.79748177 -1133.21062534]\n",
      "Train Loss: 8.373941 ---- Test Loss: 11.428545\n",
      "Epoch 53/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.09519178  7778.51383878  -956.13479035]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8285.58723046  8029.32235487 -1133.30662038]\n",
      "Train Loss: 8.362736 ---- Test Loss: 11.412828\n",
      "Epoch 54/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.00284813  7781.11728449  -956.28426723]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8285.69393669  8030.81799884 -1133.41638849]\n",
      "Train Loss: 8.351267 ---- Test Loss: 11.397581\n",
      "Epoch 55/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.14626417  7783.66774854  -956.44509286]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8286.01753373  8032.24037927 -1133.53645286]\n",
      "Train Loss: 8.339764 ---- Test Loss: 11.383530\n",
      "Epoch 56/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8019.52447655  7786.10677574  -956.61371512]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8286.54480149  8033.54815111 -1133.66305022]\n",
      "Train Loss: 8.328512 ---- Test Loss: 11.371360\n",
      "Epoch 57/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8020.12812185  7788.38011487  -956.78631615]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8287.25480075  8034.70593673 -1133.79228173]\n",
      "Train Loss: 8.317804 ---- Test Loss: 11.361625\n",
      "Epoch 58/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8020.93823385  7790.44175914  -956.95898057]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8288.12081752  8035.68644251 -1133.92039529]\n",
      "Train Loss: 8.307875 ---- Test Loss: 11.354697\n",
      "Epoch 59/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8021.9284298   7792.25568509  -957.12793439]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8289.10952413  8036.47330428 -1134.0439211 ]\n",
      "Train Loss: 8.298885 ---- Test Loss: 11.350728\n",
      "Epoch 60/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8023.06372806  7793.79951273  -957.28969133]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8290.18301773  8037.06201203 -1134.15987842]\n",
      "Train Loss: 8.290905 ---- Test Loss: 11.349654\n",
      "Epoch 61/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8024.30317806  7795.06543375  -957.44126974]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8291.29932772  8037.46084456 -1134.26587487]\n",
      "Train Loss: 8.283917 ---- Test Loss: 11.351215\n",
      "Epoch 62/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8025.60139801  7796.05965568  -957.58025358]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8292.41519034  8037.68928447 -1134.36019299]\n",
      "Train Loss: 8.277846 ---- Test Loss: 11.354984\n",
      "Epoch 63/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8026.91065209  7796.80235971  -957.70491679]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8293.48783243  8037.77653558 -1134.44181407]\n",
      "Train Loss: 8.272569 ---- Test Loss: 11.360419\n",
      "Epoch 64/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8028.18334588  7797.32514542  -957.81422322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.47743985  8037.7589841  -1134.51041809]\n",
      "Train Loss: 8.267944 ---- Test Loss: 11.366908\n",
      "Epoch 65/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8029.37483846  7797.66814824  -957.90782668]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8295.34802919  8037.67803555 -1134.56629712]\n",
      "Train Loss: 8.263832 ---- Test Loss: 11.373819\n",
      "Epoch 66/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8030.44568092  7797.87700716  -957.98602703]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.07142215  8037.57517399 -1134.61029799]\n",
      "Train Loss: 8.260107 ---- Test Loss: 11.380546\n",
      "Epoch 67/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8031.36365971  7797.99839187  -958.04962882]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.62674571  8037.49018213 -1134.64366582]\n",
      "Train Loss: 8.256663 ---- Test Loss: 11.386544\n",
      "Epoch 68/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.10628011  7798.07678776  -958.09990222]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.00310049  8037.45686575 -1134.66794057]\n",
      "Train Loss: 8.253421 ---- Test Loss: 11.391369\n",
      "Epoch 69/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.66123199  7798.1507786   -958.13839047]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.19922025  8037.50186302 -1134.68485226]\n",
      "Train Loss: 8.250328 ---- Test Loss: 11.394696\n",
      "Epoch 70/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.02748108  7798.2505646   -958.1668294 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.22343526  8037.6420384  -1134.6961407 ]\n",
      "Train Loss: 8.247356 ---- Test Loss: 11.396339\n",
      "Epoch 71/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.21491317  7798.39595019  -958.18700605]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.09338302  8037.88334765 -1134.70345764]\n",
      "Train Loss: 8.244498 ---- Test Loss: 11.396251\n",
      "Epoch 72/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.24382832  7798.59502158  -958.20064893]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.83455479  8038.22178862 -1134.70832908]\n",
      "Train Loss: 8.241766 ---- Test Loss: 11.394518\n",
      "Epoch 73/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.14320364  7798.84495592  -958.20936955]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.47859809  8038.64368734 -1134.71205463]\n",
      "Train Loss: 8.239182 ---- Test Loss: 11.391341\n",
      "Epoch 74/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.94850598  7799.1326024   -958.21456243]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.06096975  8039.12792398 -1134.71569354]\n",
      "Train Loss: 8.236765 ---- Test Loss: 11.387004\n",
      "Epoch 75/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.69997056  7799.43613884  -958.21739779]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8295.61841064  8039.64819891 -1134.72004094]\n",
      "Train Loss: 8.234530 ---- Test Loss: 11.381854\n",
      "Epoch 76/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.43917711  7799.72839903  -958.21880938]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8295.1882707   8040.17387744 -1134.72563765]\n",
      "Train Loss: 8.232483 ---- Test Loss: 11.376253\n",
      "Epoch 77/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.2067895   7799.978835    -958.21947251]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.8033823   8040.67573076 -1134.73280087]\n",
      "Train Loss: 8.230604 ---- Test Loss: 11.370563\n",
      "Epoch 78/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.03912259  7800.15766435  -958.21984064]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.49227401  8041.12614151 -1134.74165359]\n",
      "Train Loss: 8.228869 ---- Test Loss: 11.365104\n",
      "Epoch 79/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8031.96744535  7800.23768679  -958.22021121]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.27683422  8041.50240807 -1134.75218602]\n",
      "Train Loss: 8.227233 ---- Test Loss: 11.360147\n",
      "Epoch 80/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.0142402   7800.19792454  -958.22071343]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.1707938  8041.7884867 -1134.7642682]\n",
      "Train Loss: 8.225650 ---- Test Loss: 11.355893\n",
      "Epoch 81/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.19302474  7800.02490353  -958.22137412]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.17974938  8041.97608571 -1134.77772684]\n",
      "Train Loss: 8.224084 ---- Test Loss: 11.352465\n",
      "Epoch 82/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.5080315   7799.71379716  -958.22216646]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.30095707  8042.06489486 -1134.79234675]\n",
      "Train Loss: 8.222502 ---- Test Loss: 11.349914\n",
      "Epoch 83/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8032.9532939   7799.26940916  -958.22301244]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.52317895  8042.06329497 -1134.80790853]\n",
      "Train Loss: 8.220892 ---- Test Loss: 11.348206\n",
      "Epoch 84/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8033.51444854  7798.70522254  -958.22383647]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8294.83025326  8041.98496411 -1134.82420815]\n",
      "Train Loss: 8.219254 ---- Test Loss: 11.347246\n",
      "Epoch 85/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8034.1689285   7798.04296416  -958.2245508 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8295.19967295  8041.85028635 -1134.84105277]\n",
      "Train Loss: 8.217606 ---- Test Loss: 11.346876\n",
      "Epoch 86/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8034.88982797  7797.30908414  -958.22508228]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8295.60624879  8041.68261901 -1134.85826352]\n",
      "Train Loss: 8.215962 ---- Test Loss: 11.346901\n",
      "Epoch 87/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8035.64551359  7796.53500201  -958.22536264]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.02354863  8041.50668881 -1134.87566711]\n",
      "Train Loss: 8.214350 ---- Test Loss: 11.347093\n",
      "Epoch 88/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8036.4034084   7795.75309198  -958.2253212 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.42521865  8041.34708752 -1134.89308607]\n",
      "Train Loss: 8.212784 ---- Test Loss: 11.347226\n",
      "Epoch 89/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8037.13269348  7794.99369535  -958.22487261]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.7883025   8041.22484699 -1134.91033874]\n",
      "Train Loss: 8.211270 ---- Test Loss: 11.347086\n",
      "Epoch 90/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8037.8040802   7794.28510732  -958.2239023 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.09281172  8041.15745065 -1134.92720991]\n",
      "Train Loss: 8.209810 ---- Test Loss: 11.346497\n",
      "Epoch 91/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8038.39411825  7793.65007027  -958.22232249]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.32325983  8041.15796281 -1134.94349974]\n",
      "Train Loss: 8.208397 ---- Test Loss: 11.345332\n",
      "Epoch 92/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8038.88414538  7793.10517049  -958.21997229]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.47054211  8041.23234995 -1134.95897348]\n",
      "Train Loss: 8.207017 ---- Test Loss: 11.343523\n",
      "Epoch 93/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.26257618  7792.6599422   -958.21670541]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.53028879  8041.38195445 -1134.97341452]\n",
      "Train Loss: 8.205658 ---- Test Loss: 11.341068\n",
      "Epoch 94/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.52444413  7792.31704627  -958.21237313]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.50367078  8041.6025603  -1134.98661744]\n",
      "Train Loss: 8.204311 ---- Test Loss: 11.338024\n",
      "Epoch 95/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.67139794  7792.07247405  -958.2068365 ]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.39742131  8041.88484177 -1134.99842013]\n",
      "Train Loss: 8.202968 ---- Test Loss: 11.334498\n",
      "Epoch 96/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.7098944   7791.91753297  -958.19997361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8297.22072685  8042.21777773 -1135.00871779]\n",
      "Train Loss: 8.201628 ---- Test Loss: 11.330634\n",
      "Epoch 97/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.65245673  7791.83861314  -958.19174302]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.98672258  8042.58731545 -1135.01747688]\n",
      "Train Loss: 8.200289 ---- Test Loss: 11.326598\n",
      "Epoch 98/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.51361457  7791.8205192   -958.18213254]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.70983167  8042.97936333 -1135.0247519 ]\n",
      "Train Loss: 8.198958 ---- Test Loss: 11.322558\n",
      "Epoch 99/100\n",
      "----------\n",
      "[-8039.80337222  7765.79211219  -956.65097951]\n",
      "[-8039.31155481  7791.84620252  -958.17124456]\n",
      "[-8271.55879786  8100.10882815 -1137.17137312]\n",
      "[-8296.4046286   8043.38104683 -1135.03069098]\n",
      "Train Loss: 8.197637 ---- Test Loss: 11.318667\n"
     ]
    }
   ],
   "source": [
    "bs = 64  # batch_size\n",
    "num_epoch = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "def main(training_names, test_names, bs, num_epoch, y_train, y_test, normalize=True):\n",
    "    best_error = 1e+20  # a dummy and very large number for saving the best discovered model\n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epoch))\n",
    "        print('-' * 10)\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        model.train()\n",
    "        for i in range(0, len(training_names) // bs):\n",
    "            x_train, labels = Drawing_Batch(training_names, y_train, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "            x_train = x_train.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, labels[:, 0:2])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "        out = torch.cat((out, torch.unsqueeze(labels[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Training error display\n",
    "        true_train = Spher2Cart_1D(np.multiply(labels.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        pred_train = Spher2Cart_1D(np.multiply(out.cpu().data.numpy()[1], [rng_res_tr, az_step_tr, el_step_tr]) + coord_tr)\n",
    "        print(true_train)\n",
    "        print(pred_train)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(test_names) // bs):\n",
    "                x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', normalize)\n",
    "                x_test = x_test.to(device)\n",
    "                labels_test = labels_test.to(device)\n",
    "                out_test = model(x_test)\n",
    "                loss_test = criterion(out_test, labels_test[:, 0:2])\n",
    "                running_loss_test += loss_test.item()\n",
    "                \n",
    "        out_test = torch.cat((out_test, torch.unsqueeze(labels_test[:,2], dim=1)), dim=1)\n",
    "\n",
    "        # Test error display\n",
    "        true_test = Spher2Cart_1D(np.multiply(labels_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        pred_test = Spher2Cart_1D(np.multiply(out_test.cpu().data.numpy()[1], [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "        print(true_test)\n",
    "        print(pred_test)\n",
    "\n",
    "        epoch_loss_train = running_loss_train * x_train.size(0) / len(training_names)\n",
    "        epoch_loss_test = running_loss_test * x_test.size(0) / len(test_names)\n",
    "\n",
    "        print('Train Loss: {:.6f} ---- Test Loss: {:.6f}'.format(epoch_loss_train, epoch_loss_test))\n",
    "        if epoch % 5 == 0:\n",
    "            if epoch_loss_test < best_error:\n",
    "                torch.save(model.state_dict(), PATH)\n",
    "                best_error = epoch_loss_test\n",
    "\n",
    "main(names_FS, test_names, bs, num_epoch, y_FS, y_test, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azimuth Estimation Error (deg) = 1.6143039394028127\n",
      "Localization Error (m) = 169.2359471604409\n"
     ]
    }
   ],
   "source": [
    "def Spher2Cart_2D(spherical):\n",
    "    cartesian = np.zeros((len(spherical), 3))\n",
    "    hypotenuse = np.multiply(np.cos(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    cartesian[:, 0] = np.multiply(np.cos(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 1] = np.multiply(-np.sin(np.radians(spherical[:, 1])), hypotenuse)\n",
    "    cartesian[:, 2] = np.multiply(np.sin(np.radians(spherical[:, 2])), spherical[:, 0])\n",
    "    return cartesian\n",
    "\n",
    "# Testing: (range, az, el)\n",
    "model.eval()\n",
    "out_test_reg = np.zeros((len(y_test), 3))\n",
    "labels_test_reg = np.zeros((len(y_test), 3))\n",
    "\n",
    "for i in range(0, len(y_test) // bs):\n",
    "    x_test, labels_test = Drawing_Batch(test_names, y_test, bs, i, f'num{scenario_idx_test}_NAMF_DATA_20_rng30_pulse1_1000_100k/', True)\n",
    "    x_test = x_test.to(device)\n",
    "    labels_test = labels_test.cpu().data.numpy()\n",
    "    labels_test_reg[bs * i: bs * i + bs] = (labels_test[:, 0:3])\n",
    "\n",
    "    cur_test_reg = model(x_test)\n",
    "    out_test_reg[bs * i: bs * i + bs, 0:2] = cur_test_reg.cpu().data.numpy()\n",
    "    out_test_reg[bs * i: bs * i + bs, 2] = labels_test_reg[bs * i: bs * i + bs, 2]\n",
    "\n",
    "# Calculate azimuth estimation error\n",
    "azim_tot = 0\n",
    "for i in range(len(out_test_reg)):\n",
    "    azim_tot += np.linalg.norm(out_test_reg[i, 1] - labels_test_reg[i, 1])\n",
    "\n",
    "azim_tot = azim_tot / len(out_test_reg)\n",
    "print(f'Azimuth Estimation Error (deg) = {azim_tot}')\n",
    "\n",
    "# Convert spherical coordinates to Cartesian coordinates\n",
    "new_data = Spher2Cart_2D(np.multiply(out_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "new_labels_data = Spher2Cart_2D(np.multiply(labels_test_reg, [rng_res_ts, az_step_ts, el_step_ts]) + coord_ts)\n",
    "\n",
    "# Calculate localization error\n",
    "sum_tot = 0\n",
    "for i in range(0, len(new_data) - (len(new_data) % bs), 1):\n",
    "    sum_tot += np.linalg.norm(new_data[i, :] - new_labels_data[i, :])\n",
    "\n",
    "sum_tot = sum_tot / (len(new_data) - (len(new_data) % bs))\n",
    "print(f'Localization Error (m) = {sum_tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
